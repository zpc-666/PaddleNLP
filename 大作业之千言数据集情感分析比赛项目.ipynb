{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 情感分析预训练模型SKEP\n",
    "\n",
    "本项目演示如何使用情感分析预训练模型SKEP完成千言数据集：情感分析七个数据集的比赛。本项目吸取了快递单信息识别作业、情感分析比赛baseline中的优点，再加上自己的改进，训练和预测自由度较高。\n",
    "截止2021年6月23日23:50分，经过多模型集成学习，情感分析比赛排名为\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/0c9da1c48be94d8fa36badd00c84feed0afb6cb83b9447029e7b68d21057370a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!git clone https://gitee.com/paddlepaddle/PaddleNLP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!unzip -oq /home/aistudio/data/data53469/NLPCC14-SC.zip -d data/\r\n",
    "!unzip -oq /home/aistudio/data/data53469/SE-ABSA16_CAME.zip -d data/\r\n",
    "!unzip -oq /home/aistudio/data/data53469/COTE-BD.zip -d data/\r\n",
    "!unzip -oq /home/aistudio/data/data53469/COTE-DP.zip -d data/\r\n",
    "!unzip -oq /home/aistudio/data/data53469/COTE-MFW.zip -d data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade paddlenlp -i https://pypi.org/simple "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##  Part A. 情感分析任务\n",
    "\n",
    "众所周知，人类自然语言中包含了丰富的情感色彩：表达人的情绪（如悲伤、快乐）、表达人的心情（如倦怠、忧郁）、表达人的喜好（如喜欢、讨厌）、表达人的个性特征和表达人的立场等等。情感分析在商品喜好、消费决策、舆情分析等场景中均有应用。利用机器自动分析这些情感倾向，不但有助于帮助企业了解消费者对其产品的感受，为产品改进提供依据；同时还有助于企业分析商业伙伴们的态度，以便更好地进行商业决策。\n",
    "\n",
    "被人们所熟知的情感分析任务是将一段文本分类，如分为情感极性为**正向**、**负向**、**其他**的三分类问题：\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/b630901b397e4e7a8e78ab1d306dfa1fc070d91015a64ef0b8d590aaa8cfde14\" width=\"600\" ></center>\n",
    "<br><center>情感分析任务</center></br>\n",
    "\n",
    "- **正向：** 表示正面积极的情感，如高兴，幸福，惊喜，期待等。\n",
    "- **负向：** 表示负面消极的情感，如难过，伤心，愤怒，惊恐等。\n",
    "- **其他：** 其他类型的情感。\n",
    "\n",
    "实际上，以上熟悉的情感分析任务是**句子级情感分析任务**。\n",
    "\n",
    "\n",
    "情感分析任务还可以进一步分为**句子级情感分析**、**目标级情感分析**等任务。在下面章节将会详细介绍两种任务及其应用场景。\n",
    "\n",
    "\n",
    "## Part B. 情感分析预训练模型SKEP\n",
    "\n",
    "近年来，大量的研究表明基于大型语料库的预训练模型（Pretrained Models, PTM）可以学习通用的语言表示，有利于下游NLP任务，同时能够避免从零开始训练模型。随着计算能力的发展，深度模型的出现（即 Transformer）和训练技巧的增强使得 PTM 不断发展，由浅变深。\n",
    "\n",
    "情感预训练模型SKEP（Sentiment Knowledge Enhanced Pre-training for Sentiment Analysis）。SKEP利用情感知识增强预训练模型， 在14项中英情感分析典型任务上全面超越SOTA，此工作已经被ACL 2020录用。SKEP是百度研究团队提出的基于情感知识增强的情感预训练算法，此算法采用无监督方法自动挖掘情感知识，然后利用情感知识构建预训练目标，从而让机器学会理解情感语义。SKEP为各类情感分析任务提供统一且强大的情感语义表示。\n",
    "\n",
    "**论文地址**：https://arxiv.org/abs/2005.05635\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://paddlenlp.bj.bcebos.com/models/transformers/skep/skep.png\" width=\"80%\" height=\"60%\"> <br />\n",
    "</p>\n",
    "\n",
    "百度研究团队在三个典型情感分析任务，句子级情感分类（Sentence-level Sentiment Classification），评价目标级情感分类（Aspect-level Sentiment Classification）、观点抽取（Opinion Role Labeling），共计14个中英文数据上进一步验证了情感预训练模型SKEP的效果。\n",
    "\n",
    "具体实验效果参考：https://github.com/baidu/Senta#skep\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Part C 句子级情感分析 & 目标级情感分析\n",
    "\n",
    "### Part C.1 句子级情感分析\n",
    "\n",
    "\n",
    "对给定的一段文本进行情感极性分类，常用于影评分析、网络论坛舆情分析等场景。如:\n",
    "\n",
    "```text\n",
    "选择珠江花园的原因就是方便，有电动扶梯直接到达海边，周围餐馆、食廊、商场、超市、摊位一应俱全。酒店装修一般，但还算整洁。 泳池在大堂的屋顶，因此很小，不过女儿倒是喜欢。 包的早餐是西式的，还算丰富。 服务吗，一般\t1\n",
    "15.4寸笔记本的键盘确实爽，基本跟台式机差不多了，蛮喜欢数字小键盘，输数字特方便，样子也很美观，做工也相当不错\t1\n",
    "房间太小。其他的都一般。。。。。。。。。\t0\n",
    "```\n",
    "\n",
    "其中`1`表示正向情感，`0`表示负向情感。\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/4aae00a800ae4831b6811b669f7461d8482344b183454d8fb7d37c83defb9567\" width=\"550\" ></center>\n",
    "<br><center>句子级情感分析任务</center></br>\n",
    "\n",
    "\n",
    "#### 常用数据集\n",
    "\n",
    "ChnSenticorp数据集是公开中文情感分析常用数据集， 其为2分类数据集。PaddleNLP已经内置该数据集，一键即可加载。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from paddlenlp.datasets import load_dataset\r\n",
    "from paddlenlp.transformers import SkepForSequenceClassification, SkepTokenizer, SkepForTokenClassification\r\n",
    "import os\r\n",
    "from functools import partial\r\n",
    "from paddlenlp.transformers import LinearDecayWithWarmup\r\n",
    "import random\r\n",
    "import numpy as np\r\n",
    "import paddle\r\n",
    "import paddle.nn.functional as F\r\n",
    "from paddlenlp.data import Stack, Tuple, Pad\r\n",
    "\r\n",
    "from utils import create_dataloader, convert_example, predict\r\n",
    "from train import train_model\r\n",
    "from data import load_my_dataset\r\n",
    "def seed_paddle(seed=2021):\r\n",
    "    seed = int(seed)\r\n",
    "    random.seed(seed)\r\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\r\n",
    "    np.random.seed(seed)\r\n",
    "    paddle.seed(seed)\r\n",
    "\r\n",
    "seed_paddle(seed=2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1909/1909 [00:00<00:00, 46823.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset length: 9600\n",
      "dev dataset length: 1200\n",
      "test dataset length: 1200\n",
      "{'text': '选择珠江花园的原因就是方便，有电动扶梯直接到达海边，周围餐馆、食廊、商场、超市、摊位一应俱全。酒店装修一般，但还算整洁。 泳池在大堂的屋顶，因此很小，不过女儿倒是喜欢。 包的早餐是西式的，还算丰富。 服务吗，一般', 'label': 1, 'qid': ''}\n",
      "{'text': '15.4寸笔记本的键盘确实爽，基本跟台式机差不多了，蛮喜欢数字小键盘，输数字特方便，样子也很美观，做工也相当不错', 'label': 1, 'qid': ''}\n",
      "{'text': '房间太小。其他的都一般。。。。。。。。。', 'label': 0, 'qid': ''}\n"
     ]
    }
   ],
   "source": [
    "train_ds, dev_ds, test_ds = load_dataset(\"chnsenticorp\", splits=[\"train\", \"dev\", \"test\"])\n",
    "\n",
    "print(\"train dataset length:\", len(train_ds))\n",
    "print(\"dev dataset length:\", len(dev_ds))\n",
    "print(\"test dataset length:\", len(test_ds))\n",
    "print(train_ds[0])\n",
    "print(train_ds[1])\n",
    "print(train_ds[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "### SKEP模型加载\n",
    "\n",
    "PaddleNLP已经实现了SKEP预训练模型，可以通过一行代码实现SKEP加载。\n",
    "\n",
    "句子级情感分析模型是SKEP fine-tune 文本分类常用模型`SkepForSequenceClassification`。其首先通过SKEP提取句子语义特征，之后将语义特征进行分类。\n",
    "\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/fc21e1201154451a80f32e0daa5fa84386c1b12e4b3244e387ae0b177c1dc963)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 指定模型名称，一键加载模型\n",
    "model = SkepForSequenceClassification.from_pretrained(pretrained_model_name_or_path=\"skep_ernie_1.0_large_ch\", num_classes=len(train_ds.label_list))\n",
    "# 同样地，通过指定模型名称一键加载对应的Tokenizer，用于处理文本数据，如切分token，转token_id等。\n",
    "tokenizer = SkepTokenizer.from_pretrained(pretrained_model_name_or_path=\"skep_ernie_1.0_large_ch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "`SkepForSequenceClassification`可用于句子级情感分析和目标级情感分析任务。其通过预训练模型SKEP获取输入文本的表示，之后将文本表示进行分类。\n",
    "\n",
    "* `pretrained_model_name_or_path`：模型名称。支持\"skep_ernie_1.0_large_ch\"，\"skep_ernie_2.0_large_en\"。\n",
    "\t- \"skep_ernie_1.0_large_ch\"：是SKEP模型在预训练ernie_1.0_large_ch基础之上在海量中文数据上继续预训练得到的中文预训练模型；\n",
    "    - \"skep_ernie_2.0_large_en\"：是SKEP模型在预训练ernie_2.0_large_en基础之上在海量英文数据上继续预训练得到的英文预训练模型；\n",
    "    \n",
    "* `num_classes`: 数据集分类类别数。\n",
    "\n",
    "\n",
    "关于SKEP模型实现详细信息参考：https://github.com/PaddlePaddle/PaddleNLP/tree/develop/paddlenlp/transformers/skep\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 数据处理\n",
    "\n",
    "同样地，我们需要将原始ChnSentiCorp数据处理成模型可以读入的数据格式。\n",
    "\n",
    "SKEP模型对中文文本处理按照字粒度进行处理，我们可以使用PaddleNLP内置的`SkepTokenizer`完成一键式处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataloader length: 300\n",
      "dev dataloader length: 38\n"
     ]
    }
   ],
   "source": [
    "# 批量数据大小\n",
    "batch_size = 32\n",
    "# 文本序列最大长度\n",
    "max_seq_length = 128\n",
    "\n",
    "# 将数据处理成模型可读入的数据格式\n",
    "trans_func = partial(\n",
    "    convert_example,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=max_seq_length)\n",
    "\n",
    "# 将数据组成批量式数据，如\n",
    "# 将不同长度的文本序列padding到批量式数据中最大长度\n",
    "# 将每条数据label堆叠在一起\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids\n",
    "    Stack()  # labels\n",
    "): [data for data in fn(samples)]\n",
    "train_data_loader = create_dataloader(\n",
    "    train_ds,\n",
    "    mode='train',\n",
    "    batch_size=batch_size,\n",
    "    batchify_fn=batchify_fn,\n",
    "    trans_fn=trans_func)\n",
    "dev_data_loader = create_dataloader(\n",
    "    dev_ds,\n",
    "    mode='dev',\n",
    "    batch_size=batch_size,\n",
    "    batchify_fn=batchify_fn,\n",
    "    trans_fn=trans_func)\n",
    "\n",
    "print(\"train dataloader length:\", len(train_data_loader))\n",
    "print(\"dev dataloader length:\", len(dev_data_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 模型训练和评估\n",
    "\n",
    "\n",
    "定义损失函数、优化器以及评价指标后，即可开始训练。\n",
    "\n",
    "\n",
    "**推荐超参设置：**\n",
    "\n",
    "* `max_seq_length=256`\n",
    "* `batch_size=48`\n",
    "* `learning_rate=2e-5`\n",
    "* `epochs=10`\n",
    "\n",
    "实际运行时可以根据显存大小调整batch_size和max_seq_length大小。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from utils import evaluate\n",
    "\n",
    "# 训练轮次\n",
    "epochs = 5\n",
    "# 训练过程中保存模型参数的文件夹\n",
    "save_dir = \"skep_ckpt1\"\n",
    "# len(train_data_loader)一轮训练所需要的step数\n",
    "num_training_steps = len(train_data_loader) * epochs\n",
    "\n",
    "lr_scheduler = LinearDecayWithWarmup(2e-5, num_training_steps, 0.1)\n",
    "decay_params = [\n",
    "    p.name for n, p in model.named_parameters()\n",
    "    if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "]\n",
    "# Adam优化器\n",
    "optimizer = paddle.optimizer.AdamW(\n",
    "    learning_rate=lr_scheduler,\n",
    "    parameters=model.parameters(),\n",
    "    weight_decay=5e-4,\n",
    "    apply_decay_param_fun=lambda x: x in decay_params)\n",
    "# 交叉熵损失函数\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()\n",
    "# accuracy评价指标\n",
    "metric = paddle.metric.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 接下来，开始正式训练模型，训练时间较长，可注释掉这部分\r\n",
    "patience = 20\r\n",
    "# 加入日志显示\r\n",
    "from visualdl import LogWriter\r\n",
    "writer = LogWriter(save_dir)\r\n",
    "\r\n",
    "train_model(model, optimizer, epochs, criterion, metric, save_dir, tokenizer, loader_list=[train_data_loader, dev_data_loader], patience=patience, lr_scheduler=lr_scheduler, writer=writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 预测提交结果\n",
    "\n",
    "\n",
    "使用训练得到的模型还可以对文本进行情感预测。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import paddle\n",
    "# 处理测试集数据\n",
    "trans_func = partial(\n",
    "    convert_example,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=max_seq_length,\n",
    "    is_test=True)\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # segment\n",
    "    Stack() # qid\n",
    "): [data for data in fn(samples)]\n",
    "test_data_loader = create_dataloader(\n",
    "    test_ds,\n",
    "    mode='test',\n",
    "    batch_size=batch_size,\n",
    "    batchify_fn=batchify_fn,\n",
    "    trans_fn=trans_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded parameters from skep_ckpt/model_1400/model_state.pdparams\n"
     ]
    }
   ],
   "source": [
    "# 根据实际运行情况，更换加载的参数路径\n",
    "params_path = 'skep_ckpt/model_1400/model_state.pdparams'\n",
    "if params_path and os.path.isfile(params_path):\n",
    "    # 加载模型参数\n",
    "    state_dict = paddle.load(params_path)\n",
    "    model.set_dict(state_dict)\n",
    "    print(\"Loaded parameters from %s\" % params_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_map = {0: '0', 1: '1'}\n",
    "results = predict(model, test_data_loader, label_map, has_qids=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res_dir = \"./results\"\n",
    "if not os.path.exists(res_dir):\n",
    "    os.makedirs(res_dir)\n",
    "# 写入预测结果\n",
    "with open(os.path.join(res_dir, \"ChnSentiCorp.tsv\"), 'w', encoding=\"utf8\") as f:\n",
    "    f.write(\"index\\tprediction\\n\")\n",
    "    for qid, label in results:\n",
    "        f.write(str(qid[0])+\"\\t\"+label+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Part C.2 目标级情感分析\n",
    "\n",
    "在电商产品分析场景下，除了分析整体商品的情感极性外，还细化到以商品具体的“方面”为分析主体进行情感分析（aspect-level），如下、：\n",
    "\n",
    "* 这个薯片口味有点咸，太辣了，不过口感很脆。\n",
    "\n",
    "关于薯片的**口味方面**是一个负向评价（咸，太辣），然而对于**口感方面**却是一个正向评价（很脆）。\n",
    "\n",
    "* 我很喜欢夏威夷，就是这边的海鲜太贵了。\n",
    "\n",
    "关于**夏威夷**是一个正向评价（喜欢），然而对于**夏威夷的海鲜**却是一个负向评价（价格太贵）。\n",
    "\n",
    "\n",
    "\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/052d46409ba3451693a718552b968d188fa4677235bc43ddbc15fe11ad3b57b1\" width=\"600\" ></center>\n",
    "<br><center>目标级情感分析任务</center></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 常用数据集\n",
    "\n",
    "[千言数据集](https://www.luge.ai/)已提供了许多任务常用数据集。\n",
    "其中情感分析数据集下载链接：https://aistudio.baidu.com/aistudio/competition/detail/50/?isFromLUGE=TRUE\n",
    "\n",
    "SE-ABSA16_PHNS数据集是关于手机的目标级情感分析数据集。PaddleNLP已经内置了该数据集，加载方式，如下：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset length: 1336\n",
      "test dataset length: 529\n",
      "{'text': 'phone#design_features', 'text_pair': '今天有幸拿到了港版白色iPhone 5真机，试玩了一下，说说感受吧：1. 真机尺寸宽度与4/4s保持一致没有变化，长度多了大概一厘米，也就是之前所说的多了一排的图标。2. 真机重量比上一代轻了很多，个人感觉跟i9100的重量差不多。（用惯上一代的朋友可能需要一段时间适应了）3. 由于目前还没有版的SIM卡，无法插卡使用，有购买的朋友要注意了，并非简单的剪卡就可以用，而是需要去运营商更换新一代的SIM卡。4. 屏幕显示效果确实比上一代有进步，不论是从清晰度还是不同角度的视角，iPhone 5绝对要更上一层，我想这也许是相对上一代最有意义的升级了。5. 新的数据接口更小，比上一代更好用更方便，使用的过程会有这样的体会。6. 从简单的几个操作来讲速度比4s要快，这个不用测试软件也能感受出来，比如程序的调用以及照片的拍摄和浏览。不过，目前水货市场上坑爹的价格，最好大家可以再观望一下，不要急着出手。', 'label': 1}\n",
      "{'text': 'display#quality', 'text_pair': '今天有幸拿到了港版白色iPhone 5真机，试玩了一下，说说感受吧：1. 真机尺寸宽度与4/4s保持一致没有变化，长度多了大概一厘米，也就是之前所说的多了一排的图标。2. 真机重量比上一代轻了很多，个人感觉跟i9100的重量差不多。（用惯上一代的朋友可能需要一段时间适应了）3. 由于目前还没有版的SIM卡，无法插卡使用，有购买的朋友要注意了，并非简单的剪卡就可以用，而是需要去运营商更换新一代的SIM卡。4. 屏幕显示效果确实比上一代有进步，不论是从清晰度还是不同角度的视角，iPhone 5绝对要更上一层，我想这也许是相对上一代最有意义的升级了。5. 新的数据接口更小，比上一代更好用更方便，使用的过程会有这样的体会。6. 从简单的几个操作来讲速度比4s要快，这个不用测试软件也能感受出来，比如程序的调用以及照片的拍摄和浏览。不过，目前水货市场上坑爹的价格，最好大家可以再观望一下，不要急着出手。', 'label': 1}\n",
      "{'text': 'ports#connectivity', 'text_pair': '今天有幸拿到了港版白色iPhone 5真机，试玩了一下，说说感受吧：1. 真机尺寸宽度与4/4s保持一致没有变化，长度多了大概一厘米，也就是之前所说的多了一排的图标。2. 真机重量比上一代轻了很多，个人感觉跟i9100的重量差不多。（用惯上一代的朋友可能需要一段时间适应了）3. 由于目前还没有版的SIM卡，无法插卡使用，有购买的朋友要注意了，并非简单的剪卡就可以用，而是需要去运营商更换新一代的SIM卡。4. 屏幕显示效果确实比上一代有进步，不论是从清晰度还是不同角度的视角，iPhone 5绝对要更上一层，我想这也许是相对上一代最有意义的升级了。5. 新的数据接口更小，比上一代更好用更方便，使用的过程会有这样的体会。6. 从简单的几个操作来讲速度比4s要快，这个不用测试软件也能感受出来，比如程序的调用以及照片的拍摄和浏览。不过，目前水货市场上坑爹的价格，最好大家可以再观望一下，不要急着出手。', 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "train_ds, test_ds = load_dataset(\"seabsa16\", \"phns\", splits=[\"train\", \"test\"])\n",
    "\n",
    "print(\"train dataset length:\", len(train_ds))\n",
    "print(\"test dataset length:\", len(test_ds))\n",
    "print(train_ds[0])\n",
    "print(train_ds[1])\n",
    "print(train_ds[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'data.BAIDUData'>\n",
      "train dataset length: 1069\n",
      "val dataset length: 267\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\r\n",
    "length = len(train_ds)\r\n",
    "train_list = train_ds[:]\r\n",
    "random.shuffle(train_list)\r\n",
    "val_len = int(length*0.2)\r\n",
    "val_l = train_list[:val_len]\r\n",
    "train_l = train_list[val_len:]\r\n",
    "train_ds, dev_ds = load_my_dataset(splits=[\"train\", \"dev\"], SPLITS={'train':None, 'dev':None}, data_list={'train':train_l, 'dev':val_l})\r\n",
    "print(\"train dataset length:\", len(train_ds))\r\n",
    "print(\"val dataset length:\", len(dev_ds))\r\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### SKEP模型加载\n",
    "\n",
    "目标级情感分析模型同样使用`SkepForSequenceClassification`模型，但目标级情感分析模型的输入不单单是一个句子，而是句对。一个句子描述“评价对象方面（aspect）”，另一个句子描述\"对该方面的评论\"。如下图所示。\n",
    "\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/1a4b76447dae404caa3bf123ea28e375179cb09a02de4bef8a2f172edc6e3c8f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 指定模型名称一键加载模型\n",
    "model = SkepForSequenceClassification.from_pretrained(\n",
    "    'skep_ernie_1.0_large_ch', num_classes=len(train_ds.label_list), dropout=0.3)\n",
    "# 指定模型名称一键加载tokenizer\n",
    "tokenizer = SkepTokenizer.from_pretrained('skep_ernie_1.0_large_ch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 数据处理\n",
    "\n",
    "同样地，我们需要将原始SE_ABSA16_PHNS数据处理成模型可以读入的数据格式。\n",
    "\n",
    "SKEP模型对中文文本处理按照字粒度进行处理，我们可以使用PaddleNLP内置的`SkepTokenizer`完成一键式处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataloader length: 167\n"
     ]
    }
   ],
   "source": [
    "# 处理的最大文本序列长度\n",
    "max_seq_length=512\n",
    "# 批量数据大小\n",
    "batch_size=8\n",
    "\n",
    "# 将数据处理成model可读入的数据格式\n",
    "trans_func = partial(\n",
    "    convert_example,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=max_seq_length,\n",
    "    text_pair=True)\n",
    "# 将数据组成批量式数据，如\n",
    "# 将不同长度的文本序列padding到批量式数据中最大长度\n",
    "# 将每条数据label堆叠在一起\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids\n",
    "    Stack(dtype=\"int64\")  # labels\n",
    "): [data for data in fn(samples)]\n",
    "train_data_loader = create_dataloader(\n",
    "    train_ds,\n",
    "    mode='train',\n",
    "    batch_size=batch_size,\n",
    "    batchify_fn=batchify_fn,\n",
    "    trans_fn=trans_func)\n",
    "\n",
    "\"\"\"\n",
    "dev_data_loader = create_dataloader(\n",
    "    dev_ds,\n",
    "    mode='dev',\n",
    "    batch_size=batch_size,\n",
    "    batchify_fn=batchify_fn,\n",
    "    trans_fn=trans_func)\"\"\"\n",
    "print(\"train dataloader length:\", len(train_data_loader))\n",
    "#print(\"dev dataloader length:\", len(dev_data_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 模型训练\n",
    "\n",
    "定义损失函数、优化器以及评价指标后，即可开始训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 训练轮次\n",
    "epochs = 10\n",
    "# 总共需要训练的step数\n",
    "num_training_steps = len(train_data_loader) * epochs\n",
    "lr_scheduler = LinearDecayWithWarmup(2e-5, num_training_steps, 0.1)\n",
    "decay_params = [\n",
    "    p.name for n, p in model.named_parameters()\n",
    "    if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "]\n",
    "# Adam优化器\n",
    "optimizer = paddle.optimizer.AdamW(\n",
    "    learning_rate=lr_scheduler,\n",
    "    parameters=model.parameters(),\n",
    "    weight_decay=0.001,\n",
    "    apply_decay_param_fun=lambda x: x in decay_params)\n",
    "# 交叉熵损失\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()\n",
    "# Accuracy评价指标\n",
    "metric = paddle.metric.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 开启训练\n",
    "save_dir = \"skep_aspect2\"\n",
    "patience = 20\n",
    "# 加入日志显示\n",
    "from visualdl import LogWriter\n",
    "writer = LogWriter(save_dir)\n",
    "\n",
    "train_model(model, optimizer, epochs, criterion, metric, save_dir, tokenizer, loader_list=[train_data_loader, None], patience=patience, lr_scheduler=lr_scheduler, writer=writer, save_freq=len(train_data_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 预测提交结果\n",
    "\n",
    "使用训练得到的模型还可以对评价对象进行情感预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 处理测试集数据\n",
    "label_map = {0: '0', 1: '1'}\n",
    "trans_func = partial(\n",
    "    convert_example,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=max_seq_length,\n",
    "    is_test=True,\n",
    "    text_pair=True)\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids\n",
    "): [data for data in fn(samples)]\n",
    "test_data_loader = create_dataloader(\n",
    "    test_ds,\n",
    "    mode='test',\n",
    "    batch_size=batch_size,\n",
    "    batchify_fn=batchify_fn,\n",
    "    trans_fn=trans_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded parameters from skep_aspect2/model_1670/model_state.pdparams\n"
     ]
    }
   ],
   "source": [
    "# 根据实际运行情况，更换加载的参数路径\n",
    "params_path = 'skep_aspect2/model_1670/model_state.pdparams'\n",
    "if params_path and os.path.isfile(params_path):\n",
    "    # 加载模型参数\n",
    "    state_dict = paddle.load(params_path)\n",
    "    model.set_dict(state_dict)\n",
    "    print(\"Loaded parameters from %s\" % params_path)\n",
    "\n",
    "results = predict(model, test_data_loader, label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 写入预测结果\n",
    "with open(os.path.join(\"results\", \"SE-ABSA16_PHNS.tsv\"), 'w', encoding=\"utf8\") as f:\n",
    "    f.write(\"index\\tprediction\\n\")\n",
    "    for idx, label in enumerate(results):\n",
    "        f.write(str(idx)+\"\\t\"+label+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 拓展\n",
    "## NLPCC14-SC（句子级情感分类）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'data.BAIDUData'>\n",
      "train dataset length: 10000\n",
      "test dataset length: 2500\n",
      "{'text': '请问这机不是有个遥控器的吗？', 'label': 1, 'qid': ''}\n",
      "{'text': '发短信特别不方便！背后的屏幕很大用起来不舒服，是手触屏的！切换屏幕很麻烦！', 'label': 1, 'qid': ''}\n",
      "{'text': '手感超好，而且黑色相比白色在转得时候不容易眼花，找童年的记忆啦。', 'label': 1, 'qid': ''}\n"
     ]
    }
   ],
   "source": [
    "train_ds, test_ds = load_my_dataset(splits=[\"train\", \"test\"], SPLITS={'train':'data/NLPCC14-SC/train.tsv', 'test':'data/NLPCC14-SC/test.tsv'}, text_pair=False, head=True, is_transpose=True, has_qid=True)\r\n",
    "\r\n",
    "print(\"train dataset length:\", len(train_ds))\r\n",
    "print(\"test dataset length:\", len(test_ds))\r\n",
    "print(train_ds[0])\r\n",
    "print(train_ds[1])\r\n",
    "print(train_ds[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'data.BAIDUData'>\n",
      "train dataset length: 8000\n",
      "val dataset length: 2000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\r\n",
    "length = len(train_ds)\r\n",
    "train_list = train_ds[:]\r\n",
    "random.shuffle(train_list)\r\n",
    "val_len = int(length*0.2)\r\n",
    "val_l = train_list[:val_len]\r\n",
    "train_l = train_list[val_len:]\r\n",
    "train_ds, dev_ds = load_my_dataset(splits=[\"train\", \"dev\"], SPLITS={'train':None, 'dev':None}, data_list={'train':train_l, 'dev':val_l})\r\n",
    "print(\"train dataset length:\", len(train_ds))\r\n",
    "print(\"val dataset length:\", len(dev_ds))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 指定模型名称，一键加载模型\n",
    "model = SkepForSequenceClassification.from_pretrained(pretrained_model_name_or_path=\"skep_ernie_1.0_large_ch\", num_classes=len(train_ds.label_list))\n",
    "# 同样地，通过指定模型名称一键加载对应的Tokenizer，用于处理文本数据，如切分token，转token_id等。\n",
    "tokenizer = SkepTokenizer.from_pretrained(pretrained_model_name_or_path=\"skep_ernie_1.0_large_ch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataloader length: 500\n",
      "dev dataloader length: 125\n"
     ]
    }
   ],
   "source": [
    "# 批量数据大小\n",
    "batch_size = 16\n",
    "# 文本序列最大长度\n",
    "max_seq_length = 256\n",
    "\n",
    "# 将数据处理成模型可读入的数据格式\n",
    "trans_func = partial(\n",
    "    convert_example,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=max_seq_length)\n",
    "\n",
    "# 将数据组成批量式数据，如\n",
    "# 将不同长度的文本序列padding到批量式数据中最大长度\n",
    "# 将每条数据label堆叠在一起\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids\n",
    "    Stack()  # labels\n",
    "): [data for data in fn(samples)]\n",
    "train_data_loader = create_dataloader(\n",
    "    train_ds,\n",
    "    mode='train',\n",
    "    batch_size=batch_size,\n",
    "    batchify_fn=batchify_fn,\n",
    "    trans_fn=trans_func)\n",
    "dev_data_loader = create_dataloader(\n",
    "    dev_ds,\n",
    "    mode='dev',\n",
    "    batch_size=batch_size,\n",
    "    batchify_fn=batchify_fn,\n",
    "    trans_fn=trans_func)\n",
    "\n",
    "print(\"train dataloader length:\", len(train_data_loader))\n",
    "print(\"dev dataloader length:\", len(dev_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from utils import evaluate\n",
    "\n",
    "# 训练轮次\n",
    "epochs = 10\n",
    "# 训练过程中保存模型参数的文件夹\n",
    "save_dir = \"skep_nlpcc\"\n",
    "# len(train_data_loader)一轮训练所需要的step数\n",
    "num_training_steps = len(train_data_loader) * epochs\n",
    "\n",
    "lr_scheduler = LinearDecayWithWarmup(2e-5, num_training_steps, 0.1)\n",
    "decay_params = [\n",
    "    p.name for n, p in model.named_parameters()\n",
    "    if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "]\n",
    "# Adam优化器\n",
    "optimizer = paddle.optimizer.AdamW(\n",
    "    learning_rate=lr_scheduler,\n",
    "    parameters=model.parameters(),\n",
    "    weight_decay=5e-4,\n",
    "    apply_decay_param_fun=lambda x: x in decay_params)\n",
    "# 交叉熵损失函数\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()\n",
    "# accuracy评价指标\n",
    "metric = paddle.metric.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 接下来，开始正式训练模型，训练时间较长，可注释掉这部分\r\n",
    "patience = 20\r\n",
    "# 加入日志显示\r\n",
    "from visualdl import LogWriter\r\n",
    "writer = LogWriter(save_dir)\r\n",
    "\r\n",
    "train_model(model, optimizer, epochs, criterion, metric, save_dir, tokenizer, loader_list=[train_data_loader, dev_data_loader], patience=patience, lr_scheduler=lr_scheduler, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import paddle\n",
    "# 处理测试集数据\n",
    "trans_func = partial(\n",
    "    convert_example,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=max_seq_length,\n",
    "    is_test=True)\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # segment\n",
    "    Stack() # qid\n",
    "): [data for data in fn(samples)]\n",
    "test_data_loader = create_dataloader(\n",
    "    test_ds,\n",
    "    mode='test',\n",
    "    batch_size=batch_size,\n",
    "    batchify_fn=batchify_fn,\n",
    "    trans_fn=trans_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded parameters from skep_nlpcc/best_model_state.pdparams\n"
     ]
    }
   ],
   "source": [
    "# 根据实际运行情况，更换加载的参数路径\n",
    "params_path = 'skep_nlpcc/best_model_state.pdparams'\n",
    "if params_path and os.path.isfile(params_path):\n",
    "    # 加载模型参数\n",
    "    state_dict = paddle.load(params_path)\n",
    "    model.set_dict(state_dict)\n",
    "    print(\"Loaded parameters from %s\" % params_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_map = {0: '0', 1: '1'}\n",
    "results = predict(model, test_data_loader, label_map, has_qids=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res_dir = \"./results\"\n",
    "if not os.path.exists(res_dir):\n",
    "    os.makedirs(res_dir)\n",
    "# 写入预测结果\n",
    "with open(os.path.join(res_dir, \"NLPCC14-SC.tsv\"), 'w', encoding=\"utf8\") as f:\n",
    "    f.write(\"index\\tprediction\\n\")\n",
    "    for qid, label in results:\n",
    "        f.write(str(qid[0])+\"\\t\"+label+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## SE-ABSA16_CAME（评价对象级情感分类）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset length: 1317\n",
      "test dataset length: 505\n",
      "{'text': 'camera#design_features', 'text_pair': '千呼万唤始出来，尼康的APSC小相机终于发布了，COOLPIX A. 你怎么看呢？我看，尼康是挤牙膏挤惯了啊，1，外观既没有V1时尚，也没P7100专业，反而类似P系列。2，CMOS炒冷饭。3，OVF没有任何提示和显示。（除了框框)4，28MM镜头是不错，可是F2.8定焦也太小气了。5，电池坑爹，用D800和V1的电池很难吗？6，考虑到1100美元的定价，富士X100S表示很欢乐。***好处是，可以确定，尼康会继续大力发展1系列了***另外体积比X100S小也算是A的优势吧***。等2014年年中跌倒1900左右的时候就可以入手了。', 'label': 0}\n",
      "{'text': 'camera#operation_performance', 'text_pair': '千呼万唤始出来，尼康的APSC小相机终于发布了，COOLPIX A. 你怎么看呢？我看，尼康是挤牙膏挤惯了啊，1，外观既没有V1时尚，也没P7100专业，反而类似P系列。2，CMOS炒冷饭。3，OVF没有任何提示和显示。（除了框框)4，28MM镜头是不错，可是F2.8定焦也太小气了。5，电池坑爹，用D800和V1的电池很难吗？6，考虑到1100美元的定价，富士X100S表示很欢乐。***好处是，可以确定，尼康会继续大力发展1系列了***另外体积比X100S小也算是A的优势吧***。等2014年年中跌倒1900左右的时候就可以入手了。', 'label': 0}\n",
      "{'text': 'hardware#usability', 'text_pair': '千呼万唤始出来，尼康的APSC小相机终于发布了，COOLPIX A. 你怎么看呢？我看，尼康是挤牙膏挤惯了啊，1，外观既没有V1时尚，也没P7100专业，反而类似P系列。2，CMOS炒冷饭。3，OVF没有任何提示和显示。（除了框框)4，28MM镜头是不错，可是F2.8定焦也太小气了。5，电池坑爹，用D800和V1的电池很难吗？6，考虑到1100美元的定价，富士X100S表示很欢乐。***好处是，可以确定，尼康会继续大力发展1系列了***另外体积比X100S小也算是A的优势吧***。等2014年年中跌倒1900左右的时候就可以入手了。', 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "train_ds, test_ds = load_my_dataset(splits=[\"train\", \"test\"], SPLITS={'train':'data/SE-ABSA16_CAME/train.tsv', 'test':'data/SE-ABSA16_CAME/test.tsv'}, text_pair=True, head=True, is_transpose=True, has_qid=True)\r\n",
    "\r\n",
    "print(\"train dataset length:\", len(train_ds))\r\n",
    "print(\"test dataset length:\", len(test_ds))\r\n",
    "print(train_ds[0])\r\n",
    "print(train_ds[1])\r\n",
    "print(train_ds[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'data.BAIDUData'>\n",
      "train dataset length: 1054\n",
      "val dataset length: 263\n"
     ]
    }
   ],
   "source": [
    "\"\"\"length = len(train_ds)\r\n",
    "train_list = train_ds[:]\r\n",
    "random.shuffle(train_list)\r\n",
    "val_len = int(length*0.2)\r\n",
    "val_l = train_list[:val_len]\r\n",
    "train_l = train_list[val_len:]\r\n",
    "train_ds, dev_ds = load_my_dataset(splits=[\"train\", \"dev\"], SPLITS={'train':None, 'dev':None}, data_list={'train':train_l, 'dev':val_l})\r\n",
    "print(\"train dataset length:\", len(train_ds))\r\n",
    "print(\"val dataset length:\", len(dev_ds))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 指定模型名称，一键加载模型\n",
    "model = SkepForSequenceClassification.from_pretrained(pretrained_model_name_or_path=\"skep_ernie_1.0_large_ch\", num_classes=len(train_ds.label_list), dropout=0.3)\n",
    "# 同样地，通过指定模型名称一键加载对应的Tokenizer，用于处理文本数据，如切分token，转token_id等。\n",
    "tokenizer = SkepTokenizer.from_pretrained(pretrained_model_name_or_path=\"skep_ernie_1.0_large_ch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataloader length: 165\n"
     ]
    }
   ],
   "source": [
    "# 批量数据大小\n",
    "batch_size = 8\n",
    "# 文本序列最大长度\n",
    "max_seq_length = 512\n",
    "\n",
    "# 将数据处理成模型可读入的数据格式\n",
    "trans_func = partial(\n",
    "    convert_example,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=max_seq_length,\n",
    "    text_pair=True)\n",
    "\n",
    "# 将数据组成批量式数据，如\n",
    "# 将不同长度的文本序列padding到批量式数据中最大长度\n",
    "# 将每条数据label堆叠在一起\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids\n",
    "    Stack()  # labels\n",
    "): [data for data in fn(samples)]\n",
    "train_data_loader = create_dataloader(\n",
    "    train_ds,\n",
    "    mode='train',\n",
    "    batch_size=batch_size,\n",
    "    batchify_fn=batchify_fn,\n",
    "    trans_fn=trans_func)\n",
    "\"\"\"\n",
    "dev_data_loader = create_dataloader(\n",
    "    dev_ds,\n",
    "    mode='dev',\n",
    "    batch_size=batch_size,\n",
    "    batchify_fn=batchify_fn,\n",
    "    trans_fn=trans_func)\n",
    "\"\"\"\n",
    "print(\"train dataloader length:\", len(train_data_loader))\n",
    "#print(\"dev dataloader length:\", len(dev_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 训练轮次\n",
    "epochs = 10\n",
    "# 总共需要训练的step数\n",
    "num_training_steps = len(train_data_loader) * epochs\n",
    "lr_scheduler = LinearDecayWithWarmup(2e-5, num_training_steps, 0.1)\n",
    "decay_params = [\n",
    "    p.name for n, p in model.named_parameters()\n",
    "    if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "]\n",
    "# Adam优化器\n",
    "optimizer = paddle.optimizer.AdamW(\n",
    "    learning_rate=lr_scheduler,\n",
    "    parameters=model.parameters(),\n",
    "    weight_decay=0.001,\n",
    "    apply_decay_param_fun=lambda x: x in decay_params)\n",
    "# 交叉熵损失\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()\n",
    "# Accuracy评价指标\n",
    "metric = paddle.metric.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 开启训练\n",
    "save_dir = \"skep_seabsa_came1\"\n",
    "patience = 10\n",
    "# 加入日志显示\n",
    "from visualdl import LogWriter\n",
    "writer = LogWriter(save_dir)\n",
    "\n",
    "train_model(model, optimizer, epochs, criterion, metric, save_dir, tokenizer, loader_list=[train_data_loader, None], patience=patience, lr_scheduler=lr_scheduler, writer=writer, save_freq=len(train_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 处理测试集数据\n",
    "label_map = {0: '0', 1: '1'}\n",
    "trans_func = partial(\n",
    "    convert_example,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=max_seq_length,\n",
    "    is_test=True,\n",
    "    text_pair=True)\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids\n",
    "): [data for data in fn(samples)]\n",
    "test_data_loader = create_dataloader(\n",
    "    test_ds,\n",
    "    mode='test',\n",
    "    batch_size=batch_size,\n",
    "    batchify_fn=batchify_fn,\n",
    "    trans_fn=trans_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded parameters from skep_seabsa_came1/model_1650/model_state.pdparams\n"
     ]
    }
   ],
   "source": [
    "# 根据实际运行情况，更换加载的参数路径\n",
    "params_path = 'skep_seabsa_came1/model_1650/model_state.pdparams'\n",
    "if params_path and os.path.isfile(params_path):\n",
    "    # 加载模型参数\n",
    "    state_dict = paddle.load(params_path)\n",
    "    model.set_dict(state_dict)\n",
    "    print(\"Loaded parameters from %s\" % params_path)\n",
    "\n",
    "results = predict(model, test_data_loader, label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 写入预测结果\n",
    "with open(os.path.join(\"results\", \"SE-ABSA16_CAME.tsv\"), 'w', encoding=\"utf8\") as f:\n",
    "    f.write(\"index\\tprediction\\n\")\n",
    "    for idx, label in enumerate(results):\n",
    "        f.write(str(idx)+\"\\t\"+label+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## COTE-BD（观点抽取）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from paddlenlp.datasets import load_dataset\r\n",
    "from paddlenlp.transformers import SkepForSequenceClassification, SkepTokenizer, SkepForTokenClassification, ErnieGramForTokenClassification, ErnieGramTokenizer\r\n",
    "import os\r\n",
    "from functools import partial\r\n",
    "from paddlenlp.transformers import LinearDecayWithWarmup\r\n",
    "import random\r\n",
    "import numpy as np\r\n",
    "import paddle\r\n",
    "import paddle.nn.functional as F\r\n",
    "from paddlenlp.metrics import ChunkEvaluator\r\n",
    "from paddlenlp.data import Stack, Tuple, Pad\r\n",
    "\r\n",
    "from utils import create_dataloader, convert_example, predict, token_predict\r\n",
    "from train import train_token_model\r\n",
    "from data import load_my_dataset, TokenData, convert_token_example\r\n",
    "def seed_paddle(seed=2021):\r\n",
    "    seed = int(seed)\r\n",
    "    random.seed(seed)\r\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\r\n",
    "    np.random.seed(seed)\r\n",
    "    paddle.seed(seed)\r\n",
    "\r\n",
    "seed_paddle(seed=2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset length: 8533\n",
      "test dataset length: 3658\n",
      "{'text': '芝罘岛骑车去过几次，它挺壮观的，毕竟是我国典型的也是最大的陆连岛咯!我喜欢去那儿，反正全岛免费咯啊哈哈哈！风景的确不错而且海水也很干净，有些地方还是军事管理，禁地来着，但是我认识军官。', 'text_label': '芝罘岛'}\n",
      "{'text': '泽拉图（1865-2506）是暴雪娱乐开发的即时战略游戏星际争霸中的星灵角色。', 'text_label': '泽拉图'}\n",
      "{'text': '《鸟人》一书以鸟博士的遭遇作为主线，主要写了鸟博士从校园出来后的种种荒诞经历。', 'text_label': '鸟人'}\n"
     ]
    }
   ],
   "source": [
    "train_ds, test_ds = load_my_dataset(splits=[\"train\", \"test\"], SPLITS={'train':'data/COTE-BD/train.tsv', 'test':'data/COTE-BD/test.tsv'}, is_token=True)\r\n",
    "print(\"train dataset length:\", len(train_ds))\r\n",
    "print(\"test dataset length:\", len(test_ds))\r\n",
    "print(train_ds[0])\r\n",
    "print(train_ds[1])\r\n",
    "print(train_ds[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-21 14:39:16,190] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/ernie-gram-zh/ernie_gram_zh.pdparams\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.weight. classifier.weight is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.bias. classifier.bias is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n",
      "[2021-06-21 14:39:26,044] [    INFO] - Found /home/aistudio/.paddlenlp/models/ernie-gram-zh/vocab.txt\n"
     ]
    }
   ],
   "source": [
    "# 指定模型名称，一键加载模型\n",
    "#model = SkepForTokenClassification.from_pretrained(pretrained_model_name_or_path=\"skep_ernie_1.0_large_ch\", num_classes=len(train_ds.label_list))\n",
    "# 同样地，通过指定模型名称一键加载对应的Tokenizer，用于处理文本数据，如切分token，转token_id等。\n",
    "#tokenizer = SkepTokenizer.from_pretrained(pretrained_model_name_or_path=\"skep_ernie_1.0_large_ch\")\n",
    "model = ErnieGramForTokenClassification.from_pretrained(pretrained_model_name_or_path=\"ernie-gram-zh\", num_classes=3)\n",
    "# 同样地，通过指定模型名称一键加载对应的Tokenizer，用于处理文本数据，如切分token，转token_id等。\n",
    "tokenizer = ErnieGramTokenizer.from_pretrained(pretrained_model_name_or_path=\"ernie-gram-zh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataloader length: 267\n"
     ]
    }
   ],
   "source": [
    "# 批量数据大小\n",
    "batch_size = 32\n",
    "# 文本序列最大长度\n",
    "max_seq_len = 512\n",
    "ignore_label = -1\n",
    "label_vocab = TokenData().label_dict\n",
    "\n",
    "# 将数据处理成模型可读入的数据格式\n",
    "trans_func = partial(convert_token_example, tokenizer=tokenizer, label_vocab=label_vocab, max_seq_len=max_seq_len)\n",
    "\n",
    "# 将数据组成批量式数据，如\n",
    "# 将不同长度的文本序列padding到批量式数据中最大长度\n",
    "# 将每条数据label堆叠在一起\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids\n",
    "    Stack(), # seq_len\n",
    "    Pad(axis=0, pad_val=ignore_label)  # labels\n",
    "): [data for data in fn(samples)]\n",
    "train_data_loader = create_dataloader(\n",
    "    train_ds,\n",
    "    mode='train',\n",
    "    batch_size=batch_size,\n",
    "    batchify_fn=batchify_fn,\n",
    "    trans_fn=trans_func)\n",
    "\"\"\"dev_data_loader = create_dataloader(\n",
    "    dev_ds,\n",
    "    mode='dev',\n",
    "    batch_size=batch_size,\n",
    "    batchify_fn=batchify_fn,\n",
    "    trans_fn=trans_func)\"\"\"\n",
    "\n",
    "print(\"train dataloader length:\", len(train_data_loader))\n",
    "#print(\"dev dataloader length:\", len(dev_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 训练轮次\n",
    "epochs = 20\n",
    "# 总共需要训练的step数\n",
    "num_training_steps = len(train_data_loader) * epochs\n",
    "lr_scheduler = LinearDecayWithWarmup(5e-5, num_training_steps, 0.1)\n",
    "decay_params = [\n",
    "    p.name for n, p in model.named_parameters()\n",
    "    if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "]\n",
    "# Adam优化器\n",
    "optimizer = paddle.optimizer.AdamW(\n",
    "    learning_rate=lr_scheduler,\n",
    "    parameters=model.parameters(),\n",
    "    weight_decay=0.001,\n",
    "    apply_decay_param_fun=lambda x: x in decay_params)\n",
    "# 交叉熵损失\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss(ignore_index=ignore_label)\n",
    "# Accuracy评价指标\n",
    "metric = ChunkEvaluator(label_list=label_vocab.keys(), suffix=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 开启训练\n",
    "save_dir = \"eg_cotebd\"\n",
    "patience = 10\n",
    "# 加入日志显示\n",
    "from visualdl import LogWriter\n",
    "writer = LogWriter(save_dir)\n",
    "\n",
    "train_token_model(model, optimizer, epochs, criterion, metric, save_dir, tokenizer, loader_list=[train_data_loader, None], patience=patience, lr_scheduler=lr_scheduler, writer=writer, save_freq=267)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 处理测试集数据\n",
    "trans_func = partial(convert_token_example, tokenizer=tokenizer, label_vocab=label_vocab, max_seq_len=max_seq_len, is_test=True)\n",
    "test_ds.map(trans_func)\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids\n",
    "    Stack(), # seq_len\n",
    "    Stack() # qid\n",
    "): [data for data in fn(samples)]\n",
    "test_data_loader = paddle.io.DataLoader(\n",
    "    dataset=test_ds,\n",
    "    batch_size=batch_size,\n",
    "    return_list=True,\n",
    "    shuffle=False,\n",
    "    collate_fn=batchify_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 根据实际运行情况，更换加载的参数路径\n",
    "params_path = 'eg_cotebd/final_model/model_state.pdparams'\n",
    "if params_path and os.path.isfile(params_path):\n",
    "    # 加载模型参数\n",
    "    state_dict = paddle.load(params_path)\n",
    "    model.set_dict(state_dict)\n",
    "    print(\"Loaded parameters from %s\" % params_path)\n",
    "\n",
    "results = token_predict(model, test_data_loader, label_vocab, tokenizer)\n",
    "#print(len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 写入预测结果\n",
    "import re\n",
    "with open(os.path.join(\"results\", \"COTE_BD.tsv\"), 'w', encoding=\"utf8\") as f:\n",
    "    f.write(\"index\\tprediction\\n\")\n",
    "    for idx, label in enumerate(results):\n",
    "        qid = test_ds.data[idx]['qid']\n",
    "        label = [re.sub('##', '', l) for l in label]\n",
    "        label = [re.sub('[UNK]', '', l) for l in label]\n",
    "        f.write(qid+\"\\t\"+ '\\x01'.join(label)+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## COTE-DP（观点抽取）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from paddlenlp.datasets import load_dataset\r\n",
    "from paddlenlp.transformers import SkepForSequenceClassification, SkepTokenizer, SkepForTokenClassification, ErnieGramForTokenClassification, ErnieGramTokenizer\r\n",
    "import os\r\n",
    "from functools import partial\r\n",
    "from paddlenlp.transformers import LinearDecayWithWarmup\r\n",
    "import random\r\n",
    "import numpy as np\r\n",
    "import paddle\r\n",
    "import paddle.nn.functional as F\r\n",
    "from paddlenlp.metrics import ChunkEvaluator\r\n",
    "from paddlenlp.data import Stack, Tuple, Pad\r\n",
    "\r\n",
    "from utils import create_dataloader, convert_example, predict, token_predict\r\n",
    "from train import train_token_model\r\n",
    "from data import load_my_dataset, TokenData, convert_token_example\r\n",
    "def seed_paddle(seed=2021):\r\n",
    "    seed = int(seed)\r\n",
    "    random.seed(seed)\r\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\r\n",
    "    np.random.seed(seed)\r\n",
    "    paddle.seed(seed)\r\n",
    "\r\n",
    "seed_paddle(seed=2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset length: 25258\n",
      "test dataset length: 10825\n",
      "{'text': '重庆老灶火锅还是很赞的，有机会可以尝试一下！', 'text_label': '重庆老灶火锅'}\n",
      "{'text': '一入店内，就看到招牌特别大的炉鱼来了，餐桌上还摆了五颜六色的小蜡烛，挺有调调的。', 'text_label': '炉鱼来了'}\n",
      "{'text': '只能说是聚餐圣地外婆家一个需要提前来取号的地方。', 'text_label': '外婆家'}\n"
     ]
    }
   ],
   "source": [
    "train_ds, test_ds = load_my_dataset(splits=[\"train\", \"test\"], SPLITS={'train':'data/COTE-DP/train.tsv', 'test':'data/COTE-DP/test.tsv'}, is_token=True)\r\n",
    "print(\"train dataset length:\", len(train_ds))\r\n",
    "print(\"test dataset length:\", len(test_ds))\r\n",
    "print(train_ds[0])\r\n",
    "print(train_ds[1])\r\n",
    "print(train_ds[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 指定模型名称，一键加载模型\n",
    "#model = SkepForTokenClassification.from_pretrained(pretrained_model_name_or_path=\"skep_ernie_1.0_large_ch\", num_classes=len(train_ds.label_list))\n",
    "# 同样地，通过指定模型名称一键加载对应的Tokenizer，用于处理文本数据，如切分token，转token_id等。\n",
    "#tokenizer = SkepTokenizer.from_pretrained(pretrained_model_name_or_path=\"skep_ernie_1.0_large_ch\")\n",
    "model = ErnieGramForTokenClassification.from_pretrained(pretrained_model_name_or_path=\"ernie-gram-zh\", num_classes=3)\n",
    "# 同样地，通过指定模型名称一键加载对应的Tokenizer，用于处理文本数据，如切分token，转token_id等。\n",
    "tokenizer = ErnieGramTokenizer.from_pretrained(pretrained_model_name_or_path=\"ernie-gram-zh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataloader length: 1579\n"
     ]
    }
   ],
   "source": [
    "# 批量数据大小\n",
    "batch_size = 16\n",
    "# 文本序列最大长度\n",
    "max_seq_len = 512\n",
    "ignore_label = -1\n",
    "label_vocab = TokenData().label_dict\n",
    "\n",
    "# 将数据处理成模型可读入的数据格式\n",
    "trans_func = partial(convert_token_example, tokenizer=tokenizer, label_vocab=label_vocab, max_seq_len=max_seq_len)\n",
    "\n",
    "# 将数据组成批量式数据，如\n",
    "# 将不同长度的文本序列padding到批量式数据中最大长度\n",
    "# 将每条数据label堆叠在一起\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids\n",
    "    Stack(), # seq_len\n",
    "    Pad(axis=0, pad_val=ignore_label)  # labels\n",
    "): [data for data in fn(samples)]\n",
    "train_data_loader = create_dataloader(\n",
    "    train_ds,\n",
    "    mode='train',\n",
    "    batch_size=batch_size,\n",
    "    batchify_fn=batchify_fn,\n",
    "    trans_fn=trans_func)\n",
    "\"\"\"dev_data_loader = create_dataloader(\n",
    "    dev_ds,\n",
    "    mode='dev',\n",
    "    batch_size=batch_size,\n",
    "    batchify_fn=batchify_fn,\n",
    "    trans_fn=trans_func)\"\"\"\n",
    "\n",
    "print(\"train dataloader length:\", len(train_data_loader))\n",
    "#print(\"dev dataloader length:\", len(dev_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 训练轮次\n",
    "epochs = 10\n",
    "# 总共需要训练的step数\n",
    "num_training_steps = len(train_data_loader) * epochs\n",
    "lr_scheduler = LinearDecayWithWarmup(5e-5, num_training_steps, 0.1)\n",
    "decay_params = [\n",
    "    p.name for n, p in model.named_parameters()\n",
    "    if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "]\n",
    "# Adam优化器\n",
    "optimizer = paddle.optimizer.AdamW(\n",
    "    learning_rate=lr_scheduler,\n",
    "    parameters=model.parameters(),\n",
    "    weight_decay=0.001,\n",
    "    apply_decay_param_fun=lambda x: x in decay_params)\n",
    "# 交叉熵损失\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss(ignore_index=ignore_label)\n",
    "# Accuracy评价指标\n",
    "metric = ChunkEvaluator(label_list=label_vocab.keys(), suffix=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 开启训练\n",
    "save_dir = \"eg_cotedp\"\n",
    "patience = 10\n",
    "# 加入日志显示\n",
    "from visualdl import LogWriter\n",
    "writer = LogWriter(save_dir)\n",
    "\n",
    "train_token_model(model, optimizer, epochs, criterion, metric, save_dir, tokenizer, loader_list=[train_data_loader, None], patience=patience, lr_scheduler=lr_scheduler, writer=writer, save_freq=750)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 处理测试集数据\n",
    "trans_func = partial(convert_token_example, tokenizer=tokenizer, label_vocab=label_vocab, max_seq_len=max_seq_len, is_test=True)\n",
    "test_ds.map(trans_func)\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids\n",
    "    Stack(), # seq_len\n",
    "    Stack() # qid\n",
    "): [data for data in fn(samples)]\n",
    "test_data_loader = paddle.io.DataLoader(\n",
    "    dataset=test_ds,\n",
    "    batch_size=batch_size,\n",
    "    return_list=True,\n",
    "    shuffle=False,\n",
    "    collate_fn=batchify_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 根据实际运行情况，更换加载的参数路径\n",
    "params_path = 'eg_cotedp/final_model/model_state.pdparams'\n",
    "if params_path and os.path.isfile(params_path):\n",
    "    # 加载模型参数\n",
    "    state_dict = paddle.load(params_path)\n",
    "    model.set_dict(state_dict)\n",
    "    print(\"Loaded parameters from %s\" % params_path)\n",
    "\n",
    "results = token_predict(model, test_data_loader, label_vocab, tokenizer)\n",
    "#print(len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 写入预测结果\n",
    "import re\n",
    "with open(os.path.join(\"results\", \"COTE_DP.tsv\"), 'w', encoding=\"utf8\") as f:\n",
    "    f.write(\"index\\tprediction\\n\")\n",
    "    for idx, label in enumerate(results):\n",
    "        qid = test_ds.data[idx]['qid']\n",
    "        label = [re.sub('##', '', l) for l in label]\n",
    "        label = [re.sub('[UNK]', '', l) for l in label]\n",
    "        f.write(qid+\"\\t\"+ '\\x01'.join(label)+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## COTE-MFW（观点抽取）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from paddlenlp.datasets import load_dataset\r\n",
    "from paddlenlp.transformers import SkepForSequenceClassification, SkepTokenizer, SkepForTokenClassification, SkepCrfForTokenClassification, ErnieGramForTokenClassification, ErnieGramTokenizer\r\n",
    "import os\r\n",
    "import re\r\n",
    "from functools import partial\r\n",
    "from paddlenlp.transformers import LinearDecayWithWarmup\r\n",
    "import random\r\n",
    "import numpy as np\r\n",
    "import paddle\r\n",
    "import paddle.nn.functional as F\r\n",
    "from paddlenlp.metrics import ChunkEvaluator\r\n",
    "from paddlenlp.data import Stack, Tuple, Pad\r\n",
    "\r\n",
    "from utils import create_dataloader, convert_example, predict, token_predict\r\n",
    "from train import train_token_model\r\n",
    "from data import load_my_dataset, TokenData, convert_token_example\r\n",
    "def seed_paddle(seed=2021):\r\n",
    "    seed = int(seed)\r\n",
    "    random.seed(seed)\r\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\r\n",
    "    np.random.seed(seed)\r\n",
    "    paddle.seed(seed)\r\n",
    "\r\n",
    "seed_paddle(seed=2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 指定模型名称，一键加载模型\n",
    "#model = SkepForTokenClassification.from_pretrained(pretrained_model_name_or_path=\"skep_ernie_1.0_large_ch\", num_classes=len(train_ds.label_list))\n",
    "# 同样地，通过指定模型名称一键加载对应的Tokenizer，用于处理文本数据，如切分token，转token_id等。\n",
    "#tokenizer = SkepTokenizer.from_pretrained(pretrained_model_name_or_path=\"skep_ernie_1.0_large_ch\")\n",
    "model = ErnieGramForTokenClassification.from_pretrained(pretrained_model_name_or_path=\"ernie-gram-zh\", num_classes=3)\n",
    "# 同样地，通过指定模型名称一键加载对应的Tokenizer，用于处理文本数据，如切分token，转token_id等。\n",
    "tokenizer = ErnieGramTokenizer.from_pretrained(pretrained_model_name_or_path=\"ernie-gram-zh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset length: 41253\n",
      "test dataset length: 17681\n",
      "{'text': '秀美恩施大峡谷，因其奇、险让人流连忘返。', 'text_label': '恩施大峡谷'}\n",
      "{'text': '龙鳞宫说白了，就是用多种颜色的灯光打在石钟乳上，形成五光十色的视觉效果', 'text_label': '龙鳞宫'}\n",
      "{'text': '回来百度方知道，舟山跨海大桥，又叫舟山大陆连岛工程，跨四座岛屿，翻九个涵洞，穿两个隧道，全长四十八公里。', 'text_label': '舟山跨海大桥'}\n",
      "train dataloader length: 2579\n"
     ]
    }
   ],
   "source": [
    "train_ds, test_ds = load_my_dataset(splits=[\"train\", \"test\"], SPLITS={'train':'data/COTE-MFW/train.tsv', 'test':'data/COTE-MFW/test.tsv'}, is_token=True)\r\n",
    "print(\"train dataset length:\", len(train_ds))\r\n",
    "print(\"test dataset length:\", len(test_ds))\r\n",
    "print(train_ds[0])\r\n",
    "print(train_ds[1])\r\n",
    "print(train_ds[2])\r\n",
    "\r\n",
    "# 批量数据大小\r\n",
    "batch_size = 16\r\n",
    "# 文本序列最大长度\r\n",
    "max_seq_len = 512\r\n",
    "ignore_label = -1\r\n",
    "label_vocab = TokenData().label_dict\r\n",
    "\r\n",
    "# 将数据处理成模型可读入的数据格式\r\n",
    "trans_func = partial(convert_token_example, tokenizer=tokenizer, label_vocab=label_vocab, max_seq_len=max_seq_len)\r\n",
    "\r\n",
    "# 将数据组成批量式数据，如\r\n",
    "# 将不同长度的文本序列padding到批量式数据中最大长度\r\n",
    "# 将每条数据label堆叠在一起\r\n",
    "batchify_fn = lambda samples, fn=Tuple(\r\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\r\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids\r\n",
    "    Stack(), # seq_len\r\n",
    "    Pad(axis=0, pad_val=ignore_label)  # labels\r\n",
    "): [data for data in fn(samples)]\r\n",
    "train_data_loader = create_dataloader(\r\n",
    "    train_ds,\r\n",
    "    mode='train',\r\n",
    "    batch_size=batch_size,\r\n",
    "    batchify_fn=batchify_fn,\r\n",
    "    trans_fn=trans_func)\r\n",
    "\"\"\"dev_data_loader = create_dataloader(\r\n",
    "    dev_ds,\r\n",
    "    mode='dev',\r\n",
    "    batch_size=batch_size,\r\n",
    "    batchify_fn=batchify_fn,\r\n",
    "    trans_fn=trans_func)\"\"\"\r\n",
    "\r\n",
    "print(\"train dataloader length:\", len(train_data_loader))\r\n",
    "#print(\"dev dataloader length:\", len(dev_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 训练轮次\r\n",
    "epochs = 10\r\n",
    "# 总共需要训练的step数\r\n",
    "num_training_steps = len(train_data_loader) * epochs\r\n",
    "lr_scheduler = LinearDecayWithWarmup(5e-5, num_training_steps, 0.1)\r\n",
    "decay_params = [\r\n",
    "    p.name for n, p in model.named_parameters()\r\n",
    "    if not any(nd in n for nd in [\"bias\", \"norm\"])\r\n",
    "]\r\n",
    "# Adam优化器\r\n",
    "optimizer = paddle.optimizer.AdamW(\r\n",
    "    learning_rate=lr_scheduler,\r\n",
    "    parameters=model.parameters(),\r\n",
    "    weight_decay=0.001,\r\n",
    "    apply_decay_param_fun=lambda x: x in decay_params)\r\n",
    "# 交叉熵损失\r\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss(ignore_index=ignore_label)\r\n",
    "# Accuracy评价指标\r\n",
    "metric = ChunkEvaluator(label_list=label_vocab.keys(), suffix=True)\r\n",
    "\r\n",
    "# 开启训练\r\n",
    "save_dir = \"eg_cotemfw\"\r\n",
    "patience = 10\r\n",
    "# 加入日志显示\r\n",
    "from visualdl import LogWriter\r\n",
    "writer = LogWriter(save_dir)\r\n",
    "\r\n",
    "train_token_model(model, optimizer, epochs, criterion, metric, save_dir, tokenizer, loader_list=[train_data_loader, None], patience=patience, lr_scheduler=lr_scheduler, writer=writer, save_freq=len(train_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 处理测试集数据\n",
    "trans_func = partial(convert_token_example, tokenizer=tokenizer, label_vocab=label_vocab, max_seq_len=max_seq_len, is_test=True)\n",
    "test_ds.map(trans_func)\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids\n",
    "    Stack(), # seq_len\n",
    "    Stack() # qid\n",
    "): [data for data in fn(samples)]\n",
    "test_data_loader = paddle.io.DataLoader(\n",
    "    dataset=test_ds,\n",
    "    batch_size=batch_size,\n",
    "    return_list=True,\n",
    "    shuffle=False,\n",
    "    collate_fn=batchify_fn)\n",
    "\n",
    "# 根据实际运行情况，更换加载的参数路径\n",
    "params_path = 'eg_cotemfw/final_model/model_state.pdparams'\n",
    "if params_path and os.path.isfile(params_path):\n",
    "    # 加载模型参数\n",
    "    state_dict = paddle.load(params_path)\n",
    "    model.set_dict(state_dict)\n",
    "    print(\"Loaded parameters from %s\" % params_path)\n",
    "\n",
    "results = token_predict(model, test_data_loader, label_vocab, tokenizer)\n",
    "#print(len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 写入预测结果\n",
    "import re\n",
    "with open(os.path.join(\"results\", \"COTE_MFW.tsv\"), 'w', encoding=\"utf8\") as f:\n",
    "    f.write(\"index\\tprediction\\n\")\n",
    "    for idx, label in enumerate(results):\n",
    "        qid = test_ds.data[idx]['qid']\n",
    "        label = [re.sub('##', '', l) for l in label]\n",
    "        label = [re.sub('[UNK]', '', l) for l in label]\n",
    "        f.write(qid+\"\\t\"+ '\\x01'.join(label)+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 打包文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#将预测文件结果压缩至zip文件，提交\n",
    "!zip -r results.zip results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.1.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
