{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pred', '0', '0', '0', '1', '1', '1', '0', '1', '1']\n"
     ]
    }
   ],
   "source": [
    "import os\r\n",
    "results = []\r\n",
    "with open(\"ChnSentiCorp.tsv\", 'r') as f:\r\n",
    "    for l in f:\r\n",
    "        l = l.strip().split('\\t')\r\n",
    "        results.append(l[0])\r\n",
    "\r\n",
    "print(results[:10])\r\n",
    "\r\n",
    "with open(\"ChnSentiCorp.tsv\", 'w', encoding=\"utf8\") as f:\r\n",
    "    f.write(\"index\\tprediction\\n\")\r\n",
    "    for qid, label in enumerate(results[1:]):\r\n",
    "        f.write(str(qid)+\"\\t\"+label+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# ä½œä¸š\n",
    "\n",
    "å®Œæˆé¢„æµ‹ç¯èŠ‚é¢„è®­ç»ƒæ¨¡å‹çš„è°ƒç”¨ä»£ç ï¼Œå¹¶è·‘é€šæ•´ä¸ªé¡¹ç›®ï¼ŒæˆåŠŸæäº¤åƒè¨€æ–‡æœ¬ç›¸ä¼¼åº¦ç«èµ›ï¼ŒæŒ‰è¦æ±‚æˆªå›¾ï¼Œæäº¤ä½œä¸šå³å¯ã€‚\n",
    "\n",
    "tipsï¼š\n",
    "\n",
    "- é¢„æµ‹å¯ä»¥ä½¿ç”¨è‡ªå·±è®­ç»ƒçš„æ¨¡å‹ï¼ˆè®­ç»ƒæ—¶é—´è¾ƒé•¿ï¼‰ï¼Œä¹Ÿå¯ä»¥ç›´æ¥ä½¿ç”¨æä¾›ä¸‹è½½çš„æ¨¡å‹æƒé‡ï¼›\n",
    "- æŠ¥ååƒè¨€æ–‡æœ¬ç›¸ä¼¼åº¦ç«èµ›ï¼Œå¹¶æˆåŠŸæäº¤ç»“æœï¼›\n",
    "- å¹¶å°†å¦‚ä¸‹å›¾æ‰€ç¤ºçš„ç»“æœæˆªå›¾ï¼Œè´´åˆ°æœ¬é¡¹ç›®ä½œä¸šæœ€åä¸€è¡Œå³å®Œæˆä½œä¸šã€‚\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/cf119d3bc6504c098cc3cc58597b7061890d5fe915364f5fbd52341033307c7c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# åŸºäºé¢„è®­ç»ƒæ¨¡å‹ ERNIE-Gram å®ç°è¯­ä¹‰åŒ¹é…\n",
    "\n",
    "\n",
    "6.7NLPç›´æ’­æ‰“å¡è¯¾å³å°†å¼€æ’­ï¼Œæ¬¢è¿å¤§å®¶å…³æ³¨è¯¾ç¨‹ï¼Œæœ‰ä»»ä½•é—®é¢˜æ¥è¯„è®ºåŒºæˆ–**QQç¾¤**ï¼ˆç¾¤å·:758287592ï¼‰äº¤æµå§~~\n",
    "\n",
    "**[ç›´æ’­é“¾æ¥è¯·æˆ³è¿™é‡Œï¼Œæ¯æ™š20:00-21:30ğŸ‘ˆ](http://live.bilibili.com/21689802)**\n",
    "\n",
    "**[è¯¾ç¨‹åœ°å€è¯·æˆ³è¿™é‡ŒğŸ‘ˆ](https://aistudio.baidu.com/aistudio/course/introduce/24177)**\n",
    "\n",
    "\n",
    "æœ¬æ¡ˆä¾‹ä»‹ç» NLP æœ€åŸºæœ¬çš„ä»»åŠ¡ç±»å‹ä¹‹ä¸€ â€”â€” æ–‡æœ¬è¯­ä¹‰åŒ¹é…ï¼Œå¹¶ä¸”åŸºäº PaddleNLP ä½¿ç”¨ç™¾åº¦å¼€æºçš„é¢„è®­ç»ƒæ¨¡å‹ ERNIE-Gram æ­å»ºæ•ˆæœä¼˜å¼‚çš„è¯­ä¹‰åŒ¹é…æ¨¡å‹ï¼Œæ¥åˆ¤æ–­ 2 æ®µæ–‡æœ¬è¯­ä¹‰æ˜¯å¦ç›¸åŒã€‚\n",
    "\n",
    "## 1. èƒŒæ™¯ä»‹ç»\n",
    "æ–‡æœ¬è¯­ä¹‰åŒ¹é…ä»»åŠ¡ï¼Œç®€å•æ¥è¯´å°±æ˜¯ç»™å®šä¸¤æ®µæ–‡æœ¬ï¼Œè®©æ¨¡å‹æ¥åˆ¤æ–­ä¸¤æ®µæ–‡æœ¬æ˜¯ä¸æ˜¯è¯­ä¹‰ç›¸ä¼¼ã€‚\n",
    "\n",
    "åœ¨æœ¬æ¡ˆä¾‹ä¸­ä»¥æƒå¨çš„è¯­ä¹‰åŒ¹é…æ•°æ®é›† [LCQMC](http://icrc.hitsz.edu.cn/Article/show/171.html) ä¸ºä¾‹ï¼Œ[LCQMC](http://icrc.hitsz.edu.cn/Article/show/171.html) æ•°æ®é›†æ˜¯åŸºäºç™¾åº¦çŸ¥é“ç›¸ä¼¼é—®é¢˜æ¨èæ„é€ çš„é€šé—®å¥è¯­ä¹‰åŒ¹é…æ•°æ®é›†ã€‚è®­ç»ƒé›†ä¸­çš„æ¯ä¸¤æ®µæ–‡æœ¬éƒ½ä¼šè¢«æ ‡è®°ä¸º 1ï¼ˆè¯­ä¹‰ç›¸ä¼¼ï¼‰ æˆ–è€… 0ï¼ˆè¯­ä¹‰ä¸ç›¸ä¼¼ï¼‰ã€‚æ›´å¤šæ•°æ®é›†å¯è®¿é—®[åƒè¨€](https://www.luge.ai/)è·å–å“¦ã€‚\n",
    "\n",
    "ä¾‹å¦‚ç™¾åº¦çŸ¥é“åœºæ™¯ä¸‹ï¼Œç”¨æˆ·æœç´¢ä¸€ä¸ªé—®é¢˜ï¼Œæ¨¡å‹ä¼šè®¡ç®—è¿™ä¸ªé—®é¢˜ä¸å€™é€‰é—®é¢˜æ˜¯å¦è¯­ä¹‰ç›¸ä¼¼ï¼Œè¯­ä¹‰åŒ¹é…æ¨¡å‹ä¼šæ‰¾å‡ºä¸é—®é¢˜è¯­ä¹‰ç›¸ä¼¼çš„å€™é€‰é—®é¢˜è¿”å›ç»™ç”¨æˆ·ï¼ŒåŠ å¿«ç”¨æˆ·æé—®-è·å–ç­”æ¡ˆçš„æ•ˆç‡ã€‚ä¾‹å¦‚ï¼Œå½“æŸç”¨æˆ·åœ¨æœç´¢å¼•æ“ä¸­æœç´¢ â€œæ·±åº¦å­¦ä¹ çš„æ•™ææœ‰å“ªäº›ï¼Ÿâ€ï¼Œæ¨¡å‹å°±è‡ªåŠ¨æ‰¾åˆ°äº†ä¸€äº›è¯­ä¹‰ç›¸ä¼¼çš„é—®é¢˜å±•ç°ç»™ç”¨æˆ·:\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/ecc1244685ec4476b869ce8a32d421c0ad530666e98d487da21fa4f61670544f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.å¿«é€Ÿå®è·µ\n",
    "\n",
    "ä»‹ç»å¦‚ä½•å‡†å¤‡æ•°æ®ï¼ŒåŸºäº ERNIE-Gram æ¨¡å‹æ­å»ºåŒ¹é…ç½‘ç»œï¼Œç„¶åå¿«é€Ÿè¿›è¡Œè¯­ä¹‰åŒ¹é…æ¨¡å‹çš„è®­ç»ƒã€è¯„ä¼°å’Œé¢„æµ‹ã€‚\n",
    "\n",
    "### 2.1 æ•°æ®åŠ è½½\n",
    "ä¸ºäº†è®­ç»ƒåŒ¹é…æ¨¡å‹ï¼Œä¸€èˆ¬éœ€è¦å‡†å¤‡ä¸‰ä¸ªæ•°æ®é›†ï¼šè®­ç»ƒé›† train.tsvã€éªŒè¯é›†dev.tsvã€æµ‹è¯•é›†test.tsvã€‚æ­¤æ¡ˆä¾‹æˆ‘ä»¬ä½¿ç”¨ PaddleNLP å†…ç½®çš„è¯­ä¹‰æ•°æ®é›† [LCQMC](http://icrc.hitsz.edu.cn/Article/show/171.html) æ¥è¿›è¡Œè®­ç»ƒã€è¯„ä¼°ã€é¢„æµ‹ã€‚\n",
    "\n",
    "è®­ç»ƒé›†: ç”¨æ¥è®­ç»ƒæ¨¡å‹å‚æ•°çš„æ•°æ®é›†ï¼Œæ¨¡å‹ç›´æ¥æ ¹æ®è®­ç»ƒé›†æ¥è°ƒæ•´è‡ªèº«å‚æ•°ä»¥è·å¾—æ›´å¥½çš„åˆ†ç±»æ•ˆæœã€‚\n",
    "\n",
    "éªŒè¯é›†: ç”¨äºåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ£€éªŒæ¨¡å‹çš„çŠ¶æ€ï¼Œæ”¶æ•›æƒ…å†µã€‚éªŒè¯é›†é€šå¸¸ç”¨äºè°ƒæ•´è¶…å‚æ•°ï¼Œæ ¹æ®å‡ ç»„æ¨¡å‹éªŒè¯é›†ä¸Šçš„è¡¨ç°ï¼Œå†³å®šé‡‡ç”¨å“ªç»„è¶…å‚æ•°ã€‚\n",
    "\n",
    "æµ‹è¯•é›†: ç”¨æ¥è®¡ç®—æ¨¡å‹çš„å„é¡¹è¯„ä¼°æŒ‡æ ‡ï¼ŒéªŒè¯æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚\n",
    "\n",
    "[LCQMC](http://icrc.hitsz.edu.cn/Article/show/171.html) æ•°æ®é›†æ˜¯å…¬å¼€çš„è¯­ä¹‰åŒ¹é…æƒå¨æ•°æ®é›†ã€‚PaddleNLP å·²ç»å†…ç½®è¯¥æ•°æ®é›†ï¼Œä¸€é”®å³å¯åŠ è½½ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: paddlenlp in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (2.0.2)\n",
      "Requirement already satisfied, skipping upgrade: seqeval in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (1.2.2)\n",
      "Requirement already satisfied, skipping upgrade: h5py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (2.9.0)\n",
      "Requirement already satisfied, skipping upgrade: multiprocess in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.70.11.1)\n",
      "Requirement already satisfied, skipping upgrade: colorlog in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (4.1.0)\n",
      "Requirement already satisfied, skipping upgrade: jieba in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.42.1)\n",
      "Requirement already satisfied, skipping upgrade: colorama in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (0.4.4)\n",
      "Requirement already satisfied, skipping upgrade: visualdl in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp) (2.1.1)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn>=0.21.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from seqeval->paddlenlp) (0.24.2)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.14.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from seqeval->paddlenlp) (1.20.3)\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from h5py->paddlenlp) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: dill>=0.3.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from multiprocess->paddlenlp) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: Pillow>=7.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: flask>=1.1.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.1.1)\n",
      "Requirement already satisfied, skipping upgrade: flake8>=3.7.9 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (3.8.2)\n",
      "Requirement already satisfied, skipping upgrade: Flask-Babel>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: pre-commit in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (1.21.0)\n",
      "Requirement already satisfied, skipping upgrade: bce-python-sdk in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (0.8.53)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.11.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (3.14.0)\n",
      "Requirement already satisfied, skipping upgrade: requests in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: shellcheck-py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp) (0.7.1.1)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (1.6.3)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (2.1.0)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp) (0.14.1)\n",
      "Requirement already satisfied, skipping upgrade: itsdangerous>=0.24 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: Werkzeug>=0.15 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: Jinja2>=2.10.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (2.10.1)\n",
      "Requirement already satisfied, skipping upgrade: click>=5.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp) (7.0)\n",
      "Requirement already satisfied, skipping upgrade: pyflakes<2.3.0,>=2.2.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (2.2.0)\n",
      "Requirement already satisfied, skipping upgrade: mccabe<0.7.0,>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (0.6.1)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (0.23)\n",
      "Requirement already satisfied, skipping upgrade: pycodestyle<2.7.0,>=2.6.0a1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp) (2.6.0)\n",
      "Requirement already satisfied, skipping upgrade: Babel>=2.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: pytz in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp) (2019.3)\n",
      "Requirement already satisfied, skipping upgrade: nodeenv>=0.11.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (1.3.4)\n",
      "Requirement already satisfied, skipping upgrade: aspy.yaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (1.3.0)\n",
      "Requirement already satisfied, skipping upgrade: cfgv>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (2.0.1)\n",
      "Requirement already satisfied, skipping upgrade: toml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (5.1.2)\n",
      "Requirement already satisfied, skipping upgrade: virtualenv>=15.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (16.7.9)\n",
      "Requirement already satisfied, skipping upgrade: identify>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp) (1.4.10)\n",
      "Requirement already satisfied, skipping upgrade: pycryptodome>=3.8.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->paddlenlp) (3.9.9)\n",
      "Requirement already satisfied, skipping upgrade: future>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->paddlenlp) (0.18.0)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (1.25.6)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (2019.9.11)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Jinja2>=2.10.1->flask>=1.1.1->visualdl->paddlenlp) (1.1.1)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->flake8>=3.7.9->visualdl->paddlenlp) (0.6.0)\n",
      "Requirement already satisfied, skipping upgrade: more-itertools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata; python_version < \"3.8\"->flake8>=3.7.9->visualdl->paddlenlp) (7.2.0)\n"
     ]
    }
   ],
   "source": [
    "# æ­£å¼å¼€å§‹å®éªŒä¹‹å‰é¦–å…ˆé€šè¿‡å¦‚ä¸‹å‘½ä»¤å®‰è£…æœ€æ–°ç‰ˆæœ¬çš„ paddlenlp\n",
    "!pip install --upgrade paddlenlp -i https://pypi.org/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import paddle\n",
    "import paddle.nn.functional as F\n",
    "from paddlenlp.datasets import load_dataset\n",
    "import paddlenlp\n",
    "\n",
    "# ä¸€é”®åŠ è½½ Lcqmc çš„è®­ç»ƒé›†ã€éªŒè¯é›†\n",
    "train_ds, dev_ds = load_dataset(\"lcqmc\", splits=[\"train\", \"dev\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'å–œæ¬¢æ‰“ç¯®çƒçš„ç”·ç”Ÿå–œæ¬¢ä»€ä¹ˆæ ·çš„å¥³ç”Ÿ', 'title': 'çˆ±æ‰“ç¯®çƒçš„ç”·ç”Ÿå–œæ¬¢ä»€ä¹ˆæ ·çš„å¥³ç”Ÿ', 'label': 1}\n",
      "{'query': 'æˆ‘æ‰‹æœºä¸¢äº†ï¼Œæˆ‘æƒ³æ¢ä¸ªæ‰‹æœº', 'title': 'æˆ‘æƒ³ä¹°ä¸ªæ–°æ‰‹æœºï¼Œæ±‚æ¨è', 'label': 1}\n",
      "{'query': 'å¤§å®¶è§‰å¾—å¥¹å¥½çœ‹å—', 'title': 'å¤§å®¶è§‰å¾—è·‘ç”·å¥½çœ‹å—ï¼Ÿ', 'label': 0}\n",
      "{'query': 'æ±‚ç§‹è‰²ä¹‹ç©ºæ¼«ç”»å…¨é›†', 'title': 'æ±‚ç§‹è‰²ä¹‹ç©ºå…¨é›†æ¼«ç”»', 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "# è¾“å‡ºè®­ç»ƒé›†çš„å‰ 3 æ¡æ ·æœ¬\n",
    "for idx, example in enumerate(train_ds):\n",
    "    if idx <= 3:\n",
    "        print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.2 æ•°æ®é¢„å¤„ç†\n",
    "\n",
    "é€šè¿‡ PaddleNLP åŠ è½½è¿›æ¥çš„ [LCQMC](http://icrc.hitsz.edu.cn/Article/show/171.html) æ•°æ®é›†æ˜¯åŸå§‹çš„æ˜æ–‡æ•°æ®é›†ï¼Œè¿™éƒ¨åˆ†æˆ‘ä»¬æ¥å®ç°ç»„ batchã€tokenize ç­‰é¢„å¤„ç†é€»è¾‘ï¼Œå°†åŸå§‹æ˜æ–‡æ•°æ®è½¬æ¢æˆç½‘ç»œè®­ç»ƒçš„è¾“å…¥æ•°æ®ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### å®šä¹‰æ ·æœ¬è½¬æ¢å‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-10 21:50:26,907] [    INFO] - Found /home/aistudio/.paddlenlp/models/ernie-gram-zh/vocab.txt\n"
     ]
    }
   ],
   "source": [
    "# å› ä¸ºæ˜¯åŸºäºé¢„è®­ç»ƒæ¨¡å‹ ERNIE-Gram æ¥è¿›è¡Œï¼Œæ‰€ä»¥éœ€è¦é¦–å…ˆåŠ è½½ ERNIE-Gram çš„ tokenizerï¼Œ\n",
    "# åç»­æ ·æœ¬è½¬æ¢å‡½æ•°åŸºäº tokenizer å¯¹æ–‡æœ¬è¿›è¡Œåˆ‡åˆ†\n",
    "\n",
    "tokenizer = paddlenlp.transformers.ErnieGramTokenizer.from_pretrained('ernie-gram-zh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# å°† 1 æ¡æ˜æ–‡æ•°æ®çš„ queryã€title æ‹¼æ¥èµ·æ¥ï¼Œæ ¹æ®é¢„è®­ç»ƒæ¨¡å‹çš„ tokenizer å°†æ˜æ–‡è½¬æ¢ä¸º ID æ•°æ®\n",
    "# è¿”å› input_ids å’Œ token_type_ids\n",
    "\n",
    "def convert_example(example, tokenizer, max_seq_length=512, is_test=False):\n",
    "\n",
    "    query, title = example[\"query\"], example[\"title\"]\n",
    "\n",
    "    encoded_inputs = tokenizer(\n",
    "        text=query, text_pair=title, max_seq_len=max_seq_length)\n",
    "\n",
    "    input_ids = encoded_inputs[\"input_ids\"]\n",
    "    token_type_ids = encoded_inputs[\"token_type_ids\"]\n",
    "\n",
    "    if not is_test:\n",
    "        label = np.array([example[\"label\"]], dtype=\"int64\")\n",
    "        return input_ids, token_type_ids, label\n",
    "    # åœ¨é¢„æµ‹æˆ–è€…è¯„ä¼°é˜¶æ®µï¼Œä¸è¿”å› label å­—æ®µ\n",
    "    else:\n",
    "        return input_ids, token_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### å¯¹è®­ç»ƒé›†çš„ç¬¬ 1 æ¡æ•°æ®è¿›è¡Œè½¬æ¢\n",
    "input_ids, token_type_ids, label = convert_example(train_ds[0], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 692, 811, 445, 2001, 497, 5, 654, 21, 692, 811, 614, 356, 314, 5, 291, 21, 2, 329, 445, 2001, 497, 5, 654, 21, 692, 811, 614, 356, 314, 5, 291, 21, 2]\n"
     ]
    }
   ],
   "source": [
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(token_type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ä¸ºäº†åç»­æ–¹ä¾¿ä½¿ç”¨ï¼Œæˆ‘ä»¬ä½¿ç”¨pythonåå‡½æ•°ï¼ˆpartialï¼‰ç»™ convert_example èµ‹äºˆä¸€äº›é»˜è®¤å‚æ•°\n",
    "from functools import partial\n",
    "\n",
    "# è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„æ ·æœ¬è½¬æ¢å‡½æ•°\n",
    "trans_func = partial(\n",
    "    convert_example,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### ç»„è£… Batch æ•°æ® & Padding\n",
    "\n",
    "ä¸Šä¸€å°èŠ‚ï¼Œæˆ‘ä»¬å®Œæˆäº†å¯¹å•æ¡æ ·æœ¬çš„è½¬æ¢ï¼Œæœ¬èŠ‚æˆ‘ä»¬éœ€è¦å°†æ ·æœ¬ç»„åˆæˆ Batch æ•°æ®ï¼Œå¯¹äºä¸ç­‰é•¿çš„æ•°æ®è¿˜éœ€è¦è¿›è¡Œ Padding æ“ä½œï¼Œä¾¿äº GPU è®­ç»ƒã€‚\n",
    "\n",
    "PaddleNLP æä¾›äº†è®¸å¤šå…³äº NLP ä»»åŠ¡ä¸­æ„å»ºæœ‰æ•ˆçš„æ•°æ® pipeline çš„å¸¸ç”¨ API\n",
    "\n",
    "| API                             | ç®€ä»‹                                       |\n",
    "| ------------------------------- | :----------------------------------------- |\n",
    "| `paddlenlp.data.Stack`          | å †å Nä¸ªå…·æœ‰ç›¸åŒshapeçš„è¾“å…¥æ•°æ®æ¥æ„å»ºä¸€ä¸ªbatch |\n",
    "| `paddlenlp.data.Pad`            | å°†é•¿åº¦ä¸åŒçš„å¤šä¸ªå¥å­paddingåˆ°ç»Ÿä¸€é•¿åº¦ï¼Œå–Nä¸ªè¾“å…¥æ•°æ®ä¸­çš„æœ€å¤§é•¿åº¦ |\n",
    "| `paddlenlp.data.Tuple`          | å°†å¤šä¸ªbatchifyå‡½æ•°åŒ…è£…åœ¨ä¸€èµ· |\n",
    "\n",
    "æ›´å¤šæ•°æ®å¤„ç†æ“ä½œè¯¦è§ï¼š [https://paddlenlp.readthedocs.io/zh/latest/data_prepare/data_preprocess.html](https://paddlenlp.readthedocs.io/zh/latest/data_prepare/data_preprocess.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked Data: \n",
      " [[1 2 3 4]\n",
      " [3 4 5 6]\n",
      " [5 6 7 8]]\n",
      "\n",
      "Padded Data: \n",
      " [[1 2 3 4]\n",
      " [5 6 7 0]\n",
      " [8 9 0 0]]\n",
      "\n",
      "ids: \n",
      " [[1 2 3 4]\n",
      " [5 6 7 0]\n",
      " [8 9 0 0]]\n",
      "\n",
      "labels: \n",
      " [[1]\n",
      " [0]\n",
      " [1]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from paddlenlp.data import Stack, Pad, Tuple\n",
    "a = [1, 2, 3, 4]\n",
    "b = [3, 4, 5, 6]\n",
    "c = [5, 6, 7, 8]\n",
    "result = Stack()([a, b, c])\n",
    "print(\"Stacked Data: \\n\", result)\n",
    "print()\n",
    "\n",
    "a = [1, 2, 3, 4]\n",
    "b = [5, 6, 7]\n",
    "c = [8, 9]\n",
    "result = Pad(pad_val=0)([a, b, c])\n",
    "print(\"Padded Data: \\n\", result)\n",
    "print()\n",
    "\n",
    "data = [\n",
    "        [[1, 2, 3, 4], [1]],\n",
    "        [[5, 6, 7], [0]],\n",
    "        [[8, 9], [1]],\n",
    "       ]\n",
    "batchify_fn = Tuple(Pad(pad_val=0), Stack())\n",
    "ids, labels = batchify_fn(data)\n",
    "print(\"ids: \\n\", ids)\n",
    "print()\n",
    "print(\"labels: \\n\", labels)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# æˆ‘ä»¬çš„è®­ç»ƒæ•°æ®ä¼šè¿”å› input_ids, token_type_ids, labels 3 ä¸ªå­—æ®µ\n",
    "# å› æ­¤é’ˆå¯¹è¿™ 3 ä¸ªå­—æ®µéœ€è¦åˆ†åˆ«å®šä¹‰ 3 ä¸ªç»„ batch æ“ä½œ\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids\n",
    "    Stack(dtype=\"int64\")  # label\n",
    "): [data for data in fn(samples)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### å®šä¹‰ Dataloader\n",
    "ä¸‹é¢æˆ‘ä»¬åŸºäºç»„ batchify_fn å‡½æ•°å’Œæ ·æœ¬è½¬æ¢å‡½æ•° trans_func æ¥æ„é€ è®­ç»ƒé›†çš„ DataLoader, æ”¯æŒå¤šå¡è®­ç»ƒ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "# å®šä¹‰åˆ†å¸ƒå¼ Sampler: è‡ªåŠ¨å¯¹è®­ç»ƒæ•°æ®è¿›è¡Œåˆ‡åˆ†ï¼Œæ”¯æŒå¤šå¡å¹¶è¡Œè®­ç»ƒ\n",
    "batch_sampler = paddle.io.DistributedBatchSampler(train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# åŸºäº train_ds å®šä¹‰ train_data_loader\n",
    "# å› ä¸ºæˆ‘ä»¬ä½¿ç”¨äº†åˆ†å¸ƒå¼çš„ DistributedBatchSampler, train_data_loader ä¼šè‡ªåŠ¨å¯¹è®­ç»ƒæ•°æ®è¿›è¡Œåˆ‡åˆ†\n",
    "train_data_loader = paddle.io.DataLoader(\n",
    "        dataset=train_ds.map(trans_func),\n",
    "        batch_sampler=batch_sampler,\n",
    "        collate_fn=batchify_fn,\n",
    "        return_list=True)\n",
    "\n",
    "# é’ˆå¯¹éªŒè¯é›†æ•°æ®åŠ è½½ï¼Œæˆ‘ä»¬ä½¿ç”¨å•å¡è¿›è¡Œè¯„ä¼°ï¼Œæ‰€ä»¥é‡‡ç”¨ paddle.io.BatchSampler å³å¯\n",
    "# å®šä¹‰ dev_data_loader\n",
    "batch_sampler = paddle.io.BatchSampler(dev_ds, batch_size=batch_size, shuffle=False)\n",
    "dev_data_loader = paddle.io.DataLoader(\n",
    "        dataset=dev_ds.map(trans_func),\n",
    "        batch_sampler=batch_sampler,\n",
    "        collate_fn=batchify_fn,\n",
    "        return_list=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.3 æ¨¡å‹æ­å»º\n",
    "\n",
    "è‡ªä» 2018 å¹´ 10 æœˆä»¥æ¥ï¼ŒNLP ä¸ªé¢†åŸŸçš„ä»»åŠ¡éƒ½é€šè¿‡ Pretrain + Finetune çš„æ¨¡å¼ç›¸æ¯”ä¼ ç»Ÿ DNN æ–¹æ³•åœ¨æ•ˆæœä¸Šå–å¾—äº†æ˜¾è‘—çš„æå‡ï¼Œæœ¬èŠ‚æˆ‘ä»¬ä»¥ç™¾åº¦å¼€æºçš„é¢„è®­ç»ƒæ¨¡å‹ ERNIE-Gram ä¸ºåŸºç¡€æ¨¡å‹ï¼Œåœ¨æ­¤ä¹‹ä¸Šæ„å»º Point-wise è¯­ä¹‰åŒ¹é…ç½‘ç»œã€‚\n",
    "\n",
    "é¦–å…ˆæˆ‘ä»¬æ¥å®šä¹‰ç½‘ç»œç»“æ„:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-10 21:50:52,270] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/ernie-gram-zh/ernie_gram_zh.pdparams\n"
     ]
    }
   ],
   "source": [
    "import paddle.nn as nn\n",
    "\n",
    "# æˆ‘ä»¬åŸºäº ERNIE-Gram æ¨¡å‹ç»“æ„æ­å»º Point-wise è¯­ä¹‰åŒ¹é…ç½‘ç»œ\n",
    "# æ‰€ä»¥æ­¤å¤„å…ˆå®šä¹‰ ERNIE-Gram çš„ pretrained_model\n",
    "pretrained_model = paddlenlp.transformers.ErnieGramModel.from_pretrained('ernie-gram-zh')\n",
    "#pretrained_model = paddlenlp.transformers.ErnieModel.from_pretrained('ernie-1.0')\n",
    "\n",
    "\n",
    "class PointwiseMatching(nn.Layer):\n",
    "   \n",
    "    # æ­¤å¤„çš„ pretained_model åœ¨æœ¬ä¾‹ä¸­ä¼šè¢« ERNIE-Gram é¢„è®­ç»ƒæ¨¡å‹åˆå§‹åŒ–\n",
    "    def __init__(self, pretrained_model, dropout=None):\n",
    "        super().__init__()\n",
    "        self.ptm = pretrained_model\n",
    "        self.dropout = nn.Dropout(dropout if dropout is not None else 0.1)\n",
    "\n",
    "        # è¯­ä¹‰åŒ¹é…ä»»åŠ¡: ç›¸ä¼¼ã€ä¸ç›¸ä¼¼ 2 åˆ†ç±»ä»»åŠ¡\n",
    "        self.classifier = nn.Linear(self.ptm.config[\"hidden_size\"], 2)\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids,\n",
    "                token_type_ids=None,\n",
    "                position_ids=None,\n",
    "                attention_mask=None):\n",
    "\n",
    "        # æ­¤å¤„çš„ Input_ids ç”±ä¸¤æ¡æ–‡æœ¬çš„ token ids æ‹¼æ¥è€Œæˆ\n",
    "        # token_type_ids è¡¨ç¤ºä¸¤æ®µæ–‡æœ¬çš„ç±»å‹ç¼–ç \n",
    "        # è¿”å›çš„ cls_embedding å°±è¡¨ç¤ºè¿™ä¸¤æ®µæ–‡æœ¬ç»è¿‡æ¨¡å‹çš„è®¡ç®—ä¹‹åè€Œå¾—åˆ°çš„è¯­ä¹‰è¡¨ç¤ºå‘é‡\n",
    "        _, cls_embedding = self.ptm(input_ids, token_type_ids, position_ids,\n",
    "                                    attention_mask)\n",
    "\n",
    "        cls_embedding = self.dropout(cls_embedding)\n",
    "\n",
    "        # åŸºäºæ–‡æœ¬å¯¹çš„è¯­ä¹‰è¡¨ç¤ºå‘é‡è¿›è¡Œ 2 åˆ†ç±»ä»»åŠ¡\n",
    "        logits = self.classifier(cls_embedding)\n",
    "        probs = F.softmax(logits)\n",
    "\n",
    "        return probs\n",
    "\n",
    "# å®šä¹‰ Point-wise è¯­ä¹‰åŒ¹é…ç½‘ç»œ\n",
    "model = PointwiseMatching(pretrained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.4 æ¨¡å‹è®­ç»ƒ & è¯„ä¼°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from paddlenlp.transformers import LinearDecayWithWarmup\n",
    "\n",
    "epochs = 3\n",
    "num_training_steps = len(train_data_loader) * epochs\n",
    "\n",
    "# å®šä¹‰ learning_rate_schedulerï¼Œè´Ÿè´£åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯¹ lr è¿›è¡Œè°ƒåº¦\n",
    "lr_scheduler = LinearDecayWithWarmup(5E-5, num_training_steps, 0.0)\n",
    "\n",
    "# Generate parameter names needed to perform weight decay.\n",
    "# All bias and LayerNorm parameters are excluded.\n",
    "decay_params = [\n",
    "    p.name for n, p in model.named_parameters()\n",
    "    if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "]\n",
    "\n",
    "# å®šä¹‰ Optimizer\n",
    "optimizer = paddle.optimizer.AdamW(\n",
    "    learning_rate=lr_scheduler,\n",
    "    parameters=model.parameters(),\n",
    "    weight_decay=0.0,\n",
    "    apply_decay_param_fun=lambda x: x in decay_params)\n",
    "\n",
    "# é‡‡ç”¨äº¤å‰ç†µ æŸå¤±å‡½æ•°\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()\n",
    "\n",
    "# è¯„ä¼°çš„æ—¶å€™é‡‡ç”¨å‡†ç¡®ç‡æŒ‡æ ‡\n",
    "metric = paddle.metric.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# å› ä¸ºè®­ç»ƒè¿‡ç¨‹ä¸­åŒæ—¶è¦åœ¨éªŒè¯é›†è¿›è¡Œæ¨¡å‹è¯„ä¼°ï¼Œå› æ­¤æˆ‘ä»¬å…ˆå®šä¹‰è¯„ä¼°å‡½æ•°\n",
    "# åŠ å…¥æ—¥å¿—æ˜¾ç¤º\n",
    "from visualdl import LogWriter\n",
    "writer = LogWriter(\"./log\")\n",
    "@paddle.no_grad()\n",
    "def evaluate(model, criterion, metric, data_loader, phase=\"dev\"):\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    losses = []\n",
    "    for batch in data_loader:\n",
    "        input_ids, token_type_ids, labels = batch\n",
    "        probs = model(input_ids=input_ids, token_type_ids=token_type_ids)\n",
    "        loss = criterion(probs, labels)\n",
    "        losses.append(loss.numpy())\n",
    "        correct = metric.compute(probs, labels)\n",
    "        metric.update(correct)\n",
    "        accu = metric.accumulate()\n",
    "    print(\"eval {} loss: {:.5}, accu: {:.5}\".format(phase,\n",
    "                                                    np.mean(losses), accu))\n",
    "    \n",
    "    #åŠ å…¥evalæ—¥å¿—æ˜¾ç¤º\n",
    "    writer.add_scalar(tag=\"eval/loss\", step=global_step, value=np.mean(losses))\n",
    "    writer.add_scalar(tag=\"eval/acc\", step=global_step, value=accu)\n",
    "    model.train()\n",
    "    metric.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 10, epoch: 1, batch: 10, loss: 0.54774, accu: 0.67891, speed: 2.30 step/s\n",
      "global step 20, epoch: 1, batch: 20, loss: 0.48550, accu: 0.74375, speed: 2.47 step/s\n",
      "global step 30, epoch: 1, batch: 30, loss: 0.43986, accu: 0.77865, speed: 2.42 step/s\n",
      "global step 40, epoch: 1, batch: 40, loss: 0.44687, accu: 0.79941, speed: 2.47 step/s\n",
      "global step 50, epoch: 1, batch: 50, loss: 0.46142, accu: 0.81359, speed: 2.32 step/s\n",
      "global step 60, epoch: 1, batch: 60, loss: 0.47871, accu: 0.81784, speed: 2.40 step/s\n",
      "global step 70, epoch: 1, batch: 70, loss: 0.43564, accu: 0.82366, speed: 2.25 step/s\n",
      "global step 80, epoch: 1, batch: 80, loss: 0.47699, accu: 0.82793, speed: 2.32 step/s\n",
      "global step 90, epoch: 1, batch: 90, loss: 0.43301, accu: 0.83229, speed: 2.42 step/s\n",
      "global step 100, epoch: 1, batch: 100, loss: 0.46551, accu: 0.83523, speed: 2.34 step/s\n",
      "eval dev loss: 0.50839, accu: 0.80198\n",
      "global step 110, epoch: 1, batch: 110, loss: 0.43096, accu: 0.86875, speed: 0.53 step/s\n",
      "global step 120, epoch: 1, batch: 120, loss: 0.46326, accu: 0.87969, speed: 2.26 step/s\n",
      "global step 130, epoch: 1, batch: 130, loss: 0.44675, accu: 0.88281, speed: 2.38 step/s\n",
      "global step 140, epoch: 1, batch: 140, loss: 0.43439, accu: 0.88555, speed: 2.55 step/s\n",
      "global step 150, epoch: 1, batch: 150, loss: 0.45496, accu: 0.88562, speed: 2.37 step/s\n",
      "global step 160, epoch: 1, batch: 160, loss: 0.45680, accu: 0.88333, speed: 2.47 step/s\n",
      "global step 170, epoch: 1, batch: 170, loss: 0.42868, accu: 0.88426, speed: 2.46 step/s\n",
      "global step 180, epoch: 1, batch: 180, loss: 0.37411, accu: 0.88477, speed: 2.33 step/s\n",
      "global step 190, epoch: 1, batch: 190, loss: 0.42425, accu: 0.88472, speed: 2.31 step/s\n",
      "global step 200, epoch: 1, batch: 200, loss: 0.40072, accu: 0.88367, speed: 2.39 step/s\n",
      "eval dev loss: 0.48487, accu: 0.82265\n",
      "global step 210, epoch: 1, batch: 210, loss: 0.38371, accu: 0.88984, speed: 0.53 step/s\n",
      "global step 220, epoch: 1, batch: 220, loss: 0.45431, accu: 0.87852, speed: 2.32 step/s\n",
      "global step 230, epoch: 1, batch: 230, loss: 0.44778, accu: 0.87266, speed: 2.44 step/s\n",
      "global step 240, epoch: 1, batch: 240, loss: 0.39280, accu: 0.87402, speed: 2.49 step/s\n",
      "global step 250, epoch: 1, batch: 250, loss: 0.43195, accu: 0.87453, speed: 2.29 step/s\n",
      "global step 260, epoch: 1, batch: 260, loss: 0.37454, accu: 0.87656, speed: 2.06 step/s\n",
      "global step 270, epoch: 1, batch: 270, loss: 0.45167, accu: 0.87768, speed: 2.41 step/s\n",
      "global step 280, epoch: 1, batch: 280, loss: 0.41986, accu: 0.88115, speed: 2.37 step/s\n",
      "global step 290, epoch: 1, batch: 290, loss: 0.41178, accu: 0.88394, speed: 2.40 step/s\n",
      "global step 300, epoch: 1, batch: 300, loss: 0.43152, accu: 0.88414, speed: 2.29 step/s\n",
      "eval dev loss: 0.48128, accu: 0.82765\n",
      "global step 310, epoch: 1, batch: 310, loss: 0.43515, accu: 0.89453, speed: 0.52 step/s\n",
      "global step 320, epoch: 1, batch: 320, loss: 0.41861, accu: 0.88789, speed: 2.44 step/s\n",
      "global step 330, epoch: 1, batch: 330, loss: 0.38784, accu: 0.89271, speed: 2.48 step/s\n",
      "global step 340, epoch: 1, batch: 340, loss: 0.45169, accu: 0.88887, speed: 2.45 step/s\n",
      "global step 350, epoch: 1, batch: 350, loss: 0.38547, accu: 0.88891, speed: 2.37 step/s\n",
      "global step 360, epoch: 1, batch: 360, loss: 0.41142, accu: 0.89010, speed: 2.29 step/s\n",
      "global step 370, epoch: 1, batch: 370, loss: 0.40768, accu: 0.89018, speed: 2.40 step/s\n",
      "global step 380, epoch: 1, batch: 380, loss: 0.47492, accu: 0.88838, speed: 2.52 step/s\n",
      "global step 390, epoch: 1, batch: 390, loss: 0.45456, accu: 0.88863, speed: 2.43 step/s\n",
      "global step 400, epoch: 1, batch: 400, loss: 0.40703, accu: 0.88859, speed: 2.20 step/s\n",
      "eval dev loss: 0.47249, accu: 0.83776\n",
      "global step 410, epoch: 1, batch: 410, loss: 0.40265, accu: 0.88281, speed: 0.53 step/s\n",
      "global step 420, epoch: 1, batch: 420, loss: 0.47532, accu: 0.87422, speed: 2.28 step/s\n",
      "global step 430, epoch: 1, batch: 430, loss: 0.46564, accu: 0.87422, speed: 2.36 step/s\n",
      "global step 440, epoch: 1, batch: 440, loss: 0.40838, accu: 0.87539, speed: 2.34 step/s\n",
      "global step 450, epoch: 1, batch: 450, loss: 0.41016, accu: 0.87938, speed: 2.45 step/s\n",
      "global step 460, epoch: 1, batch: 460, loss: 0.44201, accu: 0.88125, speed: 2.40 step/s\n",
      "global step 470, epoch: 1, batch: 470, loss: 0.44167, accu: 0.88359, speed: 2.48 step/s\n",
      "global step 480, epoch: 1, batch: 480, loss: 0.42625, accu: 0.88379, speed: 2.52 step/s\n",
      "global step 490, epoch: 1, batch: 490, loss: 0.40678, accu: 0.88290, speed: 2.48 step/s\n",
      "global step 500, epoch: 1, batch: 500, loss: 0.42275, accu: 0.88398, speed: 2.39 step/s\n",
      "eval dev loss: 0.47397, accu: 0.83379\n",
      "global step 510, epoch: 1, batch: 510, loss: 0.41536, accu: 0.88750, speed: 0.52 step/s\n",
      "global step 520, epoch: 1, batch: 520, loss: 0.39957, accu: 0.89062, speed: 2.38 step/s\n",
      "global step 530, epoch: 1, batch: 530, loss: 0.44286, accu: 0.88906, speed: 2.42 step/s\n",
      "global step 540, epoch: 1, batch: 540, loss: 0.48460, accu: 0.88555, speed: 2.43 step/s\n",
      "global step 550, epoch: 1, batch: 550, loss: 0.49069, accu: 0.88531, speed: 2.09 step/s\n",
      "global step 560, epoch: 1, batch: 560, loss: 0.44593, accu: 0.88594, speed: 2.29 step/s\n",
      "global step 570, epoch: 1, batch: 570, loss: 0.45314, accu: 0.88739, speed: 2.45 step/s\n",
      "global step 580, epoch: 1, batch: 580, loss: 0.40637, accu: 0.88721, speed: 2.52 step/s\n",
      "global step 590, epoch: 1, batch: 590, loss: 0.42695, accu: 0.88602, speed: 2.37 step/s\n",
      "global step 600, epoch: 1, batch: 600, loss: 0.46982, accu: 0.88508, speed: 2.27 step/s\n",
      "eval dev loss: 0.48233, accu: 0.82538\n",
      "global step 610, epoch: 1, batch: 610, loss: 0.39312, accu: 0.89609, speed: 0.52 step/s\n",
      "global step 620, epoch: 1, batch: 620, loss: 0.39189, accu: 0.89375, speed: 2.51 step/s\n",
      "global step 630, epoch: 1, batch: 630, loss: 0.45439, accu: 0.88958, speed: 2.45 step/s\n",
      "global step 640, epoch: 1, batch: 640, loss: 0.36959, accu: 0.89238, speed: 2.30 step/s\n",
      "global step 650, epoch: 1, batch: 650, loss: 0.37877, accu: 0.89266, speed: 2.35 step/s\n",
      "global step 660, epoch: 1, batch: 660, loss: 0.39720, accu: 0.89245, speed: 2.53 step/s\n",
      "global step 670, epoch: 1, batch: 670, loss: 0.41932, accu: 0.89330, speed: 2.38 step/s\n",
      "global step 680, epoch: 1, batch: 680, loss: 0.40774, accu: 0.89229, speed: 2.37 step/s\n",
      "global step 690, epoch: 1, batch: 690, loss: 0.38231, accu: 0.89132, speed: 2.51 step/s\n",
      "global step 700, epoch: 1, batch: 700, loss: 0.39604, accu: 0.89219, speed: 2.34 step/s\n",
      "eval dev loss: 0.46912, accu: 0.83913\n",
      "global step 710, epoch: 1, batch: 710, loss: 0.42980, accu: 0.90156, speed: 0.53 step/s\n",
      "global step 720, epoch: 1, batch: 720, loss: 0.42265, accu: 0.89922, speed: 2.27 step/s\n",
      "global step 730, epoch: 1, batch: 730, loss: 0.48547, accu: 0.90026, speed: 2.35 step/s\n",
      "global step 740, epoch: 1, batch: 740, loss: 0.43129, accu: 0.90039, speed: 2.36 step/s\n",
      "global step 750, epoch: 1, batch: 750, loss: 0.41141, accu: 0.89797, speed: 2.54 step/s\n",
      "global step 760, epoch: 1, batch: 760, loss: 0.39331, accu: 0.89740, speed: 2.31 step/s\n",
      "global step 770, epoch: 1, batch: 770, loss: 0.37933, accu: 0.89855, speed: 2.31 step/s\n",
      "global step 780, epoch: 1, batch: 780, loss: 0.41026, accu: 0.89844, speed: 2.41 step/s\n",
      "global step 790, epoch: 1, batch: 790, loss: 0.38395, accu: 0.89852, speed: 2.42 step/s\n",
      "global step 800, epoch: 1, batch: 800, loss: 0.38750, accu: 0.89906, speed: 2.57 step/s\n",
      "eval dev loss: 0.4643, accu: 0.84401\n",
      "global step 810, epoch: 1, batch: 810, loss: 0.39837, accu: 0.89453, speed: 0.53 step/s\n",
      "global step 820, epoch: 1, batch: 820, loss: 0.40783, accu: 0.89961, speed: 2.42 step/s\n",
      "global step 830, epoch: 1, batch: 830, loss: 0.38971, accu: 0.90260, speed: 2.35 step/s\n",
      "global step 840, epoch: 1, batch: 840, loss: 0.43686, accu: 0.90020, speed: 2.36 step/s\n",
      "global step 850, epoch: 1, batch: 850, loss: 0.42927, accu: 0.89781, speed: 2.08 step/s\n",
      "global step 860, epoch: 1, batch: 860, loss: 0.40071, accu: 0.90065, speed: 2.27 step/s\n",
      "global step 870, epoch: 1, batch: 870, loss: 0.39728, accu: 0.90167, speed: 2.24 step/s\n",
      "global step 880, epoch: 1, batch: 880, loss: 0.39452, accu: 0.90195, speed: 2.36 step/s\n",
      "global step 890, epoch: 1, batch: 890, loss: 0.39809, accu: 0.90122, speed: 2.39 step/s\n",
      "global step 900, epoch: 1, batch: 900, loss: 0.39840, accu: 0.90141, speed: 2.50 step/s\n",
      "eval dev loss: 0.44988, accu: 0.85651\n",
      "global step 910, epoch: 1, batch: 910, loss: 0.41034, accu: 0.91016, speed: 0.53 step/s\n",
      "global step 920, epoch: 1, batch: 920, loss: 0.45111, accu: 0.90938, speed: 2.35 step/s\n",
      "global step 930, epoch: 1, batch: 930, loss: 0.37733, accu: 0.90938, speed: 2.26 step/s\n",
      "global step 940, epoch: 1, batch: 940, loss: 0.41944, accu: 0.91035, speed: 2.43 step/s\n",
      "global step 950, epoch: 1, batch: 950, loss: 0.40483, accu: 0.90953, speed: 2.42 step/s\n",
      "global step 960, epoch: 1, batch: 960, loss: 0.43841, accu: 0.90938, speed: 2.42 step/s\n",
      "global step 970, epoch: 1, batch: 970, loss: 0.47690, accu: 0.90938, speed: 2.24 step/s\n",
      "global step 980, epoch: 1, batch: 980, loss: 0.37474, accu: 0.91025, speed: 2.43 step/s\n",
      "global step 990, epoch: 1, batch: 990, loss: 0.37388, accu: 0.91050, speed: 2.41 step/s\n",
      "global step 1000, epoch: 1, batch: 1000, loss: 0.40334, accu: 0.91039, speed: 2.41 step/s\n",
      "eval dev loss: 0.45861, accu: 0.85083\n",
      "global step 1010, epoch: 1, batch: 1010, loss: 0.39402, accu: 0.90312, speed: 0.53 step/s\n",
      "global step 1020, epoch: 1, batch: 1020, loss: 0.41877, accu: 0.90547, speed: 2.36 step/s\n",
      "global step 1030, epoch: 1, batch: 1030, loss: 0.37582, accu: 0.90312, speed: 2.41 step/s\n",
      "global step 1040, epoch: 1, batch: 1040, loss: 0.42094, accu: 0.90273, speed: 2.43 step/s\n",
      "global step 1050, epoch: 1, batch: 1050, loss: 0.41327, accu: 0.90422, speed: 2.30 step/s\n",
      "global step 1060, epoch: 1, batch: 1060, loss: 0.40960, accu: 0.90677, speed: 2.30 step/s\n",
      "global step 1070, epoch: 1, batch: 1070, loss: 0.43424, accu: 0.90491, speed: 2.42 step/s\n",
      "global step 1080, epoch: 1, batch: 1080, loss: 0.42208, accu: 0.90342, speed: 2.31 step/s\n",
      "global step 1090, epoch: 1, batch: 1090, loss: 0.40599, accu: 0.90417, speed: 2.44 step/s\n",
      "global step 1100, epoch: 1, batch: 1100, loss: 0.45540, accu: 0.90414, speed: 2.28 step/s\n",
      "eval dev loss: 0.47143, accu: 0.83549\n",
      "global step 1110, epoch: 1, batch: 1110, loss: 0.43861, accu: 0.90234, speed: 0.53 step/s\n",
      "global step 1120, epoch: 1, batch: 1120, loss: 0.38869, accu: 0.90273, speed: 2.28 step/s\n",
      "global step 1130, epoch: 1, batch: 1130, loss: 0.35309, accu: 0.90443, speed: 2.30 step/s\n",
      "global step 1140, epoch: 1, batch: 1140, loss: 0.43629, accu: 0.90215, speed: 2.47 step/s\n",
      "global step 1150, epoch: 1, batch: 1150, loss: 0.42858, accu: 0.90203, speed: 2.53 step/s\n",
      "global step 1160, epoch: 1, batch: 1160, loss: 0.41290, accu: 0.90156, speed: 2.39 step/s\n",
      "global step 1170, epoch: 1, batch: 1170, loss: 0.43441, accu: 0.90067, speed: 2.34 step/s\n",
      "global step 1180, epoch: 1, batch: 1180, loss: 0.40921, accu: 0.89990, speed: 2.26 step/s\n",
      "global step 1190, epoch: 1, batch: 1190, loss: 0.39770, accu: 0.90009, speed: 2.30 step/s\n",
      "global step 1200, epoch: 1, batch: 1200, loss: 0.42821, accu: 0.89969, speed: 2.39 step/s\n",
      "eval dev loss: 0.45124, accu: 0.85651\n",
      "global step 1210, epoch: 1, batch: 1210, loss: 0.41854, accu: 0.90625, speed: 0.53 step/s\n",
      "global step 1220, epoch: 1, batch: 1220, loss: 0.39797, accu: 0.90625, speed: 2.32 step/s\n",
      "global step 1230, epoch: 1, batch: 1230, loss: 0.39977, accu: 0.90833, speed: 2.40 step/s\n",
      "global step 1240, epoch: 1, batch: 1240, loss: 0.42113, accu: 0.90742, speed: 2.41 step/s\n",
      "global step 1250, epoch: 1, batch: 1250, loss: 0.42905, accu: 0.90719, speed: 2.42 step/s\n",
      "global step 1260, epoch: 1, batch: 1260, loss: 0.39747, accu: 0.90664, speed: 2.36 step/s\n",
      "global step 1270, epoch: 1, batch: 1270, loss: 0.40528, accu: 0.90513, speed: 2.30 step/s\n",
      "global step 1280, epoch: 1, batch: 1280, loss: 0.40814, accu: 0.90410, speed: 2.35 step/s\n",
      "global step 1290, epoch: 1, batch: 1290, loss: 0.42880, accu: 0.90391, speed: 2.41 step/s\n",
      "global step 1300, epoch: 1, batch: 1300, loss: 0.41958, accu: 0.90312, speed: 2.36 step/s\n",
      "eval dev loss: 0.46533, accu: 0.83799\n",
      "global step 1310, epoch: 1, batch: 1310, loss: 0.40500, accu: 0.90156, speed: 0.53 step/s\n",
      "global step 1320, epoch: 1, batch: 1320, loss: 0.42412, accu: 0.90195, speed: 2.39 step/s\n",
      "global step 1330, epoch: 1, batch: 1330, loss: 0.39498, accu: 0.90026, speed: 2.47 step/s\n",
      "global step 1340, epoch: 1, batch: 1340, loss: 0.38859, accu: 0.90312, speed: 2.53 step/s\n",
      "global step 1350, epoch: 1, batch: 1350, loss: 0.37542, accu: 0.90312, speed: 2.46 step/s\n",
      "global step 1360, epoch: 1, batch: 1360, loss: 0.42814, accu: 0.90156, speed: 2.35 step/s\n",
      "global step 1370, epoch: 1, batch: 1370, loss: 0.45590, accu: 0.90190, speed: 2.43 step/s\n",
      "global step 1380, epoch: 1, batch: 1380, loss: 0.36574, accu: 0.90332, speed: 2.45 step/s\n",
      "global step 1390, epoch: 1, batch: 1390, loss: 0.41912, accu: 0.90182, speed: 2.28 step/s\n",
      "global step 1400, epoch: 1, batch: 1400, loss: 0.39596, accu: 0.90148, speed: 2.31 step/s\n",
      "eval dev loss: 0.46364, accu: 0.8414\n",
      "global step 1410, epoch: 1, batch: 1410, loss: 0.39726, accu: 0.90000, speed: 0.53 step/s\n",
      "global step 1420, epoch: 1, batch: 1420, loss: 0.36415, accu: 0.90664, speed: 2.36 step/s\n",
      "global step 1430, epoch: 1, batch: 1430, loss: 0.39153, accu: 0.90495, speed: 2.45 step/s\n",
      "global step 1440, epoch: 1, batch: 1440, loss: 0.40325, accu: 0.90703, speed: 2.37 step/s\n",
      "global step 1450, epoch: 1, batch: 1450, loss: 0.37966, accu: 0.90766, speed: 2.42 step/s\n",
      "global step 1460, epoch: 1, batch: 1460, loss: 0.38073, accu: 0.90781, speed: 2.36 step/s\n",
      "global step 1470, epoch: 1, batch: 1470, loss: 0.40907, accu: 0.90915, speed: 2.35 step/s\n",
      "global step 1480, epoch: 1, batch: 1480, loss: 0.39247, accu: 0.90830, speed: 2.36 step/s\n",
      "global step 1490, epoch: 1, batch: 1490, loss: 0.41103, accu: 0.90703, speed: 2.39 step/s\n",
      "global step 1500, epoch: 1, batch: 1500, loss: 0.39702, accu: 0.90539, speed: 2.47 step/s\n",
      "eval dev loss: 0.45115, accu: 0.85799\n",
      "global step 1510, epoch: 1, batch: 1510, loss: 0.41561, accu: 0.89609, speed: 0.52 step/s\n",
      "global step 1520, epoch: 1, batch: 1520, loss: 0.39492, accu: 0.90586, speed: 2.27 step/s\n",
      "global step 1530, epoch: 1, batch: 1530, loss: 0.37990, accu: 0.90234, speed: 2.37 step/s\n",
      "global step 1540, epoch: 1, batch: 1540, loss: 0.43601, accu: 0.90176, speed: 2.58 step/s\n",
      "global step 1550, epoch: 1, batch: 1550, loss: 0.40268, accu: 0.90109, speed: 2.40 step/s\n",
      "global step 1560, epoch: 1, batch: 1560, loss: 0.44172, accu: 0.90260, speed: 2.39 step/s\n",
      "global step 1570, epoch: 1, batch: 1570, loss: 0.40924, accu: 0.90458, speed: 2.42 step/s\n",
      "global step 1580, epoch: 1, batch: 1580, loss: 0.37833, accu: 0.90615, speed: 2.35 step/s\n",
      "global step 1590, epoch: 1, batch: 1590, loss: 0.40429, accu: 0.90590, speed: 2.40 step/s\n",
      "global step 1600, epoch: 1, batch: 1600, loss: 0.40859, accu: 0.90617, speed: 2.51 step/s\n",
      "eval dev loss: 0.461, accu: 0.84776\n",
      "global step 1610, epoch: 1, batch: 1610, loss: 0.42396, accu: 0.90547, speed: 0.53 step/s\n",
      "global step 1620, epoch: 1, batch: 1620, loss: 0.40837, accu: 0.90391, speed: 2.40 step/s\n",
      "global step 1630, epoch: 1, batch: 1630, loss: 0.41200, accu: 0.90130, speed: 2.54 step/s\n",
      "global step 1640, epoch: 1, batch: 1640, loss: 0.36615, accu: 0.90117, speed: 2.59 step/s\n",
      "global step 1650, epoch: 1, batch: 1650, loss: 0.41306, accu: 0.90141, speed: 2.15 step/s\n",
      "global step 1660, epoch: 1, batch: 1660, loss: 0.36943, accu: 0.90247, speed: 2.35 step/s\n",
      "global step 1670, epoch: 1, batch: 1670, loss: 0.37167, accu: 0.90491, speed: 2.47 step/s\n",
      "global step 1680, epoch: 1, batch: 1680, loss: 0.36564, accu: 0.90635, speed: 2.31 step/s\n",
      "global step 1690, epoch: 1, batch: 1690, loss: 0.40136, accu: 0.90564, speed: 2.34 step/s\n",
      "global step 1700, epoch: 1, batch: 1700, loss: 0.38512, accu: 0.90516, speed: 2.32 step/s\n",
      "eval dev loss: 0.46878, accu: 0.83549\n",
      "global step 1710, epoch: 1, batch: 1710, loss: 0.39396, accu: 0.89766, speed: 0.53 step/s\n",
      "global step 1720, epoch: 1, batch: 1720, loss: 0.40976, accu: 0.89883, speed: 2.47 step/s\n",
      "global step 1730, epoch: 1, batch: 1730, loss: 0.38567, accu: 0.90234, speed: 2.32 step/s\n",
      "global step 1740, epoch: 1, batch: 1740, loss: 0.40403, accu: 0.90332, speed: 2.42 step/s\n",
      "global step 1750, epoch: 1, batch: 1750, loss: 0.38942, accu: 0.90266, speed: 2.43 step/s\n",
      "global step 1760, epoch: 1, batch: 1760, loss: 0.40534, accu: 0.90065, speed: 2.38 step/s\n",
      "global step 1770, epoch: 1, batch: 1770, loss: 0.36714, accu: 0.90201, speed: 2.41 step/s\n",
      "global step 1780, epoch: 1, batch: 1780, loss: 0.41066, accu: 0.90371, speed: 2.35 step/s\n",
      "global step 1790, epoch: 1, batch: 1790, loss: 0.39028, accu: 0.90339, speed: 2.37 step/s\n",
      "global step 1800, epoch: 1, batch: 1800, loss: 0.38048, accu: 0.90422, speed: 2.34 step/s\n",
      "eval dev loss: 0.45279, accu: 0.8564\n",
      "global step 1810, epoch: 1, batch: 1810, loss: 0.38157, accu: 0.92422, speed: 0.53 step/s\n",
      "global step 1820, epoch: 1, batch: 1820, loss: 0.40360, accu: 0.91680, speed: 2.47 step/s\n",
      "global step 1830, epoch: 1, batch: 1830, loss: 0.41740, accu: 0.90677, speed: 2.34 step/s\n",
      "global step 1840, epoch: 1, batch: 1840, loss: 0.40048, accu: 0.90762, speed: 2.59 step/s\n",
      "global step 1850, epoch: 1, batch: 1850, loss: 0.40029, accu: 0.90750, speed: 2.41 step/s\n",
      "global step 1860, epoch: 1, batch: 1860, loss: 0.39277, accu: 0.90664, speed: 2.24 step/s\n",
      "global step 1870, epoch: 2, batch: 4, loss: 0.38864, accu: 0.90707, speed: 2.54 step/s\n",
      "global step 1880, epoch: 2, batch: 14, loss: 0.42840, accu: 0.90874, speed: 2.52 step/s\n",
      "global step 1890, epoch: 2, batch: 24, loss: 0.39807, accu: 0.90873, speed: 2.41 step/s\n",
      "global step 1900, epoch: 2, batch: 34, loss: 0.38360, accu: 0.90958, speed: 2.39 step/s\n",
      "eval dev loss: 0.44533, accu: 0.8614\n",
      "global step 1910, epoch: 2, batch: 44, loss: 0.37287, accu: 0.91016, speed: 0.53 step/s\n",
      "global step 1920, epoch: 2, batch: 54, loss: 0.41389, accu: 0.91133, speed: 2.34 step/s\n",
      "global step 1930, epoch: 2, batch: 64, loss: 0.40579, accu: 0.91406, speed: 2.38 step/s\n",
      "global step 1940, epoch: 2, batch: 74, loss: 0.40144, accu: 0.91504, speed: 2.48 step/s\n",
      "global step 1950, epoch: 2, batch: 84, loss: 0.43128, accu: 0.91406, speed: 2.32 step/s\n",
      "global step 1960, epoch: 2, batch: 94, loss: 0.38486, accu: 0.91589, speed: 2.49 step/s\n",
      "global step 1970, epoch: 2, batch: 104, loss: 0.42181, accu: 0.91730, speed: 2.36 step/s\n",
      "global step 1980, epoch: 2, batch: 114, loss: 0.38812, accu: 0.91768, speed: 2.47 step/s\n",
      "global step 1990, epoch: 2, batch: 124, loss: 0.41603, accu: 0.91736, speed: 2.23 step/s\n",
      "global step 2000, epoch: 2, batch: 134, loss: 0.39602, accu: 0.91789, speed: 2.19 step/s\n",
      "eval dev loss: 0.44212, accu: 0.86753\n",
      "global step 2010, epoch: 2, batch: 144, loss: 0.38004, accu: 0.91406, speed: 0.54 step/s\n",
      "global step 2020, epoch: 2, batch: 154, loss: 0.36902, accu: 0.91875, speed: 2.29 step/s\n",
      "global step 2030, epoch: 2, batch: 164, loss: 0.36962, accu: 0.92005, speed: 2.31 step/s\n",
      "global step 2040, epoch: 2, batch: 174, loss: 0.34975, accu: 0.91777, speed: 2.34 step/s\n",
      "global step 2050, epoch: 2, batch: 184, loss: 0.40658, accu: 0.92063, speed: 2.42 step/s\n",
      "global step 2060, epoch: 2, batch: 194, loss: 0.36516, accu: 0.91953, speed: 2.43 step/s\n",
      "global step 2070, epoch: 2, batch: 204, loss: 0.39047, accu: 0.91775, speed: 2.37 step/s\n",
      "global step 2080, epoch: 2, batch: 214, loss: 0.41078, accu: 0.91641, speed: 2.64 step/s\n",
      "global step 2090, epoch: 2, batch: 224, loss: 0.36514, accu: 0.91771, speed: 2.51 step/s\n",
      "global step 2100, epoch: 2, batch: 234, loss: 0.37057, accu: 0.91828, speed: 2.47 step/s\n",
      "eval dev loss: 0.45656, accu: 0.85424\n",
      "global step 2110, epoch: 2, batch: 244, loss: 0.36712, accu: 0.92656, speed: 0.53 step/s\n",
      "global step 2120, epoch: 2, batch: 254, loss: 0.43654, accu: 0.91719, speed: 2.52 step/s\n",
      "global step 2130, epoch: 2, batch: 264, loss: 0.41982, accu: 0.91146, speed: 2.40 step/s\n",
      "global step 2140, epoch: 2, batch: 274, loss: 0.38610, accu: 0.91387, speed: 2.29 step/s\n",
      "global step 2150, epoch: 2, batch: 284, loss: 0.42265, accu: 0.91391, speed: 2.35 step/s\n",
      "global step 2160, epoch: 2, batch: 294, loss: 0.37516, accu: 0.91458, speed: 2.21 step/s\n",
      "global step 2170, epoch: 2, batch: 304, loss: 0.40456, accu: 0.91484, speed: 2.28 step/s\n",
      "global step 2180, epoch: 2, batch: 314, loss: 0.41227, accu: 0.91533, speed: 2.35 step/s\n",
      "global step 2190, epoch: 2, batch: 324, loss: 0.36295, accu: 0.91519, speed: 2.35 step/s\n",
      "global step 2200, epoch: 2, batch: 334, loss: 0.42433, accu: 0.91625, speed: 2.46 step/s\n",
      "eval dev loss: 0.44553, accu: 0.8623\n",
      "global step 2210, epoch: 2, batch: 344, loss: 0.36618, accu: 0.91797, speed: 0.53 step/s\n",
      "global step 2220, epoch: 2, batch: 354, loss: 0.39134, accu: 0.91875, speed: 2.35 step/s\n",
      "global step 2230, epoch: 2, batch: 364, loss: 0.37982, accu: 0.91927, speed: 2.40 step/s\n",
      "global step 2240, epoch: 2, batch: 374, loss: 0.40019, accu: 0.91777, speed: 2.37 step/s\n",
      "global step 2250, epoch: 2, batch: 384, loss: 0.39923, accu: 0.91734, speed: 2.51 step/s\n",
      "global step 2260, epoch: 2, batch: 394, loss: 0.36445, accu: 0.91628, speed: 2.04 step/s\n",
      "global step 2270, epoch: 2, batch: 404, loss: 0.41719, accu: 0.91484, speed: 2.39 step/s\n",
      "global step 2280, epoch: 2, batch: 414, loss: 0.40692, accu: 0.91553, speed: 2.43 step/s\n",
      "global step 2290, epoch: 2, batch: 424, loss: 0.38187, accu: 0.91667, speed: 2.39 step/s\n",
      "global step 2300, epoch: 2, batch: 434, loss: 0.42092, accu: 0.91672, speed: 2.48 step/s\n",
      "eval dev loss: 0.43913, accu: 0.87048\n",
      "global step 2310, epoch: 2, batch: 444, loss: 0.41324, accu: 0.92891, speed: 0.53 step/s\n",
      "global step 2320, epoch: 2, batch: 454, loss: 0.38734, accu: 0.92148, speed: 2.35 step/s\n",
      "global step 2330, epoch: 2, batch: 464, loss: 0.38597, accu: 0.92188, speed: 2.36 step/s\n",
      "global step 2340, epoch: 2, batch: 474, loss: 0.38944, accu: 0.92012, speed: 2.48 step/s\n",
      "global step 2350, epoch: 2, batch: 484, loss: 0.40217, accu: 0.92031, speed: 2.29 step/s\n",
      "global step 2360, epoch: 2, batch: 494, loss: 0.40095, accu: 0.92109, speed: 2.44 step/s\n",
      "global step 2370, epoch: 2, batch: 504, loss: 0.37897, accu: 0.92176, speed: 2.32 step/s\n",
      "global step 2380, epoch: 2, batch: 514, loss: 0.37253, accu: 0.92109, speed: 2.38 step/s\n",
      "global step 2390, epoch: 2, batch: 524, loss: 0.42807, accu: 0.91927, speed: 2.33 step/s\n",
      "global step 2400, epoch: 2, batch: 534, loss: 0.37871, accu: 0.91930, speed: 2.44 step/s\n",
      "eval dev loss: 0.443, accu: 0.86378\n",
      "global step 2410, epoch: 2, batch: 544, loss: 0.38511, accu: 0.91406, speed: 0.53 step/s\n",
      "global step 2420, epoch: 2, batch: 554, loss: 0.37241, accu: 0.91094, speed: 2.33 step/s\n",
      "global step 2430, epoch: 2, batch: 564, loss: 0.41296, accu: 0.91198, speed: 2.34 step/s\n",
      "global step 2440, epoch: 2, batch: 574, loss: 0.40171, accu: 0.91367, speed: 2.39 step/s\n",
      "global step 2450, epoch: 2, batch: 584, loss: 0.39699, accu: 0.91594, speed: 2.34 step/s\n",
      "global step 2460, epoch: 2, batch: 594, loss: 0.41257, accu: 0.91628, speed: 2.45 step/s\n",
      "global step 2470, epoch: 2, batch: 604, loss: 0.35376, accu: 0.91674, speed: 2.53 step/s\n",
      "global step 2480, epoch: 2, batch: 614, loss: 0.36252, accu: 0.91816, speed: 2.40 step/s\n",
      "global step 2490, epoch: 2, batch: 624, loss: 0.42105, accu: 0.91675, speed: 2.43 step/s\n",
      "global step 2500, epoch: 2, batch: 634, loss: 0.37478, accu: 0.91711, speed: 2.52 step/s\n",
      "eval dev loss: 0.44476, accu: 0.86412\n",
      "global step 2510, epoch: 2, batch: 644, loss: 0.40461, accu: 0.92188, speed: 0.53 step/s\n",
      "global step 2520, epoch: 2, batch: 654, loss: 0.36997, accu: 0.92109, speed: 2.50 step/s\n",
      "global step 2530, epoch: 2, batch: 664, loss: 0.42452, accu: 0.91927, speed: 2.35 step/s\n",
      "global step 2540, epoch: 2, batch: 674, loss: 0.38623, accu: 0.91934, speed: 2.26 step/s\n",
      "global step 2550, epoch: 2, batch: 684, loss: 0.36683, accu: 0.91828, speed: 2.27 step/s\n",
      "global step 2560, epoch: 2, batch: 694, loss: 0.39320, accu: 0.91810, speed: 2.33 step/s\n",
      "global step 2570, epoch: 2, batch: 704, loss: 0.36545, accu: 0.91908, speed: 2.44 step/s\n",
      "global step 2580, epoch: 2, batch: 714, loss: 0.37336, accu: 0.91982, speed: 2.34 step/s\n",
      "global step 2590, epoch: 2, batch: 724, loss: 0.38136, accu: 0.91970, speed: 2.28 step/s\n",
      "global step 2600, epoch: 2, batch: 734, loss: 0.40993, accu: 0.91922, speed: 2.29 step/s\n",
      "eval dev loss: 0.44237, accu: 0.86458\n",
      "global step 2610, epoch: 2, batch: 744, loss: 0.37680, accu: 0.92109, speed: 0.53 step/s\n",
      "global step 2620, epoch: 2, batch: 754, loss: 0.37774, accu: 0.91836, speed: 2.38 step/s\n",
      "global step 2630, epoch: 2, batch: 764, loss: 0.39155, accu: 0.91953, speed: 2.32 step/s\n",
      "global step 2640, epoch: 2, batch: 774, loss: 0.39391, accu: 0.91855, speed: 2.47 step/s\n",
      "global step 2650, epoch: 2, batch: 784, loss: 0.37141, accu: 0.91891, speed: 2.31 step/s\n",
      "global step 2660, epoch: 2, batch: 794, loss: 0.35431, accu: 0.92031, speed: 2.30 step/s\n",
      "global step 2670, epoch: 2, batch: 804, loss: 0.38515, accu: 0.92076, speed: 2.29 step/s\n",
      "global step 2680, epoch: 2, batch: 814, loss: 0.38132, accu: 0.92070, speed: 2.28 step/s\n",
      "global step 2690, epoch: 2, batch: 824, loss: 0.40313, accu: 0.91962, speed: 2.39 step/s\n",
      "global step 2700, epoch: 2, batch: 834, loss: 0.39413, accu: 0.91828, speed: 2.26 step/s\n",
      "eval dev loss: 0.43578, accu: 0.87242\n",
      "global step 2710, epoch: 2, batch: 844, loss: 0.35810, accu: 0.92266, speed: 0.53 step/s\n",
      "global step 2720, epoch: 2, batch: 854, loss: 0.40713, accu: 0.91836, speed: 2.41 step/s\n",
      "global step 2730, epoch: 2, batch: 864, loss: 0.40666, accu: 0.91745, speed: 2.34 step/s\n",
      "global step 2740, epoch: 2, batch: 874, loss: 0.38732, accu: 0.91719, speed: 2.44 step/s\n",
      "global step 2750, epoch: 2, batch: 884, loss: 0.38362, accu: 0.91594, speed: 2.19 step/s\n",
      "global step 2760, epoch: 2, batch: 894, loss: 0.39118, accu: 0.91641, speed: 2.36 step/s\n",
      "global step 2770, epoch: 2, batch: 904, loss: 0.41757, accu: 0.91730, speed: 2.28 step/s\n",
      "global step 2780, epoch: 2, batch: 914, loss: 0.38559, accu: 0.91680, speed: 2.35 step/s\n",
      "global step 2790, epoch: 2, batch: 924, loss: 0.37502, accu: 0.91719, speed: 2.38 step/s\n",
      "global step 2800, epoch: 2, batch: 934, loss: 0.38398, accu: 0.91773, speed: 2.30 step/s\n",
      "eval dev loss: 0.43968, accu: 0.87105\n",
      "global step 2810, epoch: 2, batch: 944, loss: 0.40964, accu: 0.91094, speed: 0.53 step/s\n",
      "global step 2820, epoch: 2, batch: 954, loss: 0.38638, accu: 0.91250, speed: 2.38 step/s\n",
      "global step 2830, epoch: 2, batch: 964, loss: 0.36355, accu: 0.91458, speed: 2.32 step/s\n",
      "global step 2840, epoch: 2, batch: 974, loss: 0.35294, accu: 0.91484, speed: 2.67 step/s\n",
      "global step 2850, epoch: 2, batch: 984, loss: 0.37027, accu: 0.91625, speed: 2.40 step/s\n",
      "global step 2860, epoch: 2, batch: 994, loss: 0.40629, accu: 0.91680, speed: 2.29 step/s\n",
      "global step 2870, epoch: 2, batch: 1004, loss: 0.42743, accu: 0.91741, speed: 2.43 step/s\n",
      "global step 2880, epoch: 2, batch: 1014, loss: 0.41678, accu: 0.91797, speed: 2.30 step/s\n",
      "global step 2890, epoch: 2, batch: 1024, loss: 0.34249, accu: 0.91875, speed: 2.34 step/s\n",
      "global step 2900, epoch: 2, batch: 1034, loss: 0.41942, accu: 0.91805, speed: 2.31 step/s\n",
      "eval dev loss: 0.43881, accu: 0.87196\n",
      "global step 2910, epoch: 2, batch: 1044, loss: 0.39882, accu: 0.90938, speed: 0.53 step/s\n",
      "global step 2920, epoch: 2, batch: 1054, loss: 0.40374, accu: 0.91133, speed: 2.45 step/s\n",
      "global step 2930, epoch: 2, batch: 1064, loss: 0.37508, accu: 0.91510, speed: 2.52 step/s\n",
      "global step 2940, epoch: 2, batch: 1074, loss: 0.38115, accu: 0.91563, speed: 2.49 step/s\n",
      "global step 2950, epoch: 2, batch: 1084, loss: 0.39576, accu: 0.91812, speed: 2.32 step/s\n",
      "global step 2960, epoch: 2, batch: 1094, loss: 0.38366, accu: 0.91927, speed: 2.27 step/s\n",
      "global step 2970, epoch: 2, batch: 1104, loss: 0.33273, accu: 0.91964, speed: 2.40 step/s\n",
      "global step 2980, epoch: 2, batch: 1114, loss: 0.39211, accu: 0.92109, speed: 2.46 step/s\n",
      "global step 2990, epoch: 2, batch: 1124, loss: 0.42564, accu: 0.92014, speed: 2.42 step/s\n",
      "global step 3000, epoch: 2, batch: 1134, loss: 0.41805, accu: 0.91984, speed: 2.42 step/s\n",
      "eval dev loss: 0.44682, accu: 0.86162\n",
      "global step 3010, epoch: 2, batch: 1144, loss: 0.35532, accu: 0.92188, speed: 0.53 step/s\n",
      "global step 3020, epoch: 2, batch: 1154, loss: 0.40870, accu: 0.92617, speed: 2.50 step/s\n",
      "global step 3030, epoch: 2, batch: 1164, loss: 0.39932, accu: 0.92240, speed: 2.49 step/s\n",
      "global step 3040, epoch: 2, batch: 1174, loss: 0.39756, accu: 0.92129, speed: 2.48 step/s\n",
      "global step 3050, epoch: 2, batch: 1184, loss: 0.42146, accu: 0.92094, speed: 2.50 step/s\n",
      "global step 3060, epoch: 2, batch: 1194, loss: 0.40760, accu: 0.92201, speed: 2.48 step/s\n",
      "global step 3070, epoch: 2, batch: 1204, loss: 0.39053, accu: 0.92087, speed: 2.42 step/s\n",
      "global step 3080, epoch: 2, batch: 1214, loss: 0.40510, accu: 0.92236, speed: 2.25 step/s\n",
      "global step 3090, epoch: 2, batch: 1224, loss: 0.39842, accu: 0.92092, speed: 2.42 step/s\n",
      "global step 3100, epoch: 2, batch: 1234, loss: 0.40244, accu: 0.92000, speed: 2.36 step/s\n",
      "eval dev loss: 0.43683, accu: 0.8698\n",
      "global step 3110, epoch: 2, batch: 1244, loss: 0.38032, accu: 0.92266, speed: 0.53 step/s\n",
      "global step 3120, epoch: 2, batch: 1254, loss: 0.36842, accu: 0.92266, speed: 2.31 step/s\n",
      "global step 3130, epoch: 2, batch: 1264, loss: 0.39772, accu: 0.92422, speed: 2.34 step/s\n",
      "global step 3140, epoch: 2, batch: 1274, loss: 0.37223, accu: 0.92090, speed: 2.35 step/s\n",
      "global step 3150, epoch: 2, batch: 1284, loss: 0.40069, accu: 0.92063, speed: 2.46 step/s\n",
      "global step 3160, epoch: 2, batch: 1294, loss: 0.39672, accu: 0.92057, speed: 2.48 step/s\n",
      "global step 3170, epoch: 2, batch: 1304, loss: 0.35191, accu: 0.92098, speed: 2.44 step/s\n",
      "global step 3180, epoch: 2, batch: 1314, loss: 0.39426, accu: 0.92148, speed: 2.38 step/s\n",
      "global step 3190, epoch: 2, batch: 1324, loss: 0.39839, accu: 0.92118, speed: 2.44 step/s\n",
      "global step 3200, epoch: 2, batch: 1334, loss: 0.42366, accu: 0.91977, speed: 2.15 step/s\n",
      "eval dev loss: 0.43661, accu: 0.8706\n",
      "global step 3210, epoch: 2, batch: 1344, loss: 0.40718, accu: 0.91953, speed: 0.53 step/s\n",
      "global step 3220, epoch: 2, batch: 1354, loss: 0.37794, accu: 0.92422, speed: 2.35 step/s\n",
      "global step 3230, epoch: 2, batch: 1364, loss: 0.39930, accu: 0.92188, speed: 2.32 step/s\n",
      "global step 3240, epoch: 2, batch: 1374, loss: 0.40412, accu: 0.92129, speed: 2.30 step/s\n",
      "global step 3250, epoch: 2, batch: 1384, loss: 0.36001, accu: 0.92328, speed: 2.44 step/s\n",
      "global step 3260, epoch: 2, batch: 1394, loss: 0.40463, accu: 0.92305, speed: 2.43 step/s\n",
      "global step 3270, epoch: 2, batch: 1404, loss: 0.39804, accu: 0.92199, speed: 2.12 step/s\n",
      "global step 3280, epoch: 2, batch: 1414, loss: 0.40429, accu: 0.92119, speed: 2.42 step/s\n",
      "global step 3290, epoch: 2, batch: 1424, loss: 0.40581, accu: 0.92153, speed: 2.54 step/s\n",
      "global step 3300, epoch: 2, batch: 1434, loss: 0.43329, accu: 0.92047, speed: 2.41 step/s\n",
      "eval dev loss: 0.43462, accu: 0.87423\n",
      "global step 3310, epoch: 2, batch: 1444, loss: 0.41379, accu: 0.91719, speed: 0.54 step/s\n",
      "global step 3320, epoch: 2, batch: 1454, loss: 0.38022, accu: 0.91563, speed: 2.42 step/s\n",
      "global step 3330, epoch: 2, batch: 1464, loss: 0.40236, accu: 0.91927, speed: 2.32 step/s\n",
      "global step 3340, epoch: 2, batch: 1474, loss: 0.37791, accu: 0.91895, speed: 2.40 step/s\n",
      "global step 3350, epoch: 2, batch: 1484, loss: 0.38787, accu: 0.91625, speed: 2.38 step/s\n",
      "global step 3360, epoch: 2, batch: 1494, loss: 0.37279, accu: 0.91719, speed: 2.47 step/s\n",
      "global step 3370, epoch: 2, batch: 1504, loss: 0.41663, accu: 0.91775, speed: 2.50 step/s\n",
      "global step 3380, epoch: 2, batch: 1514, loss: 0.35735, accu: 0.91816, speed: 2.27 step/s\n",
      "global step 3390, epoch: 2, batch: 1524, loss: 0.40250, accu: 0.91806, speed: 2.46 step/s\n",
      "global step 3400, epoch: 2, batch: 1534, loss: 0.35115, accu: 0.91828, speed: 2.37 step/s\n",
      "eval dev loss: 0.42849, accu: 0.87935\n",
      "global step 3410, epoch: 2, batch: 1544, loss: 0.37013, accu: 0.92109, speed: 0.52 step/s\n",
      "global step 3420, epoch: 2, batch: 1554, loss: 0.41406, accu: 0.91563, speed: 2.41 step/s\n",
      "global step 3430, epoch: 2, batch: 1564, loss: 0.34600, accu: 0.92474, speed: 2.57 step/s\n",
      "global step 3440, epoch: 2, batch: 1574, loss: 0.41031, accu: 0.92031, speed: 2.39 step/s\n",
      "global step 3450, epoch: 2, batch: 1584, loss: 0.39276, accu: 0.92016, speed: 2.46 step/s\n",
      "global step 3460, epoch: 2, batch: 1594, loss: 0.37609, accu: 0.91927, speed: 2.47 step/s\n",
      "global step 3470, epoch: 2, batch: 1604, loss: 0.39126, accu: 0.92087, speed: 2.30 step/s\n",
      "global step 3480, epoch: 2, batch: 1614, loss: 0.35186, accu: 0.92109, speed: 2.31 step/s\n",
      "global step 3490, epoch: 2, batch: 1624, loss: 0.36017, accu: 0.92222, speed: 2.59 step/s\n",
      "global step 3500, epoch: 2, batch: 1634, loss: 0.41134, accu: 0.92125, speed: 2.42 step/s\n",
      "eval dev loss: 0.42829, accu: 0.88128\n",
      "global step 3510, epoch: 2, batch: 1644, loss: 0.38298, accu: 0.91172, speed: 0.53 step/s\n",
      "global step 3520, epoch: 2, batch: 1654, loss: 0.40737, accu: 0.92070, speed: 2.26 step/s\n",
      "global step 3530, epoch: 2, batch: 1664, loss: 0.38473, accu: 0.92057, speed: 2.40 step/s\n",
      "global step 3540, epoch: 2, batch: 1674, loss: 0.35517, accu: 0.92188, speed: 2.41 step/s\n",
      "global step 3550, epoch: 2, batch: 1684, loss: 0.40821, accu: 0.92188, speed: 2.44 step/s\n",
      "global step 3560, epoch: 2, batch: 1694, loss: 0.39532, accu: 0.92174, speed: 2.42 step/s\n",
      "global step 3570, epoch: 2, batch: 1704, loss: 0.39082, accu: 0.92143, speed: 2.39 step/s\n",
      "global step 3580, epoch: 2, batch: 1714, loss: 0.39933, accu: 0.92324, speed: 2.35 step/s\n",
      "global step 3590, epoch: 2, batch: 1724, loss: 0.40070, accu: 0.92266, speed: 2.31 step/s\n",
      "global step 3600, epoch: 2, batch: 1734, loss: 0.37589, accu: 0.92250, speed: 2.30 step/s\n",
      "eval dev loss: 0.43218, accu: 0.87685\n",
      "global step 3610, epoch: 2, batch: 1744, loss: 0.36925, accu: 0.93125, speed: 0.52 step/s\n",
      "global step 3620, epoch: 2, batch: 1754, loss: 0.34734, accu: 0.93398, speed: 2.45 step/s\n",
      "global step 3630, epoch: 2, batch: 1764, loss: 0.39199, accu: 0.93229, speed: 2.46 step/s\n",
      "global step 3640, epoch: 2, batch: 1774, loss: 0.40835, accu: 0.92852, speed: 2.24 step/s\n",
      "global step 3650, epoch: 2, batch: 1784, loss: 0.39289, accu: 0.92656, speed: 2.44 step/s\n",
      "global step 3660, epoch: 2, batch: 1794, loss: 0.37498, accu: 0.92240, speed: 2.34 step/s\n",
      "global step 3670, epoch: 2, batch: 1804, loss: 0.39046, accu: 0.92288, speed: 2.50 step/s\n",
      "global step 3680, epoch: 2, batch: 1814, loss: 0.40517, accu: 0.92207, speed: 2.34 step/s\n",
      "global step 3690, epoch: 2, batch: 1824, loss: 0.41585, accu: 0.92049, speed: 2.35 step/s\n",
      "global step 3700, epoch: 2, batch: 1834, loss: 0.38029, accu: 0.92047, speed: 2.35 step/s\n",
      "eval dev loss: 0.4254, accu: 0.88491\n",
      "global step 3710, epoch: 2, batch: 1844, loss: 0.42214, accu: 0.92031, speed: 0.53 step/s\n",
      "global step 3720, epoch: 2, batch: 1854, loss: 0.40496, accu: 0.91836, speed: 2.27 step/s\n",
      "global step 3730, epoch: 2, batch: 1864, loss: 0.39418, accu: 0.92005, speed: 2.31 step/s\n",
      "global step 3740, epoch: 3, batch: 8, loss: 0.37211, accu: 0.92060, speed: 2.50 step/s\n",
      "global step 3750, epoch: 3, batch: 18, loss: 0.38073, accu: 0.92355, speed: 2.34 step/s\n",
      "global step 3760, epoch: 3, batch: 28, loss: 0.39368, accu: 0.92393, speed: 2.12 step/s\n",
      "global step 3770, epoch: 3, batch: 38, loss: 0.40252, accu: 0.92476, speed: 2.43 step/s\n",
      "global step 3780, epoch: 3, batch: 48, loss: 0.37942, accu: 0.92538, speed: 2.39 step/s\n",
      "global step 3790, epoch: 3, batch: 58, loss: 0.35885, accu: 0.92569, speed: 2.33 step/s\n",
      "global step 3800, epoch: 3, batch: 68, loss: 0.36463, accu: 0.92617, speed: 2.29 step/s\n",
      "eval dev loss: 0.43201, accu: 0.87719\n",
      "global step 3810, epoch: 3, batch: 78, loss: 0.38248, accu: 0.93047, speed: 0.53 step/s\n",
      "global step 3820, epoch: 3, batch: 88, loss: 0.36920, accu: 0.93203, speed: 2.41 step/s\n",
      "global step 3830, epoch: 3, batch: 98, loss: 0.34733, accu: 0.93229, speed: 2.60 step/s\n",
      "global step 3840, epoch: 3, batch: 108, loss: 0.39276, accu: 0.92910, speed: 2.37 step/s\n",
      "global step 3850, epoch: 3, batch: 118, loss: 0.41458, accu: 0.92984, speed: 2.36 step/s\n",
      "global step 3860, epoch: 3, batch: 128, loss: 0.37581, accu: 0.93099, speed: 2.40 step/s\n",
      "global step 3870, epoch: 3, batch: 138, loss: 0.40072, accu: 0.93136, speed: 2.46 step/s\n",
      "global step 3880, epoch: 3, batch: 148, loss: 0.38350, accu: 0.93096, speed: 2.56 step/s\n",
      "global step 3890, epoch: 3, batch: 158, loss: 0.37527, accu: 0.93082, speed: 2.23 step/s\n",
      "global step 3900, epoch: 3, batch: 168, loss: 0.38129, accu: 0.93133, speed: 2.30 step/s\n",
      "eval dev loss: 0.42943, accu: 0.87957\n",
      "global step 3910, epoch: 3, batch: 178, loss: 0.36322, accu: 0.92656, speed: 0.53 step/s\n",
      "global step 3920, epoch: 3, batch: 188, loss: 0.37522, accu: 0.92773, speed: 2.38 step/s\n",
      "global step 3930, epoch: 3, batch: 198, loss: 0.41988, accu: 0.92448, speed: 2.36 step/s\n",
      "global step 3940, epoch: 3, batch: 208, loss: 0.39843, accu: 0.92461, speed: 2.43 step/s\n",
      "global step 3950, epoch: 3, batch: 218, loss: 0.37258, accu: 0.92688, speed: 2.40 step/s\n",
      "global step 3960, epoch: 3, batch: 228, loss: 0.37848, accu: 0.92760, speed: 2.33 step/s\n",
      "global step 3970, epoch: 3, batch: 238, loss: 0.37941, accu: 0.92857, speed: 2.54 step/s\n",
      "global step 3980, epoch: 3, batch: 248, loss: 0.39111, accu: 0.92842, speed: 2.32 step/s\n",
      "global step 3990, epoch: 3, batch: 258, loss: 0.38415, accu: 0.92917, speed: 2.39 step/s\n",
      "global step 4000, epoch: 3, batch: 268, loss: 0.37956, accu: 0.92898, speed: 2.31 step/s\n",
      "eval dev loss: 0.42397, accu: 0.88491\n",
      "global step 4010, epoch: 3, batch: 278, loss: 0.38220, accu: 0.93125, speed: 0.52 step/s\n",
      "global step 4020, epoch: 3, batch: 288, loss: 0.40993, accu: 0.92891, speed: 2.40 step/s\n",
      "global step 4030, epoch: 3, batch: 298, loss: 0.39056, accu: 0.93073, speed: 2.21 step/s\n",
      "global step 4040, epoch: 3, batch: 308, loss: 0.37433, accu: 0.93086, speed: 2.45 step/s\n",
      "global step 4050, epoch: 3, batch: 318, loss: 0.39578, accu: 0.92859, speed: 2.50 step/s\n",
      "global step 4060, epoch: 3, batch: 328, loss: 0.39138, accu: 0.93021, speed: 2.33 step/s\n",
      "global step 4070, epoch: 3, batch: 338, loss: 0.39582, accu: 0.93203, speed: 2.31 step/s\n",
      "global step 4080, epoch: 3, batch: 348, loss: 0.36431, accu: 0.93350, speed: 2.44 step/s\n",
      "global step 4090, epoch: 3, batch: 358, loss: 0.37417, accu: 0.93377, speed: 2.36 step/s\n",
      "global step 4100, epoch: 3, batch: 368, loss: 0.35624, accu: 0.93336, speed: 2.34 step/s\n",
      "eval dev loss: 0.42994, accu: 0.87935\n",
      "global step 4110, epoch: 3, batch: 378, loss: 0.36902, accu: 0.93672, speed: 0.52 step/s\n",
      "global step 4120, epoch: 3, batch: 388, loss: 0.35474, accu: 0.93867, speed: 2.38 step/s\n",
      "global step 4130, epoch: 3, batch: 398, loss: 0.39144, accu: 0.93203, speed: 2.35 step/s\n",
      "global step 4140, epoch: 3, batch: 408, loss: 0.36630, accu: 0.93477, speed: 2.40 step/s\n",
      "global step 4150, epoch: 3, batch: 418, loss: 0.35946, accu: 0.93422, speed: 2.40 step/s\n",
      "global step 4160, epoch: 3, batch: 428, loss: 0.35613, accu: 0.93385, speed: 2.41 step/s\n",
      "global step 4170, epoch: 3, batch: 438, loss: 0.38193, accu: 0.93449, speed: 2.25 step/s\n",
      "global step 4180, epoch: 3, batch: 448, loss: 0.37103, accu: 0.93467, speed: 2.32 step/s\n",
      "global step 4190, epoch: 3, batch: 458, loss: 0.36964, accu: 0.93446, speed: 2.45 step/s\n",
      "global step 4200, epoch: 3, batch: 468, loss: 0.36545, accu: 0.93344, speed: 2.38 step/s\n",
      "eval dev loss: 0.43211, accu: 0.87605\n",
      "global step 4210, epoch: 3, batch: 478, loss: 0.40013, accu: 0.94219, speed: 0.52 step/s\n",
      "global step 4220, epoch: 3, batch: 488, loss: 0.39447, accu: 0.93789, speed: 2.24 step/s\n",
      "global step 4230, epoch: 3, batch: 498, loss: 0.40329, accu: 0.93724, speed: 2.54 step/s\n",
      "global step 4240, epoch: 3, batch: 508, loss: 0.37604, accu: 0.93262, speed: 2.58 step/s\n",
      "global step 4250, epoch: 3, batch: 518, loss: 0.38237, accu: 0.93156, speed: 2.40 step/s\n",
      "global step 4260, epoch: 3, batch: 528, loss: 0.35778, accu: 0.93255, speed: 2.41 step/s\n",
      "global step 4270, epoch: 3, batch: 538, loss: 0.39266, accu: 0.93259, speed: 2.36 step/s\n",
      "global step 4280, epoch: 3, batch: 548, loss: 0.37748, accu: 0.93271, speed: 2.32 step/s\n",
      "global step 4290, epoch: 3, batch: 558, loss: 0.38368, accu: 0.93359, speed: 2.38 step/s\n",
      "global step 4300, epoch: 3, batch: 568, loss: 0.37752, accu: 0.93367, speed: 2.46 step/s\n",
      "eval dev loss: 0.42211, accu: 0.88684\n",
      "global step 4310, epoch: 3, batch: 578, loss: 0.35955, accu: 0.93047, speed: 0.52 step/s\n",
      "global step 4320, epoch: 3, batch: 588, loss: 0.39028, accu: 0.93320, speed: 2.48 step/s\n",
      "global step 4330, epoch: 3, batch: 598, loss: 0.36944, accu: 0.93542, speed: 2.25 step/s\n",
      "global step 4340, epoch: 3, batch: 608, loss: 0.35678, accu: 0.93359, speed: 2.36 step/s\n",
      "global step 4350, epoch: 3, batch: 618, loss: 0.38789, accu: 0.93219, speed: 2.30 step/s\n",
      "global step 4360, epoch: 3, batch: 628, loss: 0.39963, accu: 0.93073, speed: 2.31 step/s\n",
      "global step 4370, epoch: 3, batch: 638, loss: 0.38252, accu: 0.93036, speed: 2.46 step/s\n",
      "global step 4380, epoch: 3, batch: 648, loss: 0.34655, accu: 0.93096, speed: 2.47 step/s\n",
      "global step 4390, epoch: 3, batch: 658, loss: 0.38183, accu: 0.93168, speed: 2.41 step/s\n",
      "global step 4400, epoch: 3, batch: 668, loss: 0.40820, accu: 0.93156, speed: 2.38 step/s\n",
      "eval dev loss: 0.42169, accu: 0.88934\n",
      "global step 4410, epoch: 3, batch: 678, loss: 0.37819, accu: 0.93984, speed: 0.53 step/s\n",
      "global step 4420, epoch: 3, batch: 688, loss: 0.34818, accu: 0.94531, speed: 2.39 step/s\n",
      "global step 4430, epoch: 3, batch: 698, loss: 0.42065, accu: 0.94375, speed: 2.46 step/s\n",
      "global step 4440, epoch: 3, batch: 708, loss: 0.36306, accu: 0.93965, speed: 2.51 step/s\n",
      "global step 4450, epoch: 3, batch: 718, loss: 0.40642, accu: 0.93859, speed: 2.29 step/s\n",
      "global step 4460, epoch: 3, batch: 728, loss: 0.36106, accu: 0.93828, speed: 2.37 step/s\n",
      "global step 4470, epoch: 3, batch: 738, loss: 0.36538, accu: 0.93962, speed: 2.41 step/s\n",
      "global step 4480, epoch: 3, batch: 748, loss: 0.37634, accu: 0.93896, speed: 2.33 step/s\n",
      "global step 4490, epoch: 3, batch: 758, loss: 0.38124, accu: 0.93785, speed: 2.44 step/s\n",
      "global step 4500, epoch: 3, batch: 768, loss: 0.37156, accu: 0.93773, speed: 2.36 step/s\n",
      "eval dev loss: 0.42693, accu: 0.88162\n",
      "global step 4510, epoch: 3, batch: 778, loss: 0.39299, accu: 0.93437, speed: 0.53 step/s\n",
      "global step 4520, epoch: 3, batch: 788, loss: 0.34562, accu: 0.93047, speed: 2.32 step/s\n",
      "global step 4530, epoch: 3, batch: 798, loss: 0.40815, accu: 0.93281, speed: 2.37 step/s\n",
      "global step 4540, epoch: 3, batch: 808, loss: 0.37834, accu: 0.93359, speed: 2.39 step/s\n",
      "global step 4550, epoch: 3, batch: 818, loss: 0.41153, accu: 0.93391, speed: 2.37 step/s\n",
      "global step 4560, epoch: 3, batch: 828, loss: 0.38913, accu: 0.93203, speed: 2.46 step/s\n",
      "global step 4570, epoch: 3, batch: 838, loss: 0.37854, accu: 0.93114, speed: 2.52 step/s\n",
      "global step 4580, epoch: 3, batch: 848, loss: 0.36128, accu: 0.93047, speed: 2.47 step/s\n",
      "global step 4590, epoch: 3, batch: 858, loss: 0.36896, accu: 0.93038, speed: 2.39 step/s\n",
      "global step 4600, epoch: 3, batch: 868, loss: 0.36942, accu: 0.93086, speed: 2.36 step/s\n",
      "eval dev loss: 0.42738, accu: 0.88275\n",
      "global step 4610, epoch: 3, batch: 878, loss: 0.36438, accu: 0.93359, speed: 0.53 step/s\n",
      "global step 4620, epoch: 3, batch: 888, loss: 0.37826, accu: 0.93477, speed: 2.43 step/s\n",
      "global step 4630, epoch: 3, batch: 898, loss: 0.40711, accu: 0.93437, speed: 2.50 step/s\n",
      "global step 4640, epoch: 3, batch: 908, loss: 0.34626, accu: 0.93594, speed: 2.35 step/s\n",
      "global step 4650, epoch: 3, batch: 918, loss: 0.35780, accu: 0.93578, speed: 2.37 step/s\n",
      "global step 4660, epoch: 3, batch: 928, loss: 0.39838, accu: 0.93516, speed: 2.35 step/s\n",
      "global step 4670, epoch: 3, batch: 938, loss: 0.36802, accu: 0.93214, speed: 2.36 step/s\n",
      "global step 4680, epoch: 3, batch: 948, loss: 0.37725, accu: 0.93164, speed: 2.30 step/s\n",
      "global step 4690, epoch: 3, batch: 958, loss: 0.37882, accu: 0.93220, speed: 2.38 step/s\n",
      "global step 4700, epoch: 3, batch: 968, loss: 0.37139, accu: 0.93211, speed: 2.44 step/s\n",
      "eval dev loss: 0.42464, accu: 0.884\n",
      "global step 4710, epoch: 3, batch: 978, loss: 0.39600, accu: 0.93437, speed: 0.53 step/s\n",
      "global step 4720, epoch: 3, batch: 988, loss: 0.40202, accu: 0.92656, speed: 2.39 step/s\n",
      "global step 4730, epoch: 3, batch: 998, loss: 0.38160, accu: 0.92891, speed: 2.39 step/s\n",
      "global step 4740, epoch: 3, batch: 1008, loss: 0.38245, accu: 0.93340, speed: 2.34 step/s\n",
      "global step 4750, epoch: 3, batch: 1018, loss: 0.38416, accu: 0.93594, speed: 2.26 step/s\n",
      "global step 4760, epoch: 3, batch: 1028, loss: 0.35485, accu: 0.93633, speed: 2.33 step/s\n",
      "global step 4770, epoch: 3, batch: 1038, loss: 0.38052, accu: 0.93594, speed: 2.38 step/s\n",
      "global step 4780, epoch: 3, batch: 1048, loss: 0.40409, accu: 0.93467, speed: 2.47 step/s\n",
      "global step 4790, epoch: 3, batch: 1058, loss: 0.36066, accu: 0.93351, speed: 2.37 step/s\n",
      "global step 4800, epoch: 3, batch: 1068, loss: 0.38919, accu: 0.93312, speed: 2.35 step/s\n",
      "eval dev loss: 0.42306, accu: 0.88707\n",
      "global step 4810, epoch: 3, batch: 1078, loss: 0.37625, accu: 0.94531, speed: 0.53 step/s\n",
      "global step 4820, epoch: 3, batch: 1088, loss: 0.37124, accu: 0.94063, speed: 2.25 step/s\n",
      "global step 4830, epoch: 3, batch: 1098, loss: 0.40149, accu: 0.94089, speed: 2.45 step/s\n",
      "global step 4840, epoch: 3, batch: 1108, loss: 0.38318, accu: 0.93867, speed: 2.33 step/s\n",
      "global step 4850, epoch: 3, batch: 1118, loss: 0.37598, accu: 0.93625, speed: 2.25 step/s\n",
      "global step 4860, epoch: 3, batch: 1128, loss: 0.34983, accu: 0.93685, speed: 2.49 step/s\n",
      "global step 4870, epoch: 3, batch: 1138, loss: 0.40139, accu: 0.93627, speed: 2.47 step/s\n",
      "global step 4880, epoch: 3, batch: 1148, loss: 0.34772, accu: 0.93652, speed: 2.46 step/s\n",
      "global step 4890, epoch: 3, batch: 1158, loss: 0.36247, accu: 0.93628, speed: 2.31 step/s\n",
      "global step 4900, epoch: 3, batch: 1168, loss: 0.34147, accu: 0.93508, speed: 2.31 step/s\n",
      "eval dev loss: 0.42223, accu: 0.88843\n",
      "global step 4910, epoch: 3, batch: 1178, loss: 0.37128, accu: 0.92578, speed: 0.53 step/s\n",
      "global step 4920, epoch: 3, batch: 1188, loss: 0.38217, accu: 0.92969, speed: 2.38 step/s\n",
      "global step 4930, epoch: 3, batch: 1198, loss: 0.37287, accu: 0.93281, speed: 2.41 step/s\n",
      "global step 4940, epoch: 3, batch: 1208, loss: 0.37125, accu: 0.93301, speed: 2.47 step/s\n",
      "global step 4950, epoch: 3, batch: 1218, loss: 0.37379, accu: 0.93031, speed: 2.35 step/s\n",
      "global step 4960, epoch: 3, batch: 1228, loss: 0.36397, accu: 0.93190, speed: 2.26 step/s\n",
      "global step 4970, epoch: 3, batch: 1238, loss: 0.38883, accu: 0.93147, speed: 2.36 step/s\n",
      "global step 4980, epoch: 3, batch: 1248, loss: 0.35791, accu: 0.93076, speed: 2.54 step/s\n",
      "global step 4990, epoch: 3, batch: 1258, loss: 0.40408, accu: 0.93064, speed: 2.53 step/s\n",
      "global step 5000, epoch: 3, batch: 1268, loss: 0.38220, accu: 0.93023, speed: 2.38 step/s\n",
      "eval dev loss: 0.42084, accu: 0.89037\n",
      "global step 5010, epoch: 3, batch: 1278, loss: 0.40319, accu: 0.94063, speed: 0.53 step/s\n",
      "global step 5020, epoch: 3, batch: 1288, loss: 0.39393, accu: 0.93633, speed: 2.49 step/s\n",
      "global step 5030, epoch: 3, batch: 1298, loss: 0.36276, accu: 0.93542, speed: 2.35 step/s\n",
      "global step 5040, epoch: 3, batch: 1308, loss: 0.35368, accu: 0.93457, speed: 2.17 step/s\n",
      "global step 5050, epoch: 3, batch: 1318, loss: 0.35118, accu: 0.93641, speed: 2.29 step/s\n",
      "global step 5060, epoch: 3, batch: 1328, loss: 0.37218, accu: 0.93516, speed: 2.40 step/s\n",
      "global step 5070, epoch: 3, batch: 1338, loss: 0.38543, accu: 0.93415, speed: 2.50 step/s\n",
      "global step 5080, epoch: 3, batch: 1348, loss: 0.37336, accu: 0.93379, speed: 2.35 step/s\n",
      "global step 5090, epoch: 3, batch: 1358, loss: 0.38109, accu: 0.93368, speed: 2.45 step/s\n",
      "global step 5100, epoch: 3, batch: 1368, loss: 0.37368, accu: 0.93352, speed: 2.42 step/s\n",
      "eval dev loss: 0.4209, accu: 0.88855\n",
      "global step 5110, epoch: 3, batch: 1378, loss: 0.36411, accu: 0.93984, speed: 0.52 step/s\n",
      "global step 5120, epoch: 3, batch: 1388, loss: 0.39690, accu: 0.93203, speed: 2.29 step/s\n",
      "global step 5130, epoch: 3, batch: 1398, loss: 0.35049, accu: 0.93333, speed: 2.35 step/s\n",
      "global step 5140, epoch: 3, batch: 1408, loss: 0.35212, accu: 0.93281, speed: 2.31 step/s\n",
      "global step 5150, epoch: 3, batch: 1418, loss: 0.39318, accu: 0.93109, speed: 2.35 step/s\n",
      "global step 5160, epoch: 3, batch: 1428, loss: 0.37886, accu: 0.93281, speed: 2.47 step/s\n",
      "global step 5170, epoch: 3, batch: 1438, loss: 0.39723, accu: 0.93248, speed: 2.46 step/s\n",
      "global step 5180, epoch: 3, batch: 1448, loss: 0.35727, accu: 0.93193, speed: 2.44 step/s\n",
      "global step 5190, epoch: 3, batch: 1458, loss: 0.37929, accu: 0.93090, speed: 2.39 step/s\n",
      "global step 5200, epoch: 3, batch: 1468, loss: 0.40946, accu: 0.93078, speed: 2.33 step/s\n",
      "eval dev loss: 0.42138, accu: 0.89002\n",
      "global step 5210, epoch: 3, batch: 1478, loss: 0.39443, accu: 0.93516, speed: 0.53 step/s\n",
      "global step 5220, epoch: 3, batch: 1488, loss: 0.37485, accu: 0.92852, speed: 2.26 step/s\n",
      "global step 5230, epoch: 3, batch: 1498, loss: 0.38195, accu: 0.93151, speed: 2.48 step/s\n",
      "global step 5240, epoch: 3, batch: 1508, loss: 0.38783, accu: 0.93379, speed: 2.32 step/s\n",
      "global step 5250, epoch: 3, batch: 1518, loss: 0.35493, accu: 0.93500, speed: 2.38 step/s\n",
      "global step 5260, epoch: 3, batch: 1528, loss: 0.34983, accu: 0.93646, speed: 2.32 step/s\n",
      "global step 5270, epoch: 3, batch: 1538, loss: 0.38644, accu: 0.93449, speed: 2.30 step/s\n",
      "global step 5280, epoch: 3, batch: 1548, loss: 0.39248, accu: 0.93350, speed: 2.35 step/s\n",
      "global step 5290, epoch: 3, batch: 1558, loss: 0.38497, accu: 0.93290, speed: 2.24 step/s\n",
      "global step 5300, epoch: 3, batch: 1568, loss: 0.37767, accu: 0.93172, speed: 2.31 step/s\n",
      "eval dev loss: 0.42359, accu: 0.88639\n",
      "global step 5310, epoch: 3, batch: 1578, loss: 0.35668, accu: 0.93594, speed: 0.53 step/s\n",
      "global step 5320, epoch: 3, batch: 1588, loss: 0.38883, accu: 0.93242, speed: 2.33 step/s\n",
      "global step 5330, epoch: 3, batch: 1598, loss: 0.39620, accu: 0.93255, speed: 2.34 step/s\n",
      "global step 5340, epoch: 3, batch: 1608, loss: 0.36770, accu: 0.93516, speed: 2.20 step/s\n",
      "global step 5350, epoch: 3, batch: 1618, loss: 0.36492, accu: 0.93469, speed: 2.45 step/s\n",
      "global step 5360, epoch: 3, batch: 1628, loss: 0.38024, accu: 0.93737, speed: 2.29 step/s\n",
      "global step 5370, epoch: 3, batch: 1638, loss: 0.38244, accu: 0.93549, speed: 2.48 step/s\n",
      "global step 5380, epoch: 3, batch: 1648, loss: 0.35240, accu: 0.93711, speed: 2.53 step/s\n",
      "global step 5390, epoch: 3, batch: 1658, loss: 0.38242, accu: 0.93533, speed: 2.36 step/s\n",
      "global step 5400, epoch: 3, batch: 1668, loss: 0.37436, accu: 0.93406, speed: 2.27 step/s\n",
      "eval dev loss: 0.42223, accu: 0.88889\n",
      "global step 5410, epoch: 3, batch: 1678, loss: 0.34206, accu: 0.93437, speed: 0.53 step/s\n",
      "global step 5420, epoch: 3, batch: 1688, loss: 0.38434, accu: 0.93086, speed: 2.42 step/s\n",
      "global step 5430, epoch: 3, batch: 1698, loss: 0.40663, accu: 0.93099, speed: 2.39 step/s\n",
      "global step 5440, epoch: 3, batch: 1708, loss: 0.39134, accu: 0.93145, speed: 2.23 step/s\n",
      "global step 5450, epoch: 3, batch: 1718, loss: 0.38729, accu: 0.93172, speed: 2.25 step/s\n",
      "global step 5460, epoch: 3, batch: 1728, loss: 0.39422, accu: 0.93099, speed: 2.24 step/s\n",
      "global step 5470, epoch: 3, batch: 1738, loss: 0.35963, accu: 0.93181, speed: 2.38 step/s\n",
      "global step 5480, epoch: 3, batch: 1748, loss: 0.32507, accu: 0.93203, speed: 2.34 step/s\n",
      "global step 5490, epoch: 3, batch: 1758, loss: 0.33791, accu: 0.93290, speed: 2.36 step/s\n",
      "global step 5500, epoch: 3, batch: 1768, loss: 0.35406, accu: 0.93320, speed: 2.47 step/s\n",
      "eval dev loss: 0.42198, accu: 0.88832\n",
      "global step 5510, epoch: 3, batch: 1778, loss: 0.35382, accu: 0.94766, speed: 0.53 step/s\n",
      "global step 5520, epoch: 3, batch: 1788, loss: 0.35575, accu: 0.94453, speed: 2.33 step/s\n",
      "global step 5530, epoch: 3, batch: 1798, loss: 0.40320, accu: 0.93776, speed: 2.47 step/s\n",
      "global step 5540, epoch: 3, batch: 1808, loss: 0.35109, accu: 0.93613, speed: 2.45 step/s\n",
      "global step 5550, epoch: 3, batch: 1818, loss: 0.38518, accu: 0.93484, speed: 2.50 step/s\n",
      "global step 5560, epoch: 3, batch: 1828, loss: 0.40575, accu: 0.93581, speed: 2.38 step/s\n",
      "global step 5570, epoch: 3, batch: 1838, loss: 0.40382, accu: 0.93460, speed: 2.38 step/s\n",
      "global step 5580, epoch: 3, batch: 1848, loss: 0.37352, accu: 0.93613, speed: 2.26 step/s\n",
      "global step 5590, epoch: 3, batch: 1858, loss: 0.38386, accu: 0.93655, speed: 2.47 step/s\n"
     ]
    }
   ],
   "source": [
    "# æ¥ä¸‹æ¥ï¼Œå¼€å§‹æ­£å¼è®­ç»ƒæ¨¡å‹ï¼Œè®­ç»ƒæ—¶é—´è¾ƒé•¿ï¼Œå¯æ³¨é‡Šæ‰è¿™éƒ¨åˆ†\n",
    "\n",
    "global_step = 0\n",
    "tic_train = time.time()\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for step, batch in enumerate(train_data_loader, start=1):\n",
    "\n",
    "        input_ids, token_type_ids, labels = batch\n",
    "        probs = model(input_ids=input_ids, token_type_ids=token_type_ids)\n",
    "        loss = criterion(probs, labels)\n",
    "        correct = metric.compute(probs, labels)\n",
    "        metric.update(correct)\n",
    "        acc = metric.accumulate()\n",
    "\n",
    "        global_step += 1\n",
    "        \n",
    "        # æ¯é—´éš” 10 step è¾“å‡ºè®­ç»ƒæŒ‡æ ‡\n",
    "        if global_step % 10 == 0:\n",
    "            print(\n",
    "                \"global step %d, epoch: %d, batch: %d, loss: %.5f, accu: %.5f, speed: %.2f step/s\"\n",
    "                % (global_step, epoch, step, loss, acc,\n",
    "                    10 / (time.time() - tic_train)))\n",
    "            tic_train = time.time()\n",
    "            # åŠ å…¥trainæ—¥å¿—æ˜¾ç¤º\n",
    "            writer.add_scalar(tag=\"train/loss\", step=global_step, value=loss)\n",
    "            writer.add_scalar(tag=\"train/acc\", step=global_step, value=acc)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.clear_grad()\n",
    "\n",
    "        # æ¯é—´éš” 100 step åœ¨éªŒè¯é›†å’Œæµ‹è¯•é›†ä¸Šè¿›è¡Œè¯„ä¼°\n",
    "        if global_step % 100 == 0:\n",
    "            evaluate(model, criterion, metric, dev_data_loader, \"dev\")\n",
    "\n",
    "            save_dir = os.path.join(\"checkpoint\", \"model_%d\" % global_step)\n",
    "            os.makedirs(save_dir)\n",
    "            # åŠ å…¥ä¿å­˜\n",
    "            save_param_path = os.path.join(save_dir, 'model_state.pdparams')\n",
    "            paddle.save(model.state_dict(), save_param_path)\n",
    "            tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "            \n",
    "# è®­ç»ƒç»“æŸåï¼Œå­˜å‚¨æ¨¡å‹å‚æ•°\n",
    "save_dir = os.path.join(\"checkpoint\", \"model_%d\" % global_step)\n",
    "os.makedirs(save_dir)\n",
    "\n",
    "save_param_path = os.path.join(save_dir, 'model_state.pdparams')\n",
    "paddle.save(model.state_dict(), save_param_path)\n",
    "tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šè¾“å‡ºå¦‚ä¸‹æ—¥å¿—:\n",
    "```\n",
    "global step 5310, epoch: 3, batch: 1578, loss: 0.31671, accu: 0.95000, speed: 0.63 step/s\n",
    "global step 5320, epoch: 3, batch: 1588, loss: 0.36240, accu: 0.94063, speed: 6.98 step/s\n",
    "global step 5330, epoch: 3, batch: 1598, loss: 0.41451, accu: 0.93854, speed: 7.40 step/s\n",
    "global step 5340, epoch: 3, batch: 1608, loss: 0.31327, accu: 0.94063, speed: 7.01 step/s\n",
    "global step 5350, epoch: 3, batch: 1618, loss: 0.40664, accu: 0.93563, speed: 7.83 step/s\n",
    "global step 5360, epoch: 3, batch: 1628, loss: 0.33064, accu: 0.93958, speed: 7.34 step/s\n",
    "global step 5370, epoch: 3, batch: 1638, loss: 0.38411, accu: 0.93795, speed: 7.72 step/s\n",
    "global step 5380, epoch: 3, batch: 1648, loss: 0.35376, accu: 0.93906, speed: 7.92 step/s\n",
    "global step 5390, epoch: 3, batch: 1658, loss: 0.39706, accu: 0.93924, speed: 7.47 step/s\n",
    "global step 5400, epoch: 3, batch: 1668, loss: 0.41198, accu: 0.93781, speed: 7.41 step/s\n",
    "eval dev loss: 0.4177, accu: 0.89082\n",
    "global step 5410, epoch: 3, batch: 1678, loss: 0.34453, accu: 0.93125, speed: 0.63 step/s\n",
    "global step 5420, epoch: 3, batch: 1688, loss: 0.34569, accu: 0.93906, speed: 7.75 step/s\n",
    "global step 5430, epoch: 3, batch: 1698, loss: 0.39160, accu: 0.92917, speed: 7.54 step/s\n",
    "global step 5440, epoch: 3, batch: 1708, loss: 0.46002, accu: 0.93125, speed: 7.05 step/s\n",
    "global step 5450, epoch: 3, batch: 1718, loss: 0.32302, accu: 0.93188, speed: 7.14 step/s\n",
    "global step 5460, epoch: 3, batch: 1728, loss: 0.40802, accu: 0.93281, speed: 7.22 step/s\n",
    "global step 5470, epoch: 3, batch: 1738, loss: 0.34607, accu: 0.93348, speed: 7.44 step/s\n",
    "global step 5480, epoch: 3, batch: 1748, loss: 0.34709, accu: 0.93398, speed: 7.38 step/s\n",
    "global step 5490, epoch: 3, batch: 1758, loss: 0.31814, accu: 0.93437, speed: 7.39 step/s\n",
    "global step 5500, epoch: 3, batch: 1768, loss: 0.42689, accu: 0.93125, speed: 7.74 step/s\n",
    "eval dev loss: 0.41789, accu: 0.88968\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "åŸºäºé»˜è®¤å‚æ•°é…ç½®è¿›è¡Œå•å¡è®­ç»ƒå¤§æ¦‚è¦æŒç»­ 4 ä¸ªå°æ—¶å·¦å³ï¼Œä¼šè®­ç»ƒå®Œæˆ 3 ä¸ª Epoch, æ¨¡å‹æœ€ç»ˆçš„æ”¶æ•›æŒ‡æ ‡ç»“æœå¦‚ä¸‹:\n",
    "\n",
    "\n",
    "| æ•°æ®é›† | Accuracy |\n",
    "| -------- | -------- |\n",
    "| dev.tsv     | 89.62  |\n",
    "\n",
    "å¯ä»¥çœ‹åˆ°: æˆ‘ä»¬åŸºäº PaddleNLP ï¼Œåˆ©ç”¨ ERNIE-Gram é¢„è®­ç»ƒæ¨¡å‹ä½¿ç”¨éå¸¸ç®€æ´çš„ä»£ç ï¼Œå°±åœ¨æƒå¨è¯­ä¹‰åŒ¹é…æ•°æ®é›†ä¸Šå–å¾—äº†å¾ˆä¸é”™çš„æ•ˆæœ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.5 æ¨¡å‹é¢„æµ‹\n",
    "\n",
    "æ¥ä¸‹æ¥æˆ‘ä»¬ä½¿ç”¨å·²ç»è®­ç»ƒå¥½çš„è¯­ä¹‰åŒ¹é…æ¨¡å‹å¯¹ä¸€äº›é¢„æµ‹æ•°æ®è¿›è¡Œé¢„æµ‹ã€‚å¾…é¢„æµ‹æ•°æ®ä¸ºæ¯è¡Œéƒ½æ˜¯æ–‡æœ¬å¯¹çš„ tsv æ–‡ä»¶ï¼Œæˆ‘ä»¬ä½¿ç”¨ Lcqmc æ•°æ®é›†çš„æµ‹è¯•é›†ä½œä¸ºæˆ‘ä»¬çš„é¢„æµ‹æ•°æ®ï¼Œè¿›è¡Œé¢„æµ‹å¹¶æäº¤é¢„æµ‹ç»“æœåˆ° [åƒè¨€æ–‡æœ¬ç›¸ä¼¼åº¦ç«èµ›](https://aistudio.baidu.com/aistudio/competition/detail/45)\n",
    "\n",
    "ä¸‹è½½æˆ‘ä»¬å·²ç»è®­ç»ƒå¥½çš„è¯­ä¹‰åŒ¹é…æ¨¡å‹, å¹¶è§£å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-06-10 23:04:59--  https://paddlenlp.bj.bcebos.com/models/text_matching/ernie_gram_zh_pointwise_matching_model.tar\n",
      "Resolving paddlenlp.bj.bcebos.com (paddlenlp.bj.bcebos.com)... 182.61.200.195, 182.61.200.229, 2409:8c00:6c21:10ad:0:ff:b00e:67d\n",
      "Connecting to paddlenlp.bj.bcebos.com (paddlenlp.bj.bcebos.com)|182.61.200.195|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 597667840 (570M) [application/x-tar]\n",
      "Saving to: â€˜ernie_gram_zh_pointwise_matching_model.tar.1â€™\n",
      "\n",
      "ernie_gram_zh_point 100%[===================>] 569.98M  56.0MB/s    in 11s     \n",
      "\n",
      "2021-06-10 23:05:10 (52.7 MB/s) - â€˜ernie_gram_zh_pointwise_matching_model.tar.1â€™ saved [597667840/597667840]\n",
      "\n",
      "ernie_gram_zh_pointwise_matching_model/\n",
      "ernie_gram_zh_pointwise_matching_model/model_state.pdparams\n",
      "ernie_gram_zh_pointwise_matching_model/vocab.txt\n",
      "ernie_gram_zh_pointwise_matching_model/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "# ä¸‹è½½æˆ‘ä»¬åŸºäº Lcqmc äº‹å…ˆè®­ç»ƒå¥½çš„è¯­ä¹‰åŒ¹é…æ¨¡å‹å¹¶è§£å‹\n",
    "! wget https://paddlenlp.bj.bcebos.com/models/text_matching/ernie_gram_zh_pointwise_matching_model.tar\n",
    "! tar -xvf ernie_gram_zh_pointwise_matching_model.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è°æœ‰ç‹‚ä¸‰è¿™å¼ é«˜æ¸…çš„\tè¿™å¼ é«˜æ¸…å›¾ï¼Œè°æœ‰\r\n",
      "è‹±é›„è”ç›Ÿä»€ä¹ˆè‹±é›„æœ€å¥½\tè‹±é›„è”ç›Ÿæœ€å¥½è‹±é›„æ˜¯ä»€ä¹ˆ\r\n",
      "è¿™æ˜¯ä»€ä¹ˆæ„æ€ï¼Œè¢«è¹­ç½‘å—\tæˆ‘ä¹Ÿæ˜¯é†‰äº†ï¼Œè¿™æ˜¯ä»€ä¹ˆæ„æ€\r\n"
     ]
    }
   ],
   "source": [
    "# æµ‹è¯•æ•°æ®ç”± 2 åˆ—æ–‡æœ¬æ„æˆ tab åˆ†éš”\n",
    "# Lcqmc é»˜è®¤ä¸‹è½½åˆ°å¦‚ä¸‹è·¯å¾„\n",
    "! head -n3 \"${HOME}/.paddlenlp/datasets/LCQMC/lcqmc/lcqmc/test.tsv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### å®šä¹‰é¢„æµ‹å‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def predict(model, data_loader):\n",
    "    \n",
    "    batch_probs = []\n",
    "\n",
    "    # é¢„æµ‹é˜¶æ®µæ‰“å¼€ eval æ¨¡å¼ï¼Œæ¨¡å‹ä¸­çš„ dropout ç­‰æ“ä½œä¼šå…³æ‰\n",
    "    model.eval()\n",
    "\n",
    "    with paddle.no_grad():\n",
    "        for batch_data in data_loader:\n",
    "            input_ids, token_type_ids = batch_data\n",
    "            input_ids = paddle.to_tensor(input_ids)\n",
    "            token_type_ids = paddle.to_tensor(token_type_ids)\n",
    "            \n",
    "            # è·å–æ¯ä¸ªæ ·æœ¬çš„é¢„æµ‹æ¦‚ç‡: [batch_size, 2] çš„çŸ©é˜µ\n",
    "            batch_prob = model(\n",
    "                input_ids=input_ids, token_type_ids=token_type_ids).numpy()\n",
    "\n",
    "            batch_probs.append(batch_prob)\n",
    "        batch_probs = np.concatenate(batch_probs, axis=0)\n",
    "\n",
    "        return batch_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### å®šä¹‰é¢„æµ‹æ•°æ®çš„ data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# é¢„æµ‹æ•°æ®çš„è½¬æ¢å‡½æ•°\n",
    "# predict æ•°æ®æ²¡æœ‰ label, å› æ­¤ convert_exmaple çš„ is_test å‚æ•°è®¾ä¸º True\n",
    "trans_func = partial(\n",
    "    convert_example,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=512,\n",
    "    is_test=True)\n",
    "\n",
    "# é¢„æµ‹æ•°æ®çš„ç»„ batch æ“ä½œ\n",
    "# predict æ•°æ®åªè¿”å› input_ids å’Œ token_type_idsï¼Œå› æ­¤åªéœ€è¦ 2 ä¸ª Pad å¯¹è±¡ä½œä¸º batchify_fn\n",
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # segment_ids\n",
    "): [data for data in fn(samples)]\n",
    "\n",
    "# åŠ è½½é¢„æµ‹æ•°æ®\n",
    "test_ds = load_dataset(\"lcqmc\", splits=[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_sampler = paddle.io.BatchSampler(test_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# ç”Ÿæˆé¢„æµ‹æ•°æ® data_loader\n",
    "predict_data_loader =paddle.io.DataLoader(\n",
    "        dataset=test_ds.map(trans_func),\n",
    "        batch_sampler=batch_sampler,\n",
    "        collate_fn=batchify_fn,\n",
    "        return_list=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### å®šä¹‰é¢„æµ‹æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-10 22:55:09,386] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/ernie-gram-zh/ernie_gram_zh.pdparams\n"
     ]
    }
   ],
   "source": [
    "# é€‰æ‹©é¢„è®­ç»ƒernie gramï¼Œå¡«å†™è‡ªå·±çš„ä»£ç \n",
    "pretrained_model = paddlenlp.transformers.ErnieGramModel.from_pretrained('ernie-gram-zh')\n",
    "\n",
    "model = PointwiseMatching(pretrained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### åŠ è½½å·²è®­ç»ƒå¥½çš„æ¨¡å‹å‚æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# åˆšæ‰ä¸‹è½½çš„æ¨¡å‹è§£å‹ä¹‹åå­˜å‚¨è·¯å¾„ä¸º ./ernie_gram_zh_pointwise_matching_model/model_state.pdparams\n",
    "#state_dict = paddle.load(\"./ernie_gram_zh_pointwise_matching_model/model_state.pdparams\")\n",
    "\n",
    "# åˆšæ‰ä¸‹è½½çš„æ¨¡å‹è§£å‹ä¹‹åå­˜å‚¨è·¯å¾„ä¸º ./pointwise_matching_model/ernie1.0_base_pointwise_matching.pdparams\n",
    "# state_dict = paddle.load(\"pointwise_matching_model/ernie1.0_base_pointwise_matching.pdparams\")\n",
    "\n",
    "# ä½¿ç”¨è‡ªå·±è®­ç»ƒå¥½çš„æœ€å¥½æ¨¡å‹ï¼Œè·¯å¾„ä¸º ./checkpoint/model_5000/model_state.pdparams\n",
    "state_dict = paddle.load(\"checkpoint/model_5000/model_state.pdparams\")\n",
    "model.set_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### å¼€å§‹é¢„æµ‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tensor(shape=[128, 38], dtype=int64, place=CUDAPinnedPlace, stop_gradient=True,\n",
      "       [[1   , 1022, 9   , ..., 0   , 0   , 0   ],\n",
      "        [1   , 514 , 904 , ..., 0   , 0   , 0   ],\n",
      "        [1   , 47  , 10  , ..., 0   , 0   , 0   ],\n",
      "        ...,\n",
      "        [1   , 936 , 356 , ..., 0   , 0   , 0   ],\n",
      "        [1   , 614 , 356 , ..., 0   , 0   , 0   ],\n",
      "        [1   , 630 , 1099, ..., 0   , 0   , 0   ]]), Tensor(shape=[128, 38], dtype=int64, place=CUDAPinnedPlace, stop_gradient=True,\n",
      "       [[0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0]])]\n"
     ]
    }
   ],
   "source": [
    "for idx, batch in enumerate(predict_data_loader):\n",
    "    if idx < 1:\n",
    "        print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# æ‰§è¡Œé¢„æµ‹å‡½æ•°\n",
    "y_probs = predict(model, predict_data_loader)\n",
    "\n",
    "# æ ¹æ®é¢„æµ‹æ¦‚ç‡è·å–é¢„æµ‹ label\n",
    "y_preds = np.argmax(y_probs, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### è¾“å‡ºé¢„æµ‹ç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'è°æœ‰ç‹‚ä¸‰è¿™å¼ é«˜æ¸…çš„', 'title': 'è¿™å¼ é«˜æ¸…å›¾ï¼Œè°æœ‰', 'label': 0}\n",
      "{'query': 'è‹±é›„è”ç›Ÿä»€ä¹ˆè‹±é›„æœ€å¥½', 'title': 'è‹±é›„è”ç›Ÿæœ€å¥½è‹±é›„æ˜¯ä»€ä¹ˆ', 'label': 1}\n",
      "{'query': 'è¿™æ˜¯ä»€ä¹ˆæ„æ€ï¼Œè¢«è¹­ç½‘å—', 'title': 'æˆ‘ä¹Ÿæ˜¯é†‰äº†ï¼Œè¿™æ˜¯ä»€ä¹ˆæ„æ€', 'label': 0}\n",
      "{'query': 'ç°åœ¨æœ‰ä»€ä¹ˆåŠ¨ç”»ç‰‡å¥½çœ‹å‘¢ï¼Ÿ', 'title': 'ç°åœ¨æœ‰ä»€ä¹ˆå¥½çœ‹çš„åŠ¨ç”»ç‰‡å—ï¼Ÿ', 'label': 1}\n",
      "{'query': 'è¯·é—®æ™¶è¾¾ç”µå­å‚ç°åœ¨çš„å·¥èµ„å¾…é‡æ€ä¹ˆæ ·è¦æ±‚æœ‰å“ªäº›', 'title': 'ä¸‰æ˜Ÿç”µå­å‚å·¥èµ„å¾…é‡æ€ä¹ˆæ ·å•Š', 'label': 0}\n",
      "{'query': 'æ–‡ç« çœŸçš„çˆ±å§šç¬›å—', 'title': 'å§šç¬›çœŸçš„è¢«æ–‡ç« å¹²äº†å—', 'label': 0}\n",
      "{'query': 'é€è‡ªå·±åšçš„é—ºèœœä»€ä¹ˆç”Ÿæ—¥ç¤¼ç‰©å¥½', 'title': 'é€é—ºèœœä»€ä¹ˆç”Ÿæ—¥ç¤¼ç‰©å¥½', 'label': 1}\n",
      "{'query': 'è¿‘æœŸä¸Šæ˜ çš„ç”µå½±', 'title': 'è¿‘æœŸä¸Šæ˜ çš„ç”µå½±æœ‰å“ªäº›', 'label': 1}\n",
      "{'query': 'æ±‚è‹±é›„è”ç›Ÿå¤§ç¥å¸¦ï¼Ÿ', 'title': 'è‹±é›„è”ç›Ÿï¼Œæ±‚å¤§ç¥å¸¦~', 'label': 1}\n",
      "{'query': 'å¦‚åŠ ä¸Šä»€ä¹ˆéƒ¨é¦–', 'title': 'ç»™ä¸œåŠ ä¸Šéƒ¨é¦–æ˜¯ä»€ä¹ˆå­—ï¼Ÿ', 'label': 0}\n",
      "{'query': 'æ­å·å“ªé‡Œå¥½ç©', 'title': 'æ­å·å“ªé‡Œå¥½ç©ç‚¹', 'label': 1}\n",
      "{'query': 'è¿™æ˜¯ä»€ä¹ˆä¹Œé¾Ÿå€¼é’±å—', 'title': 'è¿™æ˜¯ä»€ä¹ˆä¹Œé¾Ÿï¼å€¼é’±å˜›ï¼Ÿ', 'label': 1}\n",
      "{'query': 'å¿ƒå„æœ‰æ‰€å±æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ', 'title': 'å¿ƒæœ‰æ‰€å±æ˜¯ä»€ä¹ˆæ„æ€?', 'label': 1}\n",
      "{'query': 'ä»€ä¹ˆä¸œè¥¿è¶Šçƒ­çˆ¬å¾—è¶Šé«˜', 'title': 'ä»€ä¹ˆä¸œè¥¿è¶Šçƒ­çˆ¬å¾—å¾ˆé«˜', 'label': 1}\n",
      "{'query': 'ä¸–ç•Œæ¯å“ªä½çƒå‘˜è¿›çƒæœ€å¤š', 'title': 'ä¸–ç•Œæ¯å•ç•Œè¿›çƒæœ€å¤šæ˜¯å“ªä½çƒå‘˜', 'label': 1}\n",
      "{'query': 'éŸ­èœå¤šåƒä»€ä¹ˆå¥½å¤„', 'title': 'å¤šåƒéŸ­èœæœ‰ä»€ä¹ˆå¥½å¤„', 'label': 1}\n",
      "{'query': 'äº‘èµšé’±æ€ä¹ˆæ ·', 'title': 'æ€ä¹ˆæ‰èƒ½èµšé’±', 'label': 0}\n",
      "{'query': 'ä½•ç‚…ç»“å©šäº†å˜›', 'title': 'ä½•ç‚…ç»“å©šäº†ä¹ˆ', 'label': 1}\n",
      "{'query': 'é•¿çš„æ¸…æ–°æ˜¯ä»€ä¹ˆæ„æ€', 'title': 'å°æ¸…æ–°çš„æ„æ€æ˜¯ä»€ä¹ˆ', 'label': 0}\n",
      "{'query': 'æˆ‘ä»¬å¯ä»¥ç»“å©šäº†å—ï¼Ÿ', 'title': 'åœ¨ç†™ç»“å©šäº†å—ï¼Ÿ', 'label': 0}\n",
      "{'query': 'æƒ³ä¹°ç”·äººé…’è¡¥è‚¾å£®é˜³é…’å“ªé‡Œæœ‰å•Š', 'title': 'å“ªé‡Œæœ‰ç”·äººé…’è¡¥è‚¾å£®é˜³é…’', 'label': 1}\n",
      "{'query': 'æ·˜å®ä¸Šæ€ä¹ˆç”¨ä¿¡ç”¨å¡åˆ†æœŸä»˜æ¬¾', 'title': 'æ·˜å®æ€ä¹ˆåˆ†æœŸä»˜æ¬¾ï¼Œæ²¡æœ‰ä¿¡ç”¨å¡', 'label': 0}\n",
      "{'query': 'æœ€è¿‘æœ‰æ²¡æœ‰ä»€ä¹ˆå¥½çœ‹çš„éŸ©å‰§', 'title': 'æœ€è¿‘æœ‰ä»€ä¹ˆå¥½çœ‹çš„éŸ©å‰§', 'label': 1}\n",
      "{'query': 'ã€Šæ ¡èŠ±çš„è´´èº«é«˜æ‰‹ã€‹ä¸­çš„æ—é€¸', 'title': 'æ ¡èŠ±è´´èº«é«˜æ‰‹', 'label': 1}\n",
      "{'query': 'å”å”æ˜¯ä»€ä¹ˆäºº', 'title': 'æˆ‘æ˜¯å”å”çš„ä»€ä¹ˆäºº', 'label': 0}\n",
      "{'query': 'è¿™å§‘å¨˜æ¼‚äº®ä¸', 'title': 'æˆ‘å§‘å¨˜æ¼‚äº®å§', 'label': 0}\n",
      "{'query': 'åœ¨æ·˜å®ç½‘ä¹°æ‰‹æœºå¯é å—ï¼Ÿ', 'title': 'åœ¨æ·˜å®ç½‘ä¸Šä¹°æ‰‹æœºå¯é å—ï¼Ÿ', 'label': 1}\n",
      "{'query': 'å±±æ¥‚å¹²æ€ä¹ˆåƒå¥½åƒï¼Ÿ', 'title': 'å±±æ¥‚æ€ä¹ˆåšå¥½åƒ', 'label': 0}\n",
      "{'query': 'æ—¶é—´éƒ½å»å“ªæ€•äº†æ­Œè°±', 'title': 'æ—¶é—´ç…®é›¨æ­Œè°±', 'label': 0}\n",
      "{'query': 'è‹å·å“ªé‡Œèƒ½ä¹°åˆ°è¿™ä¸ªè¡£æœ', 'title': 'è‹å·å“ªé‡Œæœ‰ä¹°å¤§å·è¡£æœçš„ï¼Ÿ', 'label': 0}\n",
      "{'query': 'æœ€å¥½ç©çš„æ‰‹æœºç½‘æ¸¸', 'title': 'å¥½ç©çš„æ‰‹æœºç½‘æ¸¸', 'label': 1}\n",
      "{'query': 'çŸ³æ¦´æ˜¯ä»€ä¹ˆæ—¶å€™æˆç†Ÿçš„ï¼Ÿ', 'title': 'æˆç†Ÿçš„çŸ³æ¦´åƒä»€ä¹ˆï¼Ÿ', 'label': 0}\n",
      "{'query': 'åˆ˜è¯—è¯—æ¨å¹‚è°æ¼‚äº®', 'title': 'åˆ˜è¯—è¯—å’Œæ¨å¹‚è°æ¼‚äº®', 'label': 1}\n",
      "{'query': 'å¾®ä¿¡å·æ€ä¹ˆäºŒæ¬¡ä¿®æ”¹', 'title': 'æ€ä¹ˆå†äºŒæ¬¡ä¿®æ”¹å¾®ä¿¡å·', 'label': 1}\n",
      "{'query': 'ä»€ä¹ˆç‰Œå­çš„ç²¾æ²¹çš‚å¥½', 'title': 'ä»€ä¹ˆç‰Œå­çš„ç²¾æ²¹å¥½ï¼Ÿ', 'label': 0}\n",
      "{'query': 'åˆšå‡ºç”Ÿçš„å°é‡é¸¡æ€ä¹ˆå…»', 'title': 'åˆšæŠ“æ¥çš„é‡é¸¡æ€ä¹ˆå…»æ®–', 'label': 1}\n",
      "{'query': 'å¦‚ä½•å…¥ä¾µä»–äººæ‰‹æœº', 'title': 'å¦‚ä½•å…¥ä¾µåˆ«äººçš„æ‰‹æœº', 'label': 1}\n",
      "{'query': 'çº¢ç±³åˆ·ä»€ä¹ˆç³»ç»Ÿå¥½', 'title': 'çº¢ç±³å¯ä»¥åˆ·ä»€ä¹ˆç³»ç»Ÿ', 'label': 1}\n",
      "{'query': 'è¿™å«ä»€ä¹ˆé«˜è·Ÿé‹', 'title': 'è¿™ç§é«˜è·Ÿé‹å«ä»€ä¹ˆå‘€', 'label': 1}\n",
      "{'query': 'æ±‡ç†è´¢æ€ä¹ˆæ ·', 'title': 'æ€ä¹ˆæ ·å»ç†è´¢ï¼Ÿ', 'label': 0}\n",
      "{'query': 'ä»€ä¹ˆæ˜¯åˆ·å±', 'title': 'ä»€ä¹ˆå«åˆ·å±ï¼Ÿ', 'label': 1}\n",
      "{'query': 'å„å›½è´§å¸ç¬¦å·æ˜¯ä»€ä¹ˆ?', 'title': 'å¦‚ä½•ç”¨ç”µè„‘æ‰“å‡ºå„å›½è´§å¸ç¬¦å·å‘€', 'label': 0}\n",
      "{'query': 'ä¸Šå˜´å”‡æœ‰ç—£ä»£è¡¨ä»€ä¹ˆ', 'title': 'å’¬å˜´å”‡ä»£è¡¨ä»€ä¹ˆ', 'label': 0}\n",
      "{'query': 'å“ªç§å‡è‚¥è¯æœ€å¿«æœ€æœ‰æ•ˆ', 'title': 'å“ªç§å‡è‚¥è¯æœ€æœ‰æ•ˆï¼Œå‡è‚¥æ•ˆæœæœ€å¥½', 'label': 1}\n",
      "{'query': 'é‚“ç´«æ£‹å”±åŠŸæ€ä¹ˆæ ·', 'title': 'é‚“ç´«æ£‹å”±åŠŸæ€ä¹ˆæ ·ï¼Ÿ', 'label': 1}\n",
      "{'query': 'æ€ä¹ˆåšè§†é¢‘', 'title': 'è´´å§æ€ä¹ˆè´´è§†é¢‘', 'label': 0}\n",
      "{'query': 'ç°åœ¨å¥³ç”Ÿæµè¡Œä»€ä¹ˆå‘å‹ï¼Ÿ', 'title': 'å¥³ç”Ÿç°åœ¨æµè¡Œä»€ä¹ˆå‘å‹ï¼Ÿ', 'label': 1}\n",
      "{'query': 'ä¸ºä»€ä¹ˆåè½¦ç©æ‰‹æœºä¼šæ™•è½¦', 'title': 'ä¸ºä»€ä¹ˆæˆ‘åè½¦ç©æ‰‹æœºä¸æ™•è½¦', 'label': 0}\n",
      "{'query': 'ä¸ºä»€ä¹ˆè€å©†ä¸å–œæ¬¢å’Œæˆ‘åšçˆ±', 'title': 'æˆ‘è€å©†ä¸ºä»€ä¹ˆä¸å–œæ¬¢åšçˆ±', 'label': 1}\n",
      "{'query': 'æ€ä¹ˆæµ‹è¯•æˆ‘çˆ±çš„ä»–çˆ±ä¸çˆ±æˆ‘ã€‚', 'title': 'æ€ä¹ˆæµ‹è¯•è€å…¬çˆ±ä¸çˆ±æˆ‘å•Š', 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "# æˆ‘ä»¬æŒ‰ç…§åƒè¨€æ–‡æœ¬ç›¸ä¼¼åº¦ç«èµ›çš„æäº¤æ ¼å¼å°†é¢„æµ‹ç»“æœå­˜å‚¨åœ¨ lcqmc.tsv ä¸­ï¼Œç”¨æ¥åç»­æäº¤\n",
    "# åŒæ—¶å°†é¢„æµ‹ç»“æœè¾“å‡ºåˆ°ç»ˆç«¯ï¼Œä¾¿äºå¤§å®¶ç›´è§‚æ„Ÿå—æ¨¡å‹é¢„æµ‹æ•ˆæœ\n",
    "\n",
    "test_ds = load_dataset(\"lcqmc\", splits=[\"test\"])\n",
    "\n",
    "with open(\"lcqmc.tsv\", 'w', encoding=\"utf-8\") as f:\n",
    "    f.write(\"index\\tprediction\\n\")    \n",
    "    for idx, y_pred in enumerate(y_preds):\n",
    "        f.write(\"{}\\t{}\\n\".format(idx, y_pred))\n",
    "        # è¾“å‡ºå¤ªé•¿ï¼Œåªå±•ç¤º50è¡Œ\n",
    "        if idx<50:\n",
    "            text_pair = test_ds[idx]\n",
    "            text_pair[\"label\"] = y_pred\n",
    "            print(text_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### æäº¤ LCQMC é¢„æµ‹ç»“æœ[åƒè¨€æ–‡æœ¬ç›¸ä¼¼åº¦ç«èµ›](https://aistudio.baidu.com/aistudio/competition/detail/45)\n",
    "\n",
    "åƒè¨€æ–‡æœ¬ç›¸ä¼¼åº¦ç«èµ›ä¸€å…±æœ‰ 3 ä¸ªæ•°æ®é›†: lcqmcã€bq_corpusã€paws-x, æˆ‘ä»¬åˆšæ‰ç”Ÿæˆäº† lcqmc çš„é¢„æµ‹ç»“æœ lcqmc.tsv, åŒæ—¶æˆ‘ä»¬åœ¨é¡¹ç›®å†…æä¾›äº† bq_corpusã€paw-x æ•°æ®é›†çš„ç©ºé¢„æµ‹ç»“æœï¼Œæˆ‘ä»¬å°†è¿™ 3 ä¸ªæ–‡ä»¶æ‰“åŒ…æäº¤åˆ°åƒè¨€æ–‡æœ¬ç›¸ä¼¼åº¦ç«èµ›ï¼Œå³å¯çœ‹åˆ°è‡ªå·±çš„æ¨¡å‹åœ¨ Lcqmc æ•°æ®é›†ä¸Šçš„ç«èµ›æˆç»©ã€‚\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: lcqmc.tsv (deflated 65%)\r\n",
      "  adding: paws-x.tsv (deflated 61%)\r\n",
      "  adding: bq_corpus.tsv (deflated 62%)\r\n"
     ]
    }
   ],
   "source": [
    "# æ‰“åŒ…é¢„æµ‹ç»“æœ\n",
    "!zip submit.zip lcqmc.tsv paws-x.tsv bq_corpus.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### æäº¤é¢„æµ‹ç»“æœ submit.zip åˆ° [åƒè¨€æ–‡æœ¬ç›¸ä¼¼åº¦ç«èµ›](https://aistudio.baidu.com/aistudio/competition/detail/45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# åƒè¨€æ–‡æœ¬ç›¸ä¼¼åº¦ç«èµ›ç»“æœæˆªå›¾\n",
    "\n",
    "å°†è‡ªå·±çš„ç«èµ›ç»“æœè´´åœ¨æ­¤å¤„\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/be6710236f094b9fbde5a7f2710f8c106b18571205fd48cd802bcff29b575ccb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.1.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
